\chapter{Basics of the real numbers}\label{chp2}

First of all, let us recall our definition of the real numbers.
\begin{textequation}
There exists a nonzero Dedekind-complete totally-ordered field which is unique up to isomorphism of totally-ordered fields.  This field is the field of real numbers.
\end{textequation}
Recall that we also showed (\cref{prp1.4.52}) that $\R$ must contain $\Q$, and hence in turn $\Z$ and $\N$.

\section{Cardinality and countability}\label{CardinalityAndCountability}

This section is a bit of an aside---while not on the real numbers per se, a knowledge of cardinality and countability is absolutely essential to an understanding of the real numbers.

In the first chapter, we briefly discussed the notion of cardinality and its relationship to the natural numbers.  In fact, the natural numbers themselves are, as a set, precisely the finite cardinals.

The first fact we point out is that the cardinality of the natural numbers is the smallest infinite cardinal.
\begin{prp}{}{exr2.1.1}
Let $\kappa$ be an infinite cardinal.  Then, $\abs{\N}\leq \kappa$.
\begin{rmk}
Phrased differently ,note that the contrapositive easily implies the following.
\begin{displayquote}
If $\kappa$ is a cardinal with $\kappa \leq \abs{\N}$, then either $\kappa =\abs{\N}$ or $\kappa$ is finite.
\end{displayquote}
\end{rmk}
\begin{proof}
Let $K$ be any set such that $|K|=\kappa$.  Recall that (\cref{dfn1.1.23}) to show that $|\N |\leq \kappa$ requires that we produce an injection from $\N$ into $K$.  We construct an injection $f\colon \N \rightarrow K$ inductively.  Let $k_0\in K$ be arbitrary and let us define $f(0)\coloneqq k_0$.  If $K\setminus \{ k_0\}$ were empty, then $K$ would not be infinite, therefore there must be some $k_1\in K$ distinct from $k_0$, so that we may define $f(1)\coloneqq k_1$.  Continuing this process, suppose we have defined $f$ on $0,\ldots ,n\in \N$, and wish to define $f(n+1)$.  If $K\setminus \left\{ f(0),\ldots ,f(n)\right\}$ were empty, then $K$ would be finite.  Thus, there must be some $k_{n+1}\in K$ distinct from $f(0),\ldots ,f(n)$.  We may then define $f(n+1)\coloneqq k_{n+1}$.  The function produced must be injective because, by construction, $f(m)$ is distinct from $f(n)$ for all $n<m$.  Hence, $|\N |\leq \kappa$.
\end{proof}
\end{prp}
Thus, the cardinality of the natural numbers is the smallest infinite cardinality.  We give a name to this cardinality.
\begin{dfn}{Countability}{dfn2.2}
Let $X$ be a set.  Then, $X$ is \term{countably-infinite}\index{Countably-infinite} iff $|X|=|\N |$.  $X$ is \term{countable} iff either $X$ is countably-infinite or $X$ is finite.  We write $\aleph _0\coloneqq |\N |$\index[notation]{$\aleph _0$}.
\begin{rmk}
It is not uncommon for people to use the term ``countable'' to mean what we call ``countably-infinite''.  They would would then just say ``countable or finite'' in cases that we would say ``countable''.
\end{rmk}
\end{dfn}

Our first order of business is to decide what other sets besides the natural numbers are countably-infinite.
\begin{prp}{}{}
The even natural numbers, $2\N$, are countably-infinite.
\begin{proof}
On one hand, $2\N \subseteq \N$, so that $|2\N |\leq \aleph _0$.  On the other hand, $2\N$ is infinite, and as we just showed that $\aleph _0$ is the smallest infinite cardinal, we must have that $\aleph _0\leq |2\N |$.  Therefore, by antisymmetry (Bernstein-Cantor-Schr\"{o}der Theorem, \cref{thm1.1.26}) of $\leq$, we have that $|2\N |=\aleph _0$.
\end{proof}
\end{prp}
\begin{exr}{}{}
Construct an explicit bijection from $\N$ to $2\N$.
\end{exr}
This is the first explicit example we have seen of some perhaps not-so-intuitive behavior of cardinality.  On one hand, our intuition might tell us that there are half as many even natural numbers as there are natural numbers, yet, on the other hand, we have just proven (in two different ways, if you did the exercise) that $2\N$ and $\N$ have the same number of elements!  This of course is not the only example of this sort of weird behavior.  The next exercise shows that this is actually quite general.
\begin{exr}{}{}
Let $X$ and $Y$ be countably-infinite sets.  Show that $X\sqcup Y$ is countably-infinite.
\begin{rmk}
Note that this generalizes---see \cref{exr2.1.7}.
\end{rmk}
\end{exr}
Thus, it is literally the case that $2\aleph _0=\aleph _0$.  A simple corollary of this is that $\Z$ is countably-infinite.
\begin{exr}{}{}
Show that $|\Z |=\aleph _0$.
\end{exr}

You (hopefully) just showed that $2\aleph _0=\aleph _0$, but what about $\aleph _0^2$?
\begin{exr}{}{exr2.1.7}
Let $\collection{X}$ be a countable indexed collection of countable sets.  Show that
\begin{equation}
\bigsqcup _{X\in \collection{X}}X
\end{equation}
is countable.
\end{exr}

\begin{prp}{}{}
$\aleph _0^2=\aleph _0$.
\begin{proof}
For $m\in \N$, define
\begin{equation}
X_m\coloneqq \left\{ \coord{i,j}\in \N \times \N:i+j=m\right\} 
\end{equation}
Note that each $X_m$ is finite and also that
\begin{equation}
\N \times \N =\bigsqcup _{m\in \N}X_m.
\end{equation}
Therefore, by the previous exercise, $\abs{\N \times \N }\eqqcolon \aleph _0^2$ is countable, i.e., $\aleph _0^2\leq \aleph _0$.  As $\aleph _0$ is not finite, we must thus have that $\aleph _0^2=\aleph _0$ (\cref{exr2.1.1}).
\end{proof}
\end{prp}
\begin{exr}{}{}
Use Bernstein-Cantor-Schr\"{o}der and the previous proposition to show that $|\Q |=\aleph _0$.
\end{exr}
This result might seem a bit crazy at first.  I mean, just `look' at the number line, right?  There's like bajillions more rationals than naturals.  Surely it can't be the case there there are no more rationals than natural numbers, can it?  Well, yes, in fact that can be, and in fact is, precisely the case---despite what your silly intuition might be telling you, there are no more rational numbers than there are natural numbers.

We mentioned briefly before the algebraic numbers in the remark right before \crefnameref{sbs1.4.2}.  We go ahead and define them now because they provide a good exercise in cardinality.
\begin{dfn}{Real algebraic numbers}{dfn2.13}
A real number $\alpha \in \R$ is \term{algebraic}\index{Algebraic (number)} iff there exists a nonzero polynomial with integer coefficients $p$ such that $p(\alpha )=0$.  We write $\A _{\R}$\index[notation]{$\A _{\R}$} for the set of real algebraic numbers.
\begin{rmk}
Every rational number is real algebraic:  if $q=\frac{m}{n}\in \Q$, then $q$ is a root of the polynomial $nx-m$.  On the other hand, there are real algebraic numbers that are not rational, for example, $\sqrt{2}\in \R$ is a root of $x^2-2$.\footnote{I know technically we have not yet defined $\sqrt{2}$, though as I only intend you to convince you (as opposed to prove it for you) that there are algebraic numbers which are not rational, this is not an issue.}
\end{rmk}
\begin{rmk}
The algebraic numbers (denoted simply by $\A$\index[notation]{$\A$}) are those elements in $\C$ which are roots of a nonzero polynomial with integer coefficients.  The real algebraic numbers are then those elements of $\C$ which are both real numbers and algebraic numbers.  We haven't actually defined $\C$, which is why we only define the real algebraic numbers.
\end{rmk}
\begin{rmk}
In fact, both $\A$ and $\A _{\R}$ are fields, though this is not so easy to see.  For example, can you even show that the sum of two algebraic numbers is again algebraic?
\end{rmk}
\end{dfn}
\begin{exr}{}{}
Show that $\A _{\R}$ is countable.
\end{exr}

So, we've now done both $\Z$ and $\Q$, but what about $\R$?  At first, you might have declared it obvious that there are more real numbers than natural numbers, but perhaps the result about $\Q$ has now given you some doubt.  In fact, it \emph{does} turn out that there are more real numbers than there are natural numbers.
\begin{thm}{Cantor's Cardinality Theorem}{CantorsCardinalityTheorem}\index{Cantor's Cardinality Theorem}
Let $X$ be a set.  Then, $|X|<|2^X|$.
\begin{rmk}
There is a good chance you may have heard of the term \term{Cantor's Diagonal Argument}\index{Cantor's Diagonal Argument}.  The argument here is a generalization of that (it's also `cleaner'), and so we don't present the Diagonal Argument itself.
\end{rmk}
\begin{proof}
We must show two things:  (i)~$|X|\leq |2^X|$ and (ii)~$|X|\neq |2^X|$.

The first, by definition, requires that we construct an injection from $X$ to $2^X$.  This, however, is quite easy.  We may define a function $X\rightarrow 2^X$ by $x\mapsto \{ x\}$.  This is of course an injection.

The harder part is showing that $|X|\neq |2^X|$.  To show this, we must show that there is \emph{no} surjection from $X$ to $2^X$.  So, let $f\colon X\rightarrow 2^X$ be a function.  We show that $f$ cannot be surjective.  To do this, we construct a subset of $X$ that cannot be in the image of $f$.

We define
\begin{equation}
S\coloneqq \left\{ x\in X:x\notin f(x)\right\} .
\end{equation}
We would like to show that $S$ is not in the image of $f$.  We proceed by contradiction:  suppose that $S=f(x_0)$ for some $x_0\in X$.  Now, we must have that either $x_0\in S$ or $x_0\notin S$.  If the former were true, then we would have that $x_0\notin f(x_0)=S$:  a contradiction.  On the other hand, in the latter case, we would have $x\in f(x_0)=S$:  a contradiction.  Thus, as neither of these possibilities can be true, there cannot be any $x_0\in X$ such that $f(x_0)=S$.  Thus, $S$ is not in the image of $f$, and so $f$ is not surjective.
\end{proof}
\end{thm}
Now, to show that $|\R |>\aleph _0$, we show that $|\R |=2^{\aleph _0}$.  Before we do this, however, we must know a little more about the real numbers, and so we shall return to this later in the chapter---see \nameref{sss2.4.3}.

\section{The absolute value}

$\R$ already has a decent amount of structure:  both an order and a field structure.  We now equip it with yet another structure which will make arguments easier to work with and more intuitive (though as the entire definition involves only the order and algebraic structure, in principle we could do without it).
\begin{dfn}{Absolute value}{AbsoluteValue}
The \term{absolute value}\index{Absolute value} of $x\in \R$, denoted by $\abs{x}$\index[notation]{$\abs{x}$}, is defined by
\begin{equation}
\abs{x}\coloneqq \begin{cases}x & \text{if }x\geq 0 \\ -x & \text{if }x\leq 0.\end{cases}
\end{equation}
For $\varepsilon >0$ and $x_0\in \R$, we write
\begin{equation}
\begin{aligned}
B_{\varepsilon}(x_0) & \coloneqq \left\{ x\in \R :|x-x_0|<\varepsilon \right\} \\
D_{\varepsilon}(x_0) & \coloneqq \left\{ x\in \R :|x-x_0|\leq \varepsilon \right\} .
\end{aligned}
\end{equation}
\begin{rmk}
$B_{\varepsilon}(x_0)$\index[notation]{$B_{\varepsilon}(x_0)$} is the \term{ball}\index{Ball} of radius $\varepsilon$ centered at $x_0$ and $D_{\varepsilon}(x_0)$\index[notation]{$D_{\varepsilon}(x_0)$} is the \term{disk}\index{Disk} of radius $\varepsilon$ centered at $x_0$.
\end{rmk}
\end{dfn}
\begin{exr}{}{exr3.1.4}
Let $x_1,x_2\in \R$.  Show that the following statements are true.
\begin{enumerate}
\item \label{enm3.3.i}(Nonnegativity) $\abs{x_1}\geq 0$.
\item \label{enm3.3.ii}(Definiteness) $\abs{x_1}=0$ iff $x_1=0$.
\item \label{enm3.3.iii}(Homogeneity) $\abs{x_1x_2}=\abs{x_1}\abs{x_2}$.
\item \label{enm3.3.iv}(Triangle Inequality)\index{Triangle Inequality} $\abs{x_1+x_2}\leq \abs{x_1}+\abs{x_2}$.
\item \label{enm3.3.v}(Reverse Triangle Inequality)\index{Reverse Triangle Inequality} $\abs{\abs{x_1}-\abs{x_2}}\leq \abs{x_1-x_2}$.
\end{enumerate}
\begin{rmk}
The reason that \cref{enm3.3.iv} is called the \emph{triangle inequality} is the following.  First of all, by \cref{enm3.3.iii}, the triangle inequality can instead be written as $\abs{x_1-x_2}\leq \abs{x_1}+\abs{x_2}$.  Then, if you pretend that $x_1$ and $x_2$ are vectors representing the sides of the triangle, $x_1-x_2$ is a vector representing the third side of the triangle.  The triangle inequality then states that the length of a side of a triangle is at most the sum of the lengths of the other two sides (being equal iff the angle between those two sides is precisely $\uppi$).  Your solution should help explain why the reverse triangle inequality is called what it is.
\end{rmk}
\end{exr}
Intuitively, of course, $\abs{x}$ is supposed to be the distance $x$ is from $0$.  Then, $|x-y|$ is supposed to be the distance between $x$ and $y$.

A simple result we have is that, if the distance between two integers is less than $1$, then they are the same integer.
\begin{prp}{}{prp3.2}
Let $0<\varepsilon <1$ and let $m,n\in \Z$.  Then, if $\abs{m-n}<\varepsilon$, then $m=n$.
\begin{proof}
Suppose that $\abs{m-n}<\varepsilon$.  Without loss of generality, suppose that $m\leq n$, so that we can write $n=m+k$ for $k\in \Z _0^+$.  Then, $\abs{m-n}=k<\varepsilon <1$.  It follows from \cref{exr1.2.14} that $k=0$, so that $m=n$.
\end{proof}
\end{prp}

\section{The Archimedean Property}\label{sct3.2}

The first property of the real numbers we come to is called the Archimedean Property.  The Archimedean Property essentially says that the natural numbers are unbounded.
\begin{dfn}{The Archimedean Property}{}
Let $F$ be a nonzero totally-ordered field so that $\Q \subseteq F$ (see \cref{prp1.4.52}).  Then, we say that $F$ is \term{Archimedean}\index{Archimedean field} iff for all $x\in F$ there is some $m\in \N \subseteq F$ such that $x<m$.
\begin{rmk}
Note the use of \emph{Archimedean field} in contrast with \emph{Dedekind-complete}:  one is capitalized and hte other is not.  To be honest, while this convention doesn't make the most sense to me, it seems most standard to write people's names, even if not being used to refer to that person, with an upper-case letter, but to \emph{not} use upper-case for terms derived from people's names.
\end{rmk}
\end{dfn}
\begin{exr}{}{exr3.1.6}
Let $F$ be an Archimedean totally-ordered field and let $\varepsilon >0$.  Show that there is some $m\in \N \subseteq F$ such that $\frac{1}{m}<\varepsilon$.
\end{exr}
\begin{thm}{Archimedean property of the real numbers}{thm3.2.3}
$\R$ is Archimedean.
\begin{proof}
If $x\leq 0$, we may take $m=1$.  Otherwise, $x>0$, and so the set
\begin{equation}
S\coloneqq \left\{ m\in \N :m<x\right\} 
\end{equation}
is nonempty (it contains $0$).  On the other hand, it is also bounded-above (by $x$), and so by the least upper-bound property, it has a supremum:  write $m_0\coloneqq \sup (S)$.  We first show that $m_0\in \N$, and then we will show that $x<m_0+2$.

By \cref{prp1.4.11}, there must be some $m_1\in S$ such that
\begin{equation}
m_0-\tfrac{1}{2}<m_1\leq m_0.
\end{equation}
If $m_1=m_0$, we are done showing that $m_0\in \N$, so suppose that $m_1<m_0$.  Then, we may use this same proposition again to obtain an $m_2\in S$ such that
\begin{equation}
m_1<m_2\leq m_0.
\end{equation}
But then $|m_1-m_2|<\frac{1}{2}$ (because they are both strictly between $m_0-\frac{1}{2}$ and $m_0$), and so $m_1=m_2$ (by \cref{prp3.2}):  a contradiction (of the fact that $m_1<m_2$).  hence, it must have been the case that $m_0=m_1\in \N$.

We cannot have that $m_0+1\in S$ because then otherwise $m_0$ would not be an upper-bound of $S$.  Therefore, $m_0+1\in \N \setminus S=\left\{ m\in \N :x\leq m\right\}$, and so $x\leq m_0+1$, and so $x<m_0+2$.
\end{proof}
\end{thm}
Our first application of the Archimedean Property is that it allows us to define the \emph{floor} and \emph{ceiling} functions.
\begin{prp}{Floor and ceiling}{FloorAndCeiling}
Let $x\in \R$.
\begin{itemize}
\item There is a unique integer $\floor{x}$\index[notation]{$\floor{x}$}, the \term{floor}\index{Floor} of $x$, such that
\begin{enumerate}
\item $\floor{x}\leq x$; and
\item if $\Z \ni m\leq x$, then $m\leq \floor{x}$.
\end{enumerate}
\item There is a unique integer $\ceil{x}$, \index[notation]{$\ceil{x}$}, the \term{ceiling}\index{Ceiling} of $x$, such that
\begin{enumerate}
\item $\ceil{x}\geq x$; and
\item if $\Z \ni m\geq x$, then $m\geq \ceil{x}$.
\end{enumerate}
\end{itemize}
\begin{proof}
The set $\{ m\in \Z :m\geq x\}$ is nonempty by the Archimedean Property.  Then, as subsets of $\Z$ bounded below are well-ordered, it has a smallest element $m_0$.  Define $\floor{x}\coloneqq m_0-1$.
\begin{exr}{}{}
Show that $\floor{x}$ actually has the properties claimed.
\end{exr}
\begin{exr}{}{}
Show that $\floor{x}$ is the unique integer with these properties.
\end{exr}
\begin{exr}{}{}
Prove the analogous result for $\ceil{x}$.
\end{exr}
\end{proof}
\end{prp}

It might seem like the Archimedean Property is obviously true, but it is in fact not true of all totally-ordered fields.  To present such an example, we have to take a bit of an aside and discuss polynomial crings and fields of rational functions.  Such crings and fields are important for many other reasons than to present an example of a totally-ordered field that does not have the Archimedean Property, so it is worthwhile to understand the material anyways, though you might want to come back to it later if you don't care about the counter-example at the time being.

\subsubsection{Totally-ordered field which is not Archimedean}

\begin{dfn}{Polynomial cring}{PolynomialCring}
Let $R$ be a totally-ordered cring, denote by $R[x]$ the set of all polynomials with coefficients in $R$, let $+$ and $\cdot$ on $R[x]$ be addition and multiplication of polynomials, and define $p>0$ iff the leading coefficient of $p$ is greater than $0$ in $R$.\footnote{Recall that this is enough to define a total-order by \cref{exr1.1.41}.} 
\begin{exr}[breakable=false]{}{}
Show that $\coord{R[x],+,0,-,\cdot ,1,\leq}$ is a totally-ordered cring.  What is $0\in R[x]$?  What is $1\in R[x]$?
\end{exr}
\noindent
$R[x]$ is the \term{polynomial cring}\index{Polynomial cring} with coefficients in $R$.  The \term{degree}\index{Degree (of a polynomial)} of a polynomial $p$, denoted by $\deg (p)$\index[notation]{$\deg (p)$}, is the highest power of $x$ that appears in $p$ with nonzero coefficient.
\end{dfn}
\begin{exr}{}{}
Show that the following statements are true.
\begin{enumerate}
\item $\deg (p+q)\leq \max \{ \deg (p),\deg (q)\}$.
\item $\deg (pq)\leq \deg (p)+\deg (q)$.
\item If $R$ is integral, then $\deg (pq)=\deg (p)+\deg (q)$.
\end{enumerate}
\end{exr}
\begin{exr}{}{}
Show that $R$ is integral iff $R[x]$ is.
\end{exr}
\begin{thm}{Field of rational functions}{FieldOfRationalFunctions}
Let $F$ be a field, so that $F[x]$ is a totally-ordered integral cring.  Then, there exists unique a totally-ordered field $F(x)$, the field of \term{rational functions}\index{Rational functions} with coefficients in $F$, such that
\begin{enumerate}
\item $F[x]\subseteq F(x)$; and
\item if $F'$ is any other totally-ordered field such that $F[x]\subseteq F
$, then $F(x)\subseteq F'$.
\end{enumerate}
\begin{rmk}
Compare this to our definition of the rational numbers in \cref{RationalNumbers}.  $F(x)$ is to $F[x]$ as $\Q$ is to $\Z$.  Indeed, we mentioned there that the passage from $\Z$ to $\Q$ was an example of the more general construction of the \emph{fraction field} of an integral cring.  This is another example of this construction.  In fact, the proof of this theorem is exactly the same as the proof of \cref{RationalNumbers}, and so we refrain from presenting it again (it was an exercise anyways).
\end{rmk}
\end{thm}
Of course, we have a result from $F(x)$ which is completely analogous to the result \cref{prp1.3.4} for $\Q$ (which just says that we can write any rational function uniquely as the quotient of two relatively-prime polynomials with the denominator positive).
\begin{exm}{A non-Archimedean totally-order\-ed field}{exm2.3.12}
$\R (x)$ is a totally-ordered field, but on the other hand, $m<x$ for all $m\in \N$, and so $\R (x)$ is not Archimedean.
\end{exm}
We mentioned in a remark when defining the real numbers (\cref{RealNumbers}) that, in general, the arithmetic operations on a partially-ordered field do not extend to its Dedekind-MacNeille completion in a compatible way.  That $\R (x)$ is not Archimedean immediately tells us that this field is also a counter-example to this statement.
\begin{exm}{A totally-ordered field whose \\ Dedekind-MacNeille completion cannot be given the structure of a totally-ordered field}{exm3.2.13}
If the arithmetic operations on $\R (x)$ extended to give the structure of a totally-ordered field on its Dedekind-MacNeille completion, then, by uniqueness, its Dedekind-MacNeille completion would have to be $\R$ itself.  In particular, $\R (x)$ would have to embed into $\R$, which would imply that $\R (x)$ is Archimedean:  a contradiction.
\begin{rmk}
We noted before in a remark of \cref{thm1.4.52} that, while they real numbers are the unique nonzero \emph{Dedekind}-complete totally-ordered field, there are other nonzero \emph{Cauchy}-complete totally-ordered fields.\footnote{You aren't supposed to know what this means yet of course.  This doesn't come until \cref{Completeness}.}  $\R (x)$ (or rather, it's Cauchy-completion) will serve as a counter-example for this as well---see \cref{exm4.4.41}.
\end{rmk}
\end{exm}

\horizontalrule

The second property of the real numbers we come to is what is sometimes called the density of the rationals in the reals.  This is not quite literally true, though we will see why people refer to this as density when we discuss basic topology in the next chapter---see \cref{exr4.2.38}.
\begin{thm}{`Density' of $\Q$ in $\R$}{thm3.2.14}
Let $a,b\in \R$.  Then, if $a<b$, then there exists $c\in \Q$ such that $c\in (a,b)$.
\begin{rmk}
It turns out that this is also the case for the irrational numbers, $\Q ^{\comp}$, but at the moment, we don't even know how to construct a single irrational number, so we will have to return to this at a later date (\cref{thm3.3.76}).\footnote{You might be able to show that there is no positive rational number whose square is $2$, but can you show that there \emph{is} some positive \emph{real} number whose square is $2$?}
\end{rmk}
\begin{proof}
The intuitive idea is to take a positive integer $m$ at least as large as the length of the interval $b-a$ and break-up the real line into intervals of length $\frac{1}{m}$.  Then, one of the end-points of these intervals must lie in the interval $(a,b)$.  This will be our desired point.

Define $\varepsilon \coloneqq b-a>0$ and choose $m_0\in \N$ with $\frac{1}{m_0}<\varepsilon$ (see \cref{exr3.1.6}).  Define
\begin{equation}
S\coloneqq \left\{ m\in \N :\tfrac{m}{m_0}\geq b\right\} .
\end{equation}
By the Archimedean Property, $S$ is nonempty.  Therefore, because $\N$ is well-ordered, it has a least element, say $\frac{n_0}{m_0}$.  We claim that $\frac{n_0-1}{m_0}\in (a,b)$.

First of all, we know that $n_0-1\notin S$, and so $\frac{n_0-1}{m_0}<b$.  On the other hand,
\begin{equation}
\tfrac{n_0-1}{m_0}=\tfrac{n_0}{m_0}-\tfrac{1}{m_0}\geq b-\tfrac{1}{m_0}>b-\varepsilon =b-(b-a)=a,
\end{equation}
and so $\frac{n_0-1}{m_0}\in (a,b)$.
\end{proof}
\end{thm}

\section{Nets, sequences, and limits}

\subsection{Nets and sequences}

You probably recall sequences from calculus.
\begin{textequation}
A (real-valued) \emph{sequence} is a function from $\N$ to $\R$.
\end{textequation}
(This is not our `official' definition of a sequence.  We will present another definition later.  This is why the above has no `definition bar'.)

A net is a generalization of a sequence.  It is incredibly uncommon to introduce nets in a first course on analysis (or even in second course), but we have a couple reasons for doing so.  One point of introducing them to put emphasis on the \emph{structure} $\N$ is equipped with in this context.  In particular, the $\N$ in the definition of a sequence is \emph{not} to be thought of as a crig; for the purposes of sequences, the algebraic structure does not matter.  Instead, in the context of sequences, $\N$ should be thought of as a directed set.  Another motivation, and almost certainly the more important one, for working with nets is that, when you go to generalize to examples more exotic than the real numbers, some of the results that are true in the reals would fail to generalize if we restricted ourselves to only work with sequences---see the remarks in \cref{prp3.4.21,prp3.4.56}.  There is no getting around this:  in general topology (the subject of the next chapter), you \emph{need} to use nets.\footnote{I imagine that there will be quite a few readers who have already learned at least some general topology and have never encountered nets before.  I would further imagine that at least some of these readers would be skeptical of my claim that you ``need' nets.  There are several things I could point to you to convince you of this, but one particularly important phenomenon is as follows:  there are nonhomeomorphic spaces with the same notion of sequential convergence---see \cref{exm4.2.25}.  The significance of this is that, if you want to \emph{define} a topology by saying what it means to converge (and you almost certainly will at some point if you plan to become a mathematician), you \emph{cannot} use sequences---see the remark in \cref{KelleysConvergenceTheorem}.}
\begin{dfn}{Directed set}{dfn3.3.2}
A \term{directed set}\index{Directed set} is a nonempty partially-ordered set $\coord{\Lambda ,\leq}$ such that, for every $x_1,x_2\in X$, there is some $x_3\in X$ with $x_1,x_2\leq x_3$.
\begin{rmk}
In words, a partially-ordered set is directed iff you can find an upper-bound of any two elements.
\end{rmk}
\begin{rmk}
This property is sometimes called \term{upward-directed}\index{Upward-directed}.  Compare, for example, the terminology in \cref{FilterBase,UniformSpace}.
\end{rmk}
\end{dfn}
\begin{exr}{}{}
Show that totally-ordered sets are directed.
\begin{rmk}
In particular, $\N$, $\Z$, $\Q$, and $\R$ all all directed, as well sets derived from these, e.g.~$\R ^+$.
\end{rmk}
\end{exr}
\begin{exr}{}{exr2.4.4}
Let $X$ be a set.  Show that $\coord{2^X,\subseteq}$ is directed.
\end{exr}
\begin{exm}{A partially-ordered set that is not directed}{}
Define $X\coloneqq \{ A,B\}$ and equip $X$ with the discrete-order (\cref{exmA.1.95}).
\begin{exr}{}{}
You showed (hopefully anyways) in \cref{exmA.1.95} that $X$ is a partially-ordered set.  Check now that it is not directed.
\end{exr}
\end{exm}
\begin{dfn}{Net}{}
A (real-valued) \term{net}\index{Net} is a function from a directed set $\coord{\Lambda ,\leq}$ into $\R$.
\begin{rmk}
Similarly as with sequences, if $x\colon \Lambda \rightarrow \R$ is a net, it is customary to write $x_\lambda \coloneqq x(\lambda )$\index[notation]{$x_\lambda$}.
\end{rmk}
\begin{rmk}
Single values of the sequence $x_{\lambda}$ are referred to as \term{elements}\index{Element (of a net)} or \term{terms}\index{Term (of a net)} of the net $\lambda \mapsto x_{\lambda}$.
\end{rmk}
\begin{rmk}
It is very common that the specific directed set that is the domain is not so important and that the result works for any directed set.  As a result, in an attempt to simplify notation slightly, we frequently omit the actual domain of nets.  For example, we may just write $\lambda \mapsto x_{\lambda}\in \R$, with no mention of $\Lambda$.  You should usually be able to infer the domain on the basis of the index we happen to be using.
\end{rmk}
\begin{rmk}
Note that nets are allowed to have only finitely many terms, that is, it is permitted that $\Lambda$ itself be finite.  This is not particularly interesting, however, as the directed set axiom implies that $\Lambda$ has a maximum element, say $\lambda_{\infty}$.  In this case, the limit of any net $\Lambda \ni \lambda \mapsto x_{\lambda}$ will be $x_{\lambda _{\infty}}$, and this case is not very useful---see \cref{exr2.4.19}.
\end{rmk}
\begin{rmk}
In general, nets will take their values in topological spaces.  Of course, we haven't defined what a topological space is yet (we could, but it would probably seem like I just pulled some definition out of my ass), and so for the time being nets will take their values in $\R$.
\end{rmk}
\begin{rmk}
For those of you born and raised in Sequence Land:  Nets are a bit like the iPhone---you don't think you need them, and then you find out they're a thing.
\end{rmk}
\end{dfn}
And now we give our `official' definition of a sequence.
\begin{dfn}{Sequence}{dfn3.3.4}
A (real-valued) \term{sequence}\index{Sequence} is a net whose directed set is order-isomorphic (i.e.~isomorphic in $\Pre$) to $\coord{\N ,\leq}$.
\begin{rmk}
Typically people take a sequence to be a function from $\N$ into $\R$.  This is essentially fine, but then, technically speaking, a function from $\Z ^+$ to $\R$ is not a sequence, and you have to reindex everything by $1$ to make it a sequence.  It is easier if we just ignore this by only requiring that the domain of the net be \emph{isomorphic to} (in $\Pre$) instead of \emph{equal to} $\coord{\N ,\leq}$.
\end{rmk}
\begin{rmk}
If the name of the sequence is not important, we may simply denote it by a list of its values, e.g.~$\coord{7,-\tfrac{2}{3},2,0,\ldots }$.
\end{rmk}
\end{dfn}
\begin{exm}{A net which is not a sequence}{}
Recall that (\cref{exrA.1.26}) $2^{\R}$, the power-set of the reals, is a partially-ordered set with the order relation being inclusion, and is directed by \cref{exr2.4.4}.

Now let $f\colon \R \rightarrow \R$ be any \emph{bounded} function.  That $f$ is bounded means that
\begin{equation}
a_S\coloneqq \sup \left\{ \abs{f(x)}:x\in S\right\} \coloneqq \sup _{x\in S}\left\{ \abs{f(x)}\right\}
\end{equation}\index[notation]{$\sup _{x\in S}$}
exists for all subsets $S\in 2^{\R}$.  Therefore, the map $S\mapsto a_S$ is a net.  Without knowing yet the precise definition of convergence, do you know what the limit should be?
\end{exm}

\subsection{Limits}

We now define what it means to be the \emph{limit} of a net.  To make the definition as clean and concise as possible, we introduce the concept of nets \emph{eventually} doing something.

\subsubsection{Eventuality}\label{sss2.4.2.1}

We often use the word \emph{eventually} in the context of nets and sequences.  For example, we might say ``The net $\lambda \mapsto x_\lambda$ is eventually XYZ.'', ``XYZ'' be some sort of property.  This means that there is some $\lambda _0$ such that if $\lambda \geq \lambda _0$ it follows that $\lambda \mapsto x_\lambda$ is XYZ.  For example, a fact that will come in handy is that convergent nets (and in fact, Cauchy nets) are eventually bounded (\cref{prp3.3.28}).  We make this formal.
\begin{mdf}{Eventually XYZ}{EventuallyXYZ}
Let $\Lambda \ni \lambda \mapsto x_\lambda \in \R$ be a net.  Then, $\lambda \mapsto x_\lambda$ is \term{eventually XYZ}\index{Eventually XYZ} iff there is some $\lambda _0$ such that $\{ \lambda \in \Lambda :\lambda \geq \lambda _0\} \ni \lambda \mapsto x_\lambda$ is XYZ.
\begin{rmk}
In other words, once you `go past' $\lambda _0$, you have a net which is \emph{actually} XYZ.
\end{rmk}
\begin{rmk}
For example, the sequence $\coord{-2,-1,0,1,2,\ldots }$ is \emph{eventually} positive, but of course not always positive.
\end{rmk}
\begin{rmk}
We don't have this language yet, but nets of this form are called \emph{cofinal subnets} of the original net---see \cref{StrictSubnet}.
\end{rmk}
\end{mdf}

If a net is eventually XYZ, it can be convenient to essentially just `throw away' the terms at the beginning which are not XYZ so as to obtain a net which is not just eventually XYZ, but is XYZ \emph{itself}.  In the example above, $\coord{-2,-1,0,1,2,\ldots}$, if you `chop off' the beginning of the sequence, you obtain $\coord{1,2,\ldots}$, which of course itself is always positive.

For all intents and purposes, you should think of the `first elements' of a net as not mattering; only what \emph{eventually} happens is what matters.  For example, consider the sequence $m\mapsto x_m\coloneqq 1$, that is, the constant sequence $1\in \R$.  Obviously this should converge to $1$.  The point to note here is that, you can do \emph{whatever you like} to any finite number of elements of this sequence, and you will have no effect upon the fact that it converges to $1$.  For example, $\coord{-5,-2,10,\frac{2}{3},1,1,1,\ldots}$ should obviously still converge to $1$.  We can essentially `throw away' any finite amount of the sequence and nothing will change.  This idea can actually be quite important in proofs where we might know nothing about the first couple of elements.

There is another concept that is in a sense (\cref{prp2.4.12}) `complementary' to the concept of eventuality, namely, the concept of \emph{frequently}.
\begin{mdf}{Frequently XYZ}{FrequentlyXYZ}
Let $\Lambda \ni \lambda \mapsto x_{\lambda}\in \R$ be a net.  Then, $\lambda \mapsto x_{\lambda}$ is \term{frequently}\index{Frequently XYZ} iff for every $\lambda \in \Lambda$ there is some $\lambda '\geq \lambda$ such that $x_{\lambda '}$ is XYZ.
\begin{rmk}
In other words, no matter `how far you go', there will always still be at least one more term of the net that is XYZ.
\end{rmk}
\begin{rmk}
In particular, note that if a net is eventually XYZ, then it is frequently XYZ.
\end{rmk}
\begin{rmk}
Note that the properties applicable here in place of XYZ are \emph{not} the same as they are for eventuality.  The reason for this is that the definition of eventuality is reduced to the case of a certain `\emph{subnet}'\footnote{Whatever that mean (\cref{dfnSubnet}).} having the property XYZ, whereas here the definition of frequently is reduced to the case of a certain \emph{term} in the net having the property XYZ.  Thus, for example, it doesn't make sense to say that a net is frequently bounded---the term ``bounded'' is really meant to be applied to \emph{nets} (or subsets of $\R$), not \emph{terms}.\footnote{I suppose you could make sense of this, but then, as $\{ x_{\lambda}\}$ is always bounded (it's a singleton), you would have that every net is frequently bounded, which is a bit silly.}
\end{rmk}
\begin{rmk}
Of the two, ``eventually XYZ'' is \emph{by far} the more important.  Indeed, one of our motivations for introducing the terminology ``frequently XYZ'' is that it appears in other sources, and so will be convenient for you to know when consulting other references.  We ourselves make use of the term quite minimally.  That said, there are at least two uses worth mentioning:  being ``not eventually XYZ'' can be understood in terms of being ``frequently XYZ'' (\cref{prp2.4.12}); and if a net is ``frequently XYZ'', then the terms which are actually XYZ define a \emph{cofinal subnet} (\cref{StrictSubnet,mpr2.4.194}).
\end{rmk}
\end{mdf}

The key relationship between these two concepts is given by the following result.
\begin{mpr}{}{prp2.4.12}
Let XYZ be a property that is such that a net $\lambda \mapsto x_{\lambda}$ is XYZ iff each $x_{\lambda}$ is XYZ.  Then, a net $\lambda \mapsto x_{\lambda}\in \R$ be a net is frequently not XYZ iff it is not eventually XYZ.
\begin{rmk}
In particular, for $S\subseteq \R$, a net is frequently contained in $S$ iff it is not eventually contained in $S^{\comp}$.
\end{rmk}
\begin{proof}
$\Lambda \ni \lambda \mapsto x_{\lambda}$ is frequently not XYZ iff for every $\lambda$ there is some $\lambda '\geq \lambda$ such that $x_{\lambda '}$ is not XYZ iff for every $\lambda$ it is not the case that for every $\lambda '\geq \lambda$ that $x_{\lambda '}$ is XYZ iff for every $\lambda$ it is not the case that $\{ \lambda '\in \Lambda :\lambda '\geq \lambda \} \ni \lambda '\mapsto x_{\lambda '}$ is XYZ iff it is not the case that there is some $\lambda$ such that $\{ \lambda '\in \Lambda :\lambda '\geq \lambda \} \ni \lambda '\mapsto x_{\lambda '}$ is XYZ iff it is not the case that $\lambda \mapsto x_{\lambda}$ is eventually XYZ.
\end{proof}
\end{mpr}

\horizontalrule

Now with the concept of eventuality in hand, we present the definition of convergence.
\begin{dfn}{Limit (of a net)}{dfn3.3.8}
Let $\lambda \mapsto x_\lambda$ be a net and let $x_\infty\in \R$.  Then, $x_\infty$ is the \term{limit}\index{Limit (of a net)} of $\lambda \mapsto x_\lambda$ iff for every $\varepsilon >0$, $\lambda \mapsto x_\lambda$ is eventually contained in $B_\varepsilon (x_\infty)$.   If a net has a limit, then we say that it \term{converges}\index{Convergence (of net (in $\R$))}.\footnote{Divergence is not the same as nonconvergence.  We will define divergence momentarily.}
\begin{rmk}
Note the use of the term \emph{eventually}.  Explicitly, this is equivalent to
\begin{textequation}
$x_\infty$ is the limit of $\lambda \mapsto x_\lambda$ iff for every $\varepsilon >0$ there is some $\lambda _0$ such that, whenever $\lambda \geq \lambda _0$, it follows that $x_\lambda \in B_{\varepsilon}(x_\infty)$.
\end{textequation}
Of course, when actually doing proofs, you will often need to make use of the explicit definition, but I think it's fair to say that the definition that uses the term ``eventually'' is both more intuitive and concise.
\end{rmk}
\begin{exr}[breakable=false]{Limits are unique (in $\R$)}{}
Let $x_\infty,x'_\infty\in \R$ be limits of the net $\lambda \mapsto x_\lambda$.  Show that $x_\infty=x'_\infty$.
\begin{rmk}
In general topological spaces (though not for most `reasonable' ones), limits need not be unique (hence the reason for adding ``in $\R$'').  In fact, limits are unique iff the space is $T_2$---see \cref{prp4.5.37}.
\end{rmk}
\end{exr}
\begin{rmk}
If $x_\infty$ is the limit of $\lambda \mapsto x_\lambda$, then we write $\lim _\lambda x_\lambda =x_\infty$\index[notation]{$\lim _\lambda x_\lambda$}, or sometimes even just $\lim x_{\lambda}=x_{\infty}$\index[notation]{$\lim x_{\lambda}=x_{\infty}$}.  Note that this is unambiguous by the previous exercise.
\end{rmk}
\begin{rmk}
There are at least three directed sets that are commonly the domains of nets in calculus:  $\N$, $\coord{\R ^+,\leq}$, and $\coord{\R ^+,\geq}$, that is, the natural numbers with the usual ordering, the positive reals with the usual ordering, and the positive reals with the \emph{reverse} of the usual ordering.  We shall denote limits of nets with these respective domains as
\begin{equation}
\lim _mx_m,\qquad \lim _{t\to \infty}x_t,\qquad \lim _{t\to 0^+}x_t.
\end{equation}\index[notation]{$\lim _mx_m$}\index[notation]{$\lim _{t\to \infty}x_t$}\index[notation]{$\lim _{t\to 0^+}x_t$}
\end{rmk}
\end{dfn}
\begin{prp}{}{}
Let $\lambda \mapsto x_{\lambda}$ be a net and let $x_{\infty}\in \R$.  Then, it is \emph{not} the case that $\lambda \mapsto x_{\lambda}$ converges to $x_{\infty}$ iff there is some $\varepsilon _0>0$ such that $\lambda \mapsto x_{\lambda}$ is frequently contained in $B_{\varepsilon _0}(x_{\infty})^{\comp}$.
\begin{proof}
By \cref{prp2.4.12} and the definition of convergence, it is not the case that $\lambda \mapsto x_{\lambda}$ converges to $x_{\infty}$ iff it is not the case that for every $\varepsilon >0$, $\lambda \mapsto x_{\lambda}$ is eventually contained in $B_{\varepsilon}(x_{\infty})$ iff there is some $\varepsilon _0>0$ such that it is not the case that $\lambda \mapsto x_{\lambda}$ is eventually contained in $B_{\varepsilon _0}(x_{\infty})$ iff there is some $\varepsilon _0>0$ such that $\lambda \mapsto x_{\lambda}$ is frequently contained in $B_{\varepsilon _0}(x_{\infty})$.
\end{proof}
\end{prp}
\begin{exr}{}{exr2.4.19}
Let $\Lambda \ni \lambda \mapsto x_{\lambda}\in \R$.  Show that if $\Lambda$ is finite, then (i) $\Lambda$ has a maximum element $\lambda _{\infty}$ and (ii) that $\lim _{\lambda}x_{\lambda}=x_{\lambda _{\infty}}$.
\end{exr}
\begin{exr}{}{}
Let $x_0\in \R$ and define $\lambda \mapsto x_\lambda \coloneqq x_0$.  Show that $\lim _\lambda x_\lambda =x_0$.
\begin{rmk}
That is, constant nets converge to that constant.  While trivial, it is significant in that it becomes an axiom of the convergence definition of a topological space---see \cref{KelleysConvergenceTheorem}.
\end{rmk}
\end{exr}
\begin{exr}{}{}
Show that $\lim _m\frac{1}{m}=0$.
\end{exr}
\begin{prp}{}{prp2.4.16}
Let $\abs{a}<1$.  Then, $\lim _ma^m=0$.
\begin{rmk}
For the first time in these notes, we encounter (in the proof) what is known as \emph{sigma notation}.  You likely already know what this is, but it should probably be included somewhere for the sake of completeness, and perhaps more importantly, it makes sense to introduce alongside sigma notation something which is less commonly known:  \emph{pi notation}.

In a general rg $\coord{R,+,0,\cdot}$ (\cref{Rg}), if $\{ x_m,\ldots ,x_n\} \subseteq R$ is a finite subset, then we write
\begin{equation}\label{eqn2.4.23}
\begin{split}
\MoveEqLeft
\sum _{\{ x_m,\ldots ,x_n\}}x_k\ceqq \sum _{k=m}^nx_k \\
& \ceqq x_m+x_{m+1}+\cdots +x_{n-1}+x_n.
\end{split}
\end{equation}\index[notation]{$\sum _{k=m}^nx_k$}
Similarly, we write
\intomargin
\begin{equation}\label{eqn2.4.24}
\begin{split}
\prod _{\{ x_m,\ldots ,x_n\}}x_k & \ceqq \prod _{k=m}^nx_k \\
& \ceqq x_m\cdot x_{m+1}\cdot \cdots \cdot x_{n-1}\cdot x_n.\footnote{Note that these definitions implicitly make use of associativity of the binary operations.  (Otherwise, does $\prod _{k=1}^3x_k$ mean $(x_1\cdot x_2)\cdot x_3$ or $x_1\cdot (x_2\cdot x_3)$?).}
\end{split}
\end{equation}\index[notation]{$\prod _{k=m}^nx_k$}
The empty sum ($n<m$) is defined to be $0$, and for rigs the empty product ($n<m$) is defined to be $1$.

The definition in \eqref{eqn2.4.23} is \term{sigma notation}\index{Sigma notation} and the definition in \eqref{eqn2.4.24} is \term{pi notation}\index{Pi notation}.\footnote{``$\Sigma$'' is for ``sum'' and ``$\Pi$'' is for ``product''.}
\end{rmk}
\begin{proof}
Define $b\coloneqq \frac{1}{\abs{a}}$.  Let $M>0$.  We show that $m\mapsto b^m$ is eventually larger than $M$.  It will then follows that $m\mapsto \abs{a}^m$ is eventually smaller than $\frac{1}{M}$, or in other words, $m\mapsto \abs{a}^m$ is eventually contained in $B_{\frac{1}{M}}(0)$.  As $\frac{1}{M}$ is just as arbitrary as $\varepsilon >0$, this will show that $\lim _ma^m=0$.

We have
\begin{equation*}
b^m=(1+(b-1))^m=\sum _{k=0}^m\binom{m}{k}(b-1)^k\geq 1+m(b-1).\footnote{$\binom{m}{k}$ is the $\coord{m,k}$ \term{binomial coefficient} and is defined by $\binom{m}{k}\ceqq \frac{m(m-1)(m-2)\cdots (m-(k+1))}{\prod _{j=1}^kj}$---see \cref{BinomialCoefficient} for elaboration if you need it.}
\end{equation*}
Because $b-1>0$, it follows from the Archimedean Property (\cref{thm3.2.3})\footnote{Choose $m\in \N$ to be strictly larger than $\frac{M-1}{b-1}$, so that $1+m(b-1)>M$.} that $m\mapsto b^m$ is eventually larger than $M$, which completes the proof.
\end{proof}
\begin{rmk}
Note the phrase ``just as arbitrary''.  You will see arguments, and ones like it, quite frequently, so let us take the time at least once to spell out exactly what's happening.

In this case, to show that $\lim _ma^m=0$, we want to show that, for every $\varepsilon >0$, $m\mapsto \abs{a}^m$ is eventually less than $\varepsilon >0$.  Instead, what I do, is I prove that for every $M>0$, $m\mapsto \abs{a}^m$ is eventually less than $\frac{1}{M}$.  So, suppose I have done this, and let $\varepsilon >0$.  Define $M\coloneqq \frac{1}{\varepsilon}$.  Then, $m\mapsto \abs{a}^m$ is eventually less than $\frac{1}{M}=\varepsilon$.  That is, if I have proven the statement involving $M$, then I have proven the statement involving $\varepsilon$ because $\frac{1}{M}$ is ``just as arbitrary'' as $\varepsilon$.

I could quite possibly be over-explaining this to the point of being unhelpful.  Thus, if you thought you understood before, but this remark confused you, don't worry about it---you can just move on.

Finally, I explain a similar argument once again at the end of the proof of \cref{CompletenessOfR}, so feel free to glance there now if you like.
\end{rmk}
\end{prp}

\begin{dfn}{Divergence}{Divergence}
Let $\lambda \mapsto x_\lambda$ be a net.  Then, $\lambda \mapsto x_\lambda$ \term{diverges to $+\infty$}\index{Diverges to $\pm \infty$} iff for every $M>0$, $\lambda \mapsto x_{\lambda}$ is eventually larger than $M$.

Similarly for $-\infty$.\footnote{You should consider writing this definition out explicitly yourself.}

$\lambda \mapsto x_\lambda$ \term{diverges}\index{Diverges} iff $\lambda \mapsto \abs{x_{\lambda}}$ diverges.
\begin{rmk}
Of course, the intuition is just that $x_\lambda$ grows arbitrarily large.
\end{rmk}
\begin{rmk}
Note the use of the term \emph{eventually}.  Explicitly, this is equivalent to
\begin{textequation}
$\lambda \mapsto x_{\lambda}$ diverges to $+\infty$ iff for every $M>0$ there is some $\lambda _0$ such that, whenever $\lambda \geq \lambda _0$, it follows that $x_{\lambda}\geq M$.
\end{textequation}
\end{rmk}
\begin{rmk}
Note that this terminology is slightly nonstandard.  It is common for authors to take ``diverge'' to be ``doesn't converge''; however, I prefer having the more refined terminology ``converges'', ``diverges to $\pm \infty$'', ``diverges'', and ``doesn't converge''.
\end{rmk}
\end{dfn}
\begin{exr}{}{}
\begin{enumerate}
\item What is an example of a net which neither converges nor diverges?
\item What is an example of a net which diverges, but doesn't diverge to either $\pm \infty$.
\end{enumerate}
\end{exr}

The biggest problem with proving that a net converges is that we first need to know what the limit is before hand.  For example, consider
\begin{equation}\label{3.2.16}
\lim _m\sum _{k=0}^m\tfrac{1}{k!}.\footnotemark
\end{equation}\footnotetext{In case you're wondering what this crazy new symbol ``$!$'' means, for $n\in \N$, $n!$, the \term{factorial}\index{Factorial} of $n$, is defined inductively by $0!\coloneqq 1$ and $(n+1)!\coloneqq (n+1)\cdot n!$.}
You probably recall that this \emph{should} converge to $\e$, but what the hell is $\e$?  It has not yet been defined.  In fact, we will later define
\begin{equation}
\e \coloneqq \lim _m\sum _{k=0}^m\tfrac{1}{k!},\footnotemark
\end{equation}\footnotetext{Actually we will define $\e \coloneqq \exp (1)$, but this will of course turn out to be exactly this limit.}
but we cannot even make this definition if we don't know a priori that the limit of \eqref{3.2.16} exists!  Thus, we need to have a way of showing that \eqref{3.2.16} exists without making explicit reference to $\e$.  The concepts of \emph{Cauchyness} and \emph{completeness} (in the sense of uniform spaces, as opposed to in the sense of preordered sets) allow us to do this.

\subsection{Cauchyness and completeness}\label{sbs3.3.3}

Like with the concept of convergence and limits, we will first define what it means to be Cauchy, and then explain the intuition behind the definition.
\begin{dfn}{Cauchyness}{dfn3.3.26}
Let $\lambda \mapsto x_\lambda$ be a net.  Then, $\lambda \mapsto x_\lambda$ is \term{Cauchy}\index{Cauchy} iff for every $\varepsilon >0$, $\lambda \mapsto x_\lambda$ is eventually contained in some $\varepsilon$-ball.
\begin{rmk}
Note the use of the term \emph{eventually}.  Similarly as before, explicitly, this means that
\begin{textequation}
$\lambda \mapsto x_\lambda$ is Cauchy iff there is some $\lambda _0$ and some $\varepsilon$-ball $B$ such that, whenever $\lambda \geq \lambda _0$, it follows that $x_\lambda \in B$.
\end{textequation}
\end{rmk}
\end{dfn}
You should compare this with the definition of a limit, \cref{dfn3.3.8}.  The only essential difference between the definition of convergence and the definition of Cauchy is that, in the former case, there is a \emph{single} center of the $\varepsilon$-ball that works for all $\varepsilon$, whereas, in the case of Cauchyness, the center of the ball can vary with the choice of $\varepsilon$.  Thus, we immediately have the implication:
\begin{exr}{}{exr3.3.27}
Let $\lambda \mapsto x_\lambda$ be a convergent net.  Show that $\lambda \mapsto x_\lambda$ is Cauchy.
\end{exr}

As a short aside, we note that this is not how the definition of Cauchyness is usually stated.  Instead, the following equivalent condition is given as the definition.
\begin{exr}{}{exr3.3.28}
Let $\lambda \mapsto x_\lambda$ be a net.  Show that $\lambda \mapsto x_\lambda$ is Cauchy iff for all $\varepsilon >0$ there is some $\lambda _0$ such that whenever $\lambda _1,\lambda _2\geq \lambda _0$ it follows that $\abs{x_{\lambda _1}-x_{\lambda _2}}<\varepsilon$.
\end{exr}
We choose to use the \cref{dfn3.3.26} because (i)~it more closely resembles the definition of convergence and (ii)~it more closely resembles the definition in the more general case of uniform spaces (see \cref{Cauchyness}).  That being said, the equivalent condition in this exercise (\cref{exr3.3.28}) is frequently easier to check.  For example:
\begin{exm}{}{exm3.3.39}
In this example, we check that the sequence
\begin{equation}
m\mapsto s_m\coloneqq \sum _{k=0}^m\tfrac{1}{k!}
\end{equation}
is Cauchy.  (Note that $s_m\in \Q$, so in fact, this sequence is Cauchy in $\Q$ as well as in $\R$.)
\begin{equation}
\begin{split}
\abs{s_{m_1}-s_{m_2
}} & =\left| \sum _{k=0}^{m_1}\tfrac{1}{k!}-\sum _{k=0}^{m_2}\tfrac{1}{k!}\right| =\sum _{k=m_1+1}^{m_2}\tfrac{1}{k!} \\
& \leq \sum _{k=m_1+1}^{m_2}\tfrac{1}{2^k}=\frac{2^{m_2}-2^{m_1}}{2^{m_1+m_2}}\leq 2^{-m_1}
\end{split}
\end{equation}
where we have without loss of generality assumed that $3\leq m_1\leq m_2$ (because $2^k\leq k!$ for $k\geq 4$).

We use this to check the condition of \cref{exr3.3.28}.  So, let $\varepsilon >0$.  By \cref{prp2.4.16}, $\lim _m2^{-m}=0$, and so there is some $m_0\in \N$ such that, whenever $m\geq m_0$, it follows that $2^{-m}=\abs{2^{-m}-0}<\varepsilon$.  Hence, whenever $m_1,m_2\geq m_0$, it follows that
\begin{equation}
\abs{s_{m_1}-s_{m_2}}\leq 2^{-m_1}<\varepsilon .
\end{equation}
\end{exm}

One way to immediately tell if a net is not Cauchy is if it is eventually unbounded. 
\begin{prp}{}{prp3.3.28}
Let $\lambda \mapsto x_\lambda$ be a Cauchy net.  Then, $\lambda \mapsto x_\lambda$ is eventually bounded.  In particular, if this net is a sequence, the set $\left\{ x_m :m\in \N \right\}$ is bounded.
\begin{proof}
Applying the definition to $\varepsilon \coloneqq 1$, we deduce that there must be some $\lambda _0$ and some open ball $B$ of radius $1$ such that, whenever $\lambda \geq \lambda _0$, it follows that $x_\lambda \in B$.  Thus, $\lambda \mapsto x_\lambda$ is eventually in $B$, a bounded subset of $\R$, and hence itself is eventually bounded.

Now assume that $m\mapsto x_m$ is a sequence so that $m\in \N$.  Let us also write $m_0\coloneqq \lambda _0$.  The key to note here is that, unlike in the general case, the set $\{ m\in \N :m<m_0\}$ is \emph{finite}.  This enables us to make the definition
\begin{equation}
r_0\coloneqq \max \left\{ \abs{x_0}|,\ldots ,\abs{x_{m_0-1}},1+\abs{x_0}\right\} ,
\end{equation}
where $x_0$ is the center of the ball $B$.  The reason for the $1+|x_0|$ is as follows.  For $m\geq m_0$, we have that
\begin{equation}
\begin{split}
\abs{x_m} & =\abs{(x_m-x_0)+x_0}\leq \abs{x_m-x_0}+\abs{x_0} \\
& \leq 1+\abs{x_0}.
\end{split}
\end{equation}
From this, it follows that $\left\{ x_m:m\in \N ]\right\} \subseteq B_{r_0}(x_\infty)$, so that $\left\{ x_m:m\in \N \right\}$ is bounded.
\end{proof}
\end{prp}

\subsubsection{Algebraic and Order Limit Theorems}

Ideally, the content in this subsubsection would have been discussed very soon after defining a limit itself, however, we will use the fact that convergent nets are eventually bounded.  We could have proved this, then done the Algebraic and Order Limit Theorems, but when we would have wound up essentially proving the same theorem twice (because Cauchy nets are eventually bounded as well).  We decided to just postpone these results to a slightly less-than-maximally-coherent location in the notes instead.
\begin{prp}{Algebraic Limit Theorems}{AlgebraicLimitTheorems}\index{Algebraic Limit Theorems}
Let $\lambda \mapsto x_\lambda$ and $\lambda \mapsto y_\lambda$ be two convergent nets.\footnote{The common index is supposed to indicate that the two nets have the same domain.}  Then,
\begin{enumerate}
\item \label{enmAlgebraicLimitTheorems.i}$\lim (x_\lambda +y_\lambda )=\lim _{\lambda}x_{\lambda}+\lim _{\lambda}y_{\lambda}$;
\item \label{enmAlgebraicLimitTheorems.ii}$\lim (x_\lambda y_\lambda )=\left( \lim _{\lambda}x_{\lambda}\right) \left( \lim _{\lambda}y_{\lambda}\right)$;
\item \label{enmAlgebraicLimitTheorems.iii}$\lim \frac{1}{x_\lambda}=\frac{1}{\lim _{\lambda}x_{\lambda}}$ if $\lim _{\lambda}x_{\lambda}\neq 0$; and
\item \label{enmAlgebraicLimitTheorems.iv}$\lim (\alpha x_\lambda )=\alpha \lim _{\lambda}x_{\lambda}$ for $\alpha \in \R$.
\end{enumerate}
\begin{rmk}
Later, we will see how all of this follow automatically from continuity considerations (because $+$ is continuous, for example).
\end{rmk}
\begin{proof}
We leave \cref{enmAlgebraicLimitTheorems.i}, \cref{enmAlgebraicLimitTheorems.ii}, and \cref{enmAlgebraicLimitTheorems.iv} as exercises.  Should you need guidance, check out our proof of \cref{enmAlgebraicLimitTheorems.iii}.
\begin{exr}[breakable=false]{}{}
Prove \cref{enmAlgebraicLimitTheorems.i}.
\end{exr}
\begin{exr}[breakable=false]{}{}
Prove \cref{enmAlgebraicLimitTheorems.ii}.
\end{exr}
We prove \cref{enmAlgebraicLimitTheorems.iii}.  It is likely the most difficult, and if you can follow this, you should be able to do the others on your own.

Define $x_{\infty}\ceqq \lim _{\lambda}x_{\lambda}$.  Suppose that $x_{\infty}\neq 0$.  Without loss of generality, assume that $x_{\infty}>0$.  As $\lambda \mapsto x_{\lambda}$ converge to $x_{\infty}>0$, eventually it must be the case that $\lambda \mapsto x_{\lambda}$ is positive.  We now prove this claim in more detail.  Define $\varepsilon \coloneqq \frac{1}{2}\abs{x_\infty}=\frac{1}{2}x_\infty$.  Then, there is some $\lambda _0$ such that, whenever $\lambda \geq \lambda _0$, it follows that
\begin{equation}
x_\infty -x_\lambda \leq \abs{x_\lambda -x_\infty}<\varepsilon \coloneqq \frac{1}{2}x_\infty .
\end{equation}
It follows that, for $\lambda \geq \lambda _0$,
\begin{equation}
x_\lambda >x_\infty -\tfrac{1}{2}x_\infty =\tfrac{1}{2}x_\infty .
\end{equation}

As $\lambda \mapsto x_\lambda$ is eventually positive, we may without loss of generality assume that there there is some $M>0$ such that $x_\lambda ,x_{\infty}\geq M$ \emph{for all} $\lambda$.\footnote{Here we are making use of the trick we mentioned before about `throwing away' the beginning terms of a net---see the end of \nameref{sss2.4.2.1}.}

Now, let $\varepsilon >0$ and, redefining notation,\footnote{That is, forget our previous definition of $\lambda _0$.} choose $\lambda _0$ such that, whenever $\lambda \geq \lambda _0$, it follows that $\abs{x_\lambda -x_\infty}<\varepsilon$.  Then, for $\lambda \geq \lambda _0$,
\begin{equation}
\abs{x_\lambda ^{-1}-x_\infty ^{-1}}=\frac{\abs{x_\infty -x_\lambda}}{\abs{x_\lambda x_\infty}}<\tfrac{1}{M^2}\abs{x_\lambda -x_\infty}<\tfrac{1}{M^2}\varepsilon .
\end{equation}
As $\frac{1}{M^2}\varepsilon$ is just as arbitrary as $\varepsilon$ (this uses the fact that $M$ does not depend on $\varepsilon$), this completes the proof.
\begin{exr}[breakable=false]{}{}
Prove \cref{enmAlgebraicLimitTheorems.iv}.
\end{exr}
\end{proof}
\end{prp}
\begin{exr}{Order Limit Theorem}{exr3.3.30}\index{Order Limit Theorem}
Let $\lambda \mapsto x_\lambda$ and $\lambda \mapsto b_\lambda$ be two convergent nets.  Show that if it is eventually the case that $x_\lambda \leq y_\lambda$, then $\lim _\lambda x_\lambda \leq \lim _\lambda y_\lambda$.
\begin{rmk}
Of course, it is also true if the limits are $\pm \infty$.
\end{rmk}
\begin{rmk}
Sometimes this (or really, I suppose, an immediate corollary of this) is called the \term{Squeeze Theorem}\index{Squeeze Theorem}.
\end{rmk}
\end{exr}
\begin{exr}{Squeeze Theorem}{SqueezeTheorem}
Let $\lambda \mapsto x_{\lambda}$, $\lambda \mapsto y_{\lambda}$, and $\lambda \mapsto z_{\lambda}$ be nets.  Show that if (i)~it is eventually the case that $x_{\lambda}\leq y_{\lambda}\leq z_{\lambda}$ and (ii)~$\lim _{\lambda}x_{\lambda}=\lim _{\lambda}z_{\lambda}$, then $\lambda \mapsto y_{\lambda}$ is convergent, and $\lim _{\lambda}x_{\lambda}=\lim _{\lambda}y_{\lambda}=\lim _{\lambda}z_{\lambda}$.
\begin{rmk}
The \nameref{exr3.3.30} tells us that, \emph{if} $\lim _{\lambda}y_{\lambda}$ exists, then it must be equal to the common value $\lim _{\lambda}x_{\lambda}=\lim _{\lambda}z_{\lambda}$.  The Squeeze Theorem tells us furthermore that this limit does in fact exist.
\end{rmk}
\end{exr}

\horizontalrule

You (hopefully) just showed in \cref{exr3.3.27}, convergent sequences are always Cauchy.  However, it is in general not the case that every Cauchy sequence converges.  For example, once we show that the definition of $\e \coloneqq \lim _m\sum _{k=0}^m\frac{1}{k!}$ makes sense (we're about to) and that $\e$ is irrational\footnote{Unfortunately this won't come for awhile---see \cref{thm6.4.107}.}, then this will serve as an example of a sequence that is Cauchy in $\Q$ (as we just showed) but does not converge in $\Q$ (because $\e$ is irrational).  However, it \emph{is} true that every Cauchy sequence does converge in $\R$, and in fact, you might say this is the real reason we care about $\R$ at all and that the motivation for requiring the existence of least upper-bounds was so that we could prove this.  Before we actually prove this, however, we first need to discuss limit superiors and limit inferiors.

\subsubsection{Limit superiors and limit inferiors}

The Monotone Convergence Theorem is the tool that will allow us to define $\limsup$ and $\liminf$.
\begin{prp}{Monotone Convergence Theorem}{MonotoneConvergenceTheorem}\index{Monotone Convergence Theorem}
Let $\lambda \mapsto x_{\lambda}$ be a net.  Then,
\begin{enumerate}
\item if $\lambda \mapsto x_{\lambda}$ is nondecreasing, then $\lim _{\lambda}x_{\lambda}=\sup \{ x_{\lambda}:\lambda \}$; and
\item if $\lambda \mapsto x_{\lambda}$ is nonincreasing, then $\lim _{\lambda}x_{\lambda}=\inf \{ x_{\lambda}:\lambda \}$.
\end{enumerate}
\begin{rmk}
Note that we allow here the limits to be $\pm \infty$.  Thus, nondecreasing and nonincreasing alone is not enough to guarantee convergence, however, from this it follows that, in the nondecreasing case, being bounded above, and in the nonincreasing case, being bounded below, is sufficient to guarantee convergence.
\end{rmk}
\begin{rmk}
If you add ``eventually'' in front of ``nondecreasing'' and ``bounded above'', this is still sufficient to guarantee convergence, but you no longer have equality of the limit with the supremum (and similarly for the nonincreasing case).
\end{rmk}
\begin{proof}
We just do the case where $\lambda \mapsto x_\lambda$ is nondecreasing.

First, suppose that the net is not bounded above, so that $\sup _{\lambda}\{ x_{\lambda}\} =+\infty$.  We wish to show that $\lim _{\lambda}x_{\lambda}=+\infty$.  So, let $M>0$.  As the net is not bounded above, there is some $\lambda _0$ such that $x_{\lambda _0}\geq M$.  But then, whenever $\lambda \geq \lambda _0$, as the net is nondecreasing, we have $M\leq x_{\lambda _0}\leq x_{\lambda}$.  Thus, by definition, we have that $\lim _{\lambda}x_{\lambda}=+\infty$.

Now suppose that the net is bounded above.  We may then define
\begin{equation}
x_\infty \coloneqq \sup _{\lambda}\{ x_\lambda \} \in \R .
\end{equation}
We want to show that $\lim _\lambda x_\lambda =x_\infty$.

So, let $\varepsilon >0$.  By \cref{prp1.4.11}, there is some $x_{\lambda _0}$ such that
\begin{equation}
x_\infty-\varepsilon <x_{\lambda _0}\leq x_\infty.
\end{equation}
Then, by monotonicity, whenever $\lambda \geq \lambda _0$, we have that
\begin{equation}
x_\infty-\varepsilon <x_{\lambda _0}\leq x_\lambda \leq x_\infty.
\end{equation}
This, however, implies that $x_\lambda \in B_{\varepsilon}(x_\infty )$, so that, by definition, $\lim _\lambda x_\lambda =x_\infty$.
\end{proof}
\end{prp}
\begin{dfn}{Limit superior and limit inferior}{}
Let $x\colon \Lambda \rightarrow \R$ be a net and for each $\lambda _0\in \Lambda$ define
\begin{equation}\label{3.3.48}
u_{\lambda _0}\coloneqq \sup _{\lambda \geq \lambda _0}\{ x_\lambda \} \text{ and }l_{\lambda _0}\coloneqq \inf _{\lambda \geq \lambda _0}\{ x_\lambda \} .
\end{equation}
\begin{exr}[breakable=false]{}{}
Check that $\lambda \mapsto u_\lambda$ is nonincreasing and that $\lambda \mapsto l_\lambda$ is nondecreasing.
\end{exr}
Then, the \term{limit superior}\index{Limit superior} and the \term{limit inferior}\index{Limit inferior} of $\lambda \mapsto x_{\lambda}$, $\limsup _{\lambda}x_{\lambda}$ and $\liminf _{\lambda}x_{\lambda}$ respectively, are defined by
\begin{subequations}\label{3.3.50}
\begin{align}
\limsup x_\lambda & \coloneqq \lim _\lambda u_\lambda \\
\liminf x_\lambda &\coloneqq \lim _\lambda l_\lambda .
\end{align}
\end{subequations}\index[notation]{$\limsup _\lambda x_\lambda$}\index[notation]{$\liminf _\lambda x_\lambda$}
\begin{rmk}
Note that it is the Monotone Convergence Theorem which guarantees that this definition makes sense.  In particular, unlike limits themselves, $\limsup$ and $\liminf$ \emph{always} make sense (though they may be $\pm \infty$).
\end{rmk}
\begin{rmk}
The intuition is that $\lambda \mapsto u_\lambda$ is the net of `eventual upper bounds'.  $\limsup _\lambda x_\lambda$ is then the limit of this net (similarly for $\liminf$).
\end{rmk}
\end{dfn}
\begin{exr}{}{}
Let $\lambda \mapsto x_\lambda$ be a net.  Show one of the following two statements.
\begin{enumerate}
\item $\limsup _\lambda x_\lambda$ is finite iff $\lambda \mapsto x_\lambda$ is eventually bounded above.
\item $\liminf _\lambda x_\lambda$ is finite iff $\lambda \mapsto x_\lambda$ is eventually bounded below.
\end{enumerate}
\begin{rmk}
Both statements are true of course, but the proofs will be so similar that there is not really much point in asking you to write both down.
\end{rmk}
\end{exr}
\begin{exr}{}{exr3.3.50}
Let $\lambda \mapsto x_\lambda$ be a net.  Show that $\liminf _\lambda x_\lambda \leq \limsup _\lambda x_\lambda$.
\begin{rmk}
Note that if we have equality at a finite value, then, by the \namerefpcref{SqueezeTheorem}, the net converges to the common value.  In fact, the converse is also true---see the following proposition (\cref{prp3.3.52}).
\end{rmk}
\end{exr}
\begin{prp}{}{prp3.3.52}
Let $\lambda \mapsto x_\lambda$ be a net and let $x_{\infty}\in [-\infty ,\infty ]$.  Then, $\lim _{\lambda}x_\lambda =x_{\infty}$ iff $\limsup _\lambda x_\lambda =x_{\infty}=\liminf _\lambda x_\lambda$.
\begin{proof}
$(\Rightarrow )$ Suppose that $\lambda \mapsto x_\lambda =x_{\infty}$ converges.  First suppose that $x_{\infty}$ is finite.  Then, it is eventually bounded, and so both $\limsup _\lambda x_\lambda$ and $\liminf _\lambda x_\lambda$ are finite.  Define $u\coloneqq \limsup _\lambda x_\lambda$ and $l\coloneqq \liminf _\lambda x_\lambda$.  Let $\varepsilon >0$ and choose $\lambda _0$ such that, whenever $\lambda \geq \lambda _0$, it follows that
\begin{equation}
\sup _{\mu \geq \lambda}\{ x_\mu \} -u<\varepsilon \text{ and }l-\inf _{\mu \geq \lambda}\{ x_\mu \} <\varepsilon .
\end{equation}
Adding these two inequality, we find that, for $\lambda \geq \lambda _0$,
\begin{equation}
l-u<2\varepsilon +\left( \inf _{\mu\geq \lambda}\{ x_\mu \} -\sup _{\mu \geq \lambda}\{ x_\mu \}\right) .
\end{equation}
Now define $x_\infty \coloneqq \lim _\lambda x_\lambda$ and choose $\lambda _0'$ such that, whenever $\lambda \geq \lambda _0'$, we have that $\abs{x_\lambda -x_\infty}<\varepsilon$.  In other words,
\begin{equation}
x_\lambda -x_\infty <\varepsilon \text{ and }x_\infty -x_\lambda <\varepsilon .
\end{equation}
Taking the $\inf$ of this inequality (and using the fact that $\inf (-S)=-\sup (S)$---see \cref{exr1.4.66}), we find
\begin{equation}
\inf _{\mu \geq \lambda}\{ x_\mu \} <\varepsilon +x_\infty \text{ and }-\sup _{\mu \geq \lambda}\{ x_\mu \} <\varepsilon -x_\infty .
\end{equation}
Pick something larger than both $\lambda _0$ and $\lambda _0'$ and change notation so that this new larger thing is called $\lambda _0$.  Thus, now, for $\lambda \geq \lambda _0$, both sets of inequalities hold, and so
\begin{equation}
l-u<2\varepsilon +\left( (\varepsilon +x_\infty) +(\varepsilon -x_\infty )\right) =4\varepsilon .
\end{equation}
As $\varepsilon$ was arbitrary, we have $l=u$.

Now consider the case $x_{\infty}=\pm \infty$.  As the two cases are essentially the same, we prove this only in the case $x_{\infty}=+\infty$.  As $\liminf _{\lambda}x_{\lambda}\leq \limsup _{\lambda}x_{\lambda}$, it suffices to show that $\liminf _{\lambda}x_{\lambda}=\infty$.  Let $M>0$.  Then, there is some $\lambda _0$ such that, whenever $\lambda \geq \lambda _0$, it follows that $x_{\lambda}\geq M$.  Hence, $\inf _{\mu \geq \lambda _0}\{ x_{\mu }\} \geq M$.  Hence, whenever $\lambda \geq \lambda _0$, it follows that
\begin{equation}
\inf _{\mu \geq \lambda}\{ x_{\mu}\} \geq \inf _{\mu \geq \lambda _0}\{ x_{\mu}\} \geq M.
\end{equation}
It follows that, by definition (\cref{Divergence}), $\liminf _{\lambda}x_{\lambda}=\infty$.

\blankline
\noindent
$(\Leftarrow )$ Suppose that $\limsup _\lambda x_\lambda =x_{\infty}=\liminf _\lambda x_\lambda$.  Then, as
\begin{equation}
\inf _{\mu \geq \lambda}\{ x_{\mu}\} \leq x_{\lambda}\leq \sup _{\mu \geq \lambda}\{ x_{\mu}\} ,
\end{equation}
the \nameref{SqueezeTheorem} implies that $\lim _{\lambda}x_{\lambda}=x_{\infty}$.
\end{proof}
\end{prp}
\begin{exm}{}{}
Note that we can have equality at an \emph{infinite} value.  For example, consider the sequence $m\mapsto x_m\coloneqq m$.  This is obviously not bounded above, and so by definition $\limsup _mx_m=\infty$.  On the other hand, $\inf _{m\geq m_0}\{ x_m\} =m_0$, and so $\liminf _mx_m=\infty$ as well.
\end{exm}

\horizontalrule

Now we can finally return to our current goal of proving that Cauchy nets converge in $\R$.
\begin{thm}{Completeness of $\R$}{CompletenessOfR}
Let $\lambda \mapsto x_\lambda$ be a Cauchy net.  Then, $\lambda \mapsto x_\lambda$ converges.
\begin{rmk}
This property of having all Cauchy nets converge is known as \emph{Cauchy-completeness}.  Being Cauchy-complete is a property that a uniform space may or may not have,\footnote{No, you are not expected to know what a uniform space is yet (though see \cref{UniformSpace} if you can't wait to find out).} and we will see that, when we equip $\R$ with a uniform structure, this result is equivalent to completeness in the sense of uniform spaces.  You should be careful not to confuse Cauchy-completeness with Dedekind-completeness.  For example, while $\R$ is the unique (up to unique isomorphism) nonzero Dedekind-complete totally-ordered field, there are Cauchy-complete totally-ordered field distinct from $\R$.  In brief, the example will turn out to be the Cauchy-completion of $\R (x)$ (which cannot be Dedekind-complete because otherwise $\R (x)$ would embed into $\R$---see \cref{exm3.2.13}), but we will have to wait until the next chapter to be more precise about this.
\end{rmk}
\begin{proof}
To show that $\lambda \mapsto x_\lambda$ converges, we first have to find some number $x_\infty\in \R$ which we think is going to be the limit of the net.  Our guess, of course, is going to be $x_\infty\coloneqq \limsup _\lambda x_\lambda$.  We know that the net $\lambda \mapsto x_\lambda$ is eventually bounded (\cref{prp3.3.28}), so that $x_\infty \in \R$ is finite.

We wish to show that $\lim _\lambda x_\lambda =x_\infty$.  So, let $\varepsilon >0$.  First of all, choose $\lambda _0$ so that there is some $\varepsilon$-ball $B_{\varepsilon}$ such that
\begin{equation}\label{3.3.45}
\left\{ x_\lambda :\lambda \geq \lambda _0\right\} \subseteq B_{\varepsilon}.\footnote{This is the definition of Cauchy.}
\end{equation}
We will use this later.

Now recall the definition of $\limsup$:
\begin{equation}
x_\infty \coloneqq \lim _\lambda u_\lambda \coloneqq \lim _{\lambda _0}\left( \sup _{\lambda \geq \lambda _0}\{ x_\lambda \}\right) ,
\end{equation}
so there is some $\lambda _0'$ such that, whenever $\lambda \geq \lambda _0'$,
\begin{equation}\label{3.3.47}
u_\lambda -x_\infty =\abs{u_{\lambda}-x_{\infty}}<\varepsilon .
\end{equation}
Redefine $\lambda _0$ to be the maximum of $\lambda _0$ and $\lambda _0'$.  This way, both \eqref{3.3.45} and \eqref{3.3.47} will hold for $\lambda \geq \lambda _0$.

On the other hand, for every $\lambda \geq \lambda _0$, by \cref{prp1.4.11}, there is some $\lambda '\geq \lambda \geq \lambda _0$ such that
\begin{equation}
u_\lambda -\varepsilon <x_{\lambda '}\leq u_\lambda .
\end{equation}
In particular,
\begin{equation}
u_\lambda -x_{\lambda '}<\varepsilon .
\end{equation}
Hence,
\begin{equation}
\begin{split}
\abs{x_{\lambda '}-x_\infty} & =\abs{(x_{\lambda '}-u_\lambda )+(u_\lambda -x_\infty )} \\
& \leq \abs{x_{\lambda '}-u_\lambda}+\abs{u_\lambda -x_\infty} \\
& <\varepsilon +\varepsilon =2\varepsilon .
\end{split}
\end{equation}
We're almost done.  The only problem with this is that $\lambda '$ is a specific index.  We need this inequality to hold for all $\lambda$ sufficiently large, not just a single $\lambda '$.  Fortunately, the Cauchyness can do this for us:  it follows from \eqref{3.3.45} that, whenever $\lambda \geq \lambda _0$, $\abs{x_\lambda -x_{\lambda '}}<2\varepsilon$.  Hence, for $\lambda \geq \lambda _0$,
\begin{equation}
\begin{split}
\abs{x_\lambda -x_\infty} & =\abs{(x_\lambda -x_{\lambda '})+(x_{\lambda '}-x_\infty)} \\
& \leq \abs{x_\lambda -x_{\lambda '}}+\abs{x_{\lambda '}-x_\infty} \\
& <2\varepsilon +2\varepsilon =4\varepsilon .
\end{split}
\end{equation}
\end{proof}
\begin{rmk}
You'll note that the final inequality the proof ended with was $\abs{x_\lambda -x_\infty}<4\varepsilon$, in contrast to the inequality (implicitly) in the definition of convergence (\cref{dfn3.3.8}), $\abs{x_\lambda -x_\infty}<\varepsilon$.  This of course makes no difference because $\frac{\varepsilon}{4}>0$ is just as arbitrary as $\varepsilon>0$.  Some people actually go to the trouble of doing the proof and then going back and changing all the $\varepsilon$s to $\frac{\varepsilon}{n}$s just so that the final inequality has an $\varepsilon$ in it.  This is completely unnecessary.  Don't waste your time.
\end{rmk}
\end{thm}

We showed above in \cref{exm3.3.39} that the sequence $m\mapsto s_m\coloneqq \sum _{k=0}^m\frac{1}{k!}$ is Cauchy.  The completeness of $\R$ that we just established then tells us that this sequence converges, and so now we can define
\begin{equation}
\e \coloneqq \lim _m\sum _{k=0}^m\tfrac{1}{k!}.
\end{equation}
But let's not.  Given what we currently know, it would seem like we would just be pulling this definition out of our ass.  This series is not why $\e$ matters.  In fact, I would argue that $\e$ itself doesn't matter---it is the function $\exp$ that arises naturally in calculus (being the unique (up to scalar multiples) function equal to its own derivative).  Thus, we refrain from `officially' defining $\e$ until we have defined the exponential, which of course we won't be able to do until we know about differentiation---see \cref{ExponentialFunction}.

Nevertheless, it would be nice to `officially' know that there is at least some real number that is not rational.

\subsubsection{Square-roots}\label{sssSquareRoots}

I mentioned back right before \crefnameref{sbs1.4.2} that sometimes people introduce the real numbers so that we can take square-roots of numbers, but that this logic is a bit silly, because if that were the objective, then we should really be extending our number system from $\Q$ to $\A$, not from $\Q$ to $\R$.  That being said, even though our justification for the introduction of the reals was really so that we can do calculus (take limits), it does turn out that we do get square-roots of all nonnegative reals.  This should be viewed more as ``icing on the cake'' instead of the raison d'\^{e}tre.

\begin{prp}{}{prp3.3.59}
Let $x\geq 0$, define $x_0\coloneqq 1$ and
\begin{equation}\label{3.3.64}
x_{m+1}\coloneqq \frac{1}{2}\left( x_m+\frac{x}{x_m}\right)
\end{equation}
for $m\geq 0$.  Then, $m\mapsto x_m$ converges and $(\lim _mx_m)^2=x$.
\begin{rmk}
In fact, the same sort of trick can be used to prove the existence and uniqueness of all $m^{\text{th}}$ roots, $m\in \Z ^+$.  The statement is \cref{prp3.3.66}, though we leave the `meat' of the proof as an exercise (\cref{exr2.4.71}).
\end{rmk}
\begin{rmk}
Not only does this proposition show the existence of a number whose square is $x$, but it tells you how to compute it as well.
\end{rmk}
\begin{proof}
Let us first assume that $m\mapsto x_m$ \emph{does} converge, say to $x_\infty \coloneqq \lim _mx_m$.  Then, taking the limit of both sides of the equation \eqref{3.3.64}, we find that it must be the case that
\begin{equation}
x_\infty =\frac{1}{2}\left( x_\infty +\frac{x}{x_\infty}\right) ,
\end{equation}
and hence that
\begin{equation}
x_\infty ^2=x.
\end{equation}
Thus, if the limit exists, it converges to a nonnegative number whose square is $x$ (nonnegative because each $x_m$ is nonnegative).  Thus, we now check that in fact it converges.

We apply the \nameref{MonotoneConvergenceTheorem}.  The sequence is bounded below by $0$, so it suffices to show that it is nonincreasing.  We thus look at
\begin{equation}
x_{m+1}-x_m=\frac{1}{2}\left( x_m+\frac{x}{x_m}\right) -x_m=\frac{x-x_m^2}{2x_m}.
\end{equation}
We want this to be $\leq 0$, so it suffices to show that eventually $x_m^2\geq x$.  However, for $m\geq 1$ (so that $x_{m-1}$ makes sense),
\begin{equation}
\begin{split}
x_m^2 & =\frac{1}{4}\left( x_{m-1}+\frac{x}{x_{m-1}}\right) ^2 \\
& =x+\left[ \frac{1}{4}\left( x_{m-1}^2+2x+\frac{x^2}{x_{m-1}^2}\right) -x\right] \\
& =x+\frac{1}{4}\left( x_{m-1}-\frac{x}{x_{m-1}}\right) ^2\geq x.
\end{split}
\end{equation}
\end{proof}
\end{prp}
\begin{exr}{}{exr2.4.71}
Let $m\in \Z ^+$.  Develop an algorithm which computes $m^{\text{th}}$ roots.
\end{exr}
\begin{prp}{}{prp3.3.66}
Let $m\in \Z ^+$ and let $x\geq 0$.  Then, there is a unique nonnegative real number, $\sqrt[m]{x}$\index[notation]{$\sqrt[m]{x}$}, whose $m^{\text{th}}$ power is $x$.
\begin{rmk}
Of course this is not true if you remove the word ``nonnegative'':  $(-1)^2=1=1^2$.
\end{rmk}
\begin{rmk}
As I'm sure you're aware, if $m=2$, it is common to write $\sqrt{x}\coloneqq \sqrt[2]{x}$\index[notation]{$\sqrt{x}$}.
\end{rmk}
\begin{rmk}
If $x<0$, then we consider the symbol $\sqrt[m]{x}$ to be undefined.
\end{rmk}
\begin{proof}
We have just established existence.
\begin{exr}[breakable=false]{}{}
Show that the map $x\mapsto x^m$ is strictly increasing on $\R _0^+$.
\end{exr}
As strictly increasing (and decreasing) functions are injective, it follows that there is at most one nonnegative real number whose $m^{\text{th}}$ power is $x$.
\end{proof}
\end{prp}

We would have been able to show almost from the very beginning that, if $m\in \Z ^+$ is not a perfect square, then there is no element $x\in \Q$ such that $x^2=m$.  What we haven't been able to show until just now, however, is that there is a \emph{real} number $x\in \R$ such that $x^2=m$.  We now finally check that indeed there is no rational number whose square is $m$.  Thus, we will have finally established the existence of real numbers which are not rational.
\begin{prp}{}{prp3.3.68}
Let $m\in \Z ^+$.  Then, if $m$ is not a perfect square, then there is no $x\in \Q$ such that $x^2=m$.
\begin{rmk}
$m\in \Z ^+$ is a \term{perfect-square}\index{Perfect-square} iff $m=n^2$ for some $n\in \Z$.  In other words, an integer is a perfect square iff it is the square of another integer.
\end{rmk}
\begin{rmk}
Thus, in particular, \cref{prp3.3.68} gives an example of a sequence that is Cauchy in $\Q$ but does \emph{not} converge in $\Q$.  In other words, $\Q$ is \emph{not} Cauchy-complete.
\end{rmk}
\begin{proof}
Suppose that $m$ is not a perfect square.  Then, we can write $m=k^2n$ where $n>1$ is square-free\footnote{$m\in \Z$ is \term{square-free}\index{Square-free} iff the only perfect-square which divides $m$ is $1$.}.  It thus suffices to show that there is no rational number whose square is $n$.  We proceed by contradiction:  suppose there is some $x\in \Q$ such that $x^2=n$.  Write $x=\frac{a}{b}$ with $b>0$ and $\gcd (a,b)=1$.  Then,
\begin{equation}
a^2=nb^2,
\end{equation}
and so every prime factor of $n$ divides $a^2$, and hence $a$ (because the factor is prime---see \cref{Prime}).  Let $p$ be some prime factor of $n$ and write $n=pn'$ and $a=pa'$.  This gives us
\begin{equation}
p^2(a')^2=pn'b^2,
\end{equation}
and hence
\begin{equation}
p(a')^2=n'b^2
\end{equation}
As $n$ is square-free, $\gcd (p,n')=1$,\footnote{$n=pn'$, and so if $p\mid n'$, then we would have $n=p^2\cdot \text{integer}$, that is, $p^2\mid n$.} and hence this equation implies that $p$ divides $b^2$ (by \nameref{EuclidsLemma}), and hence divides $b$.  But then $\gcd (a,b)\geq p>1$:  a contradiction.
\end{proof}
\end{prp}

We now show that all intervals in $\R$ are exactly what you think they are (confer \cref{Interval}).  We could have done this awhile ago now, but we wanted to wait until we could definitively provide an example of an interval in $\Q$ that is \emph{not} of the form you would expect.\footnote{By ``what you would expect'', we mean something like $[a,b]$, or $[a,b)$, etc.}
\begin{prp}{}{prp3.3.70}
Let $I\subseteq \R$.  Then, $I$ is an interval iff either
\begin{enumerate}
\item \label{enm3.3.70.i}$I=[a,b]$ for $-\infty <a\leq b<\infty $,
\item \label{enm3.3.70.ii}$I=(a,b)$ for $-\infty \leq a\leq b\leq \infty$,
\item \label{enm3.3.70.iii}$I=[a,b)$ for $-\infty <a\leq b\leq \infty$, or
\item \label{enm3.3.70.iv}$I=(a,b]$ for $-\infty \leq a\leq b<\infty$,
\end{enumerate}
where $a=\inf (I)$ and $b=\sup (I)$.
\begin{rmk}
Note that when the side of the interval is open, we allow $\pm \infty$ (depending on which side it is).
\end{rmk}
\begin{rmk}
In the future, we shall simply write $I=[(a,b)]$\index[notation]{$[(a,b)]$} to indicate that each end may be either open or closed---it is a pain to keep writing out all the four cases separately.
\end{rmk}
\begin{proof}
$(\Rightarrow )$ Suppose that $I$ is an interval.  If $I$ is empty, then we have that $I=(0,0)$, and so is of the form \cref{enm3.3.70.ii}.  Otherwise, we may define $b\coloneqq \sup (I)$ and $a\coloneqq \inf (I)$.\footnote{You might be used to checking that a set is bounded above (resp.~below) before being able to take its supremum (resp.~infimum).  This is essentially correct---this case is exceptional in that we don't mind if $\sup (I)=\infty$ or $\inf (I)=-\infty$.}  For each $a$ and $b$ there are two possibilities: either $I$ contains $a$ or it does not (and similarly for $b$).  We do one of the four cases:  suppose that $a,b\in I$.  Then, because $I$ is an interval, we must have immediately that $[a,b]\subseteq I$.  To show the other inclusion, let $x\in I$.  We proceed by contradiction:  suppose that either $x<a$ or $x>b$.  Both cases are similar, so let us just assume that $x>b$.  Then, $b$ is no longer an upper-bound for $I$:  a contradiction.  Therefore, $I\subseteq [a,b]$, and so $I=[a,b]$.

\blankline
\noindent
$(\Leftarrow )$ Suppose that $I$ is of the form \cref{enm3.3.70.i}--\cref{enm3.3.70.iv}.  Recall that the definition of an interval (\cref{Interval}) is that, for any $x_1,x_2\in I$ with $x_1\leq x_2$, then $x_1\leq x\leq x_2$ implies that $x\in I$.  As each of these forms satisfies this property (for trivial reasons---feel free to do the case-work if you're not convinced), $I$ is an interval.
\end{proof}
\end{prp}
Of course, we mentioned above, this is \emph{not} true in $\Q$.
\begin{exm}{An interval in $\Q$ not of the form \cref{prp3.3.70}(\cref{enm3.3.70.i}--\cref{enm3.3.70.iv})}{exm3.3.71}
Define $I\coloneqq [0,\sqrt{2}]\cap \Q \subseteq \Q$.  It follows from the fact  that $[0,\sqrt{2}]$ is an interval in $\R$ that $I$ is an interval in $\Q$ (because it satisfies the defining condition of \cref{Interval}).  If it were of the form $[(a,b)]$, then we would have to have $I=[0,b]$ or $I=[0,b)$.  From the definition of $I$, however, it follows that we must have $b^2=2$, and so as there is no such $b$ (in $\Q$), $I$ cannot be of this form.
\end{exm}

\subsubsection{`Density' of \texorpdfstring{$\Q ^{\comp}$}{Qc} in \texorpdfstring{$\R$}{R}}

We mentioned back when we showed the `density' of $\Q$ in $\R$ (\cref{thm3.2.14}) that it was also true that $\Q ^{\comp}$ was `dense' in $\R$.  At the time, however, we could not even construct a single real number that we could prove was irrational.  Now, however, we have done so, and so we can return to the issue of the `density' of $\Q ^{\comp}$ in $\R$.
\begin{thm}{`Density' of $\Q ^{\comp}$ in $\R$}{thm3.3.76}
Let $a,b\in \R$.  Then, if $a<b$, then there exists $c\in \Q ^{\comp}$ such that $c\in (a,b)$.
\begin{proof}
We have just shown that $\Q ^{\comp}$ is nonempty, so let $x\in \Q ^{\comp}$.  By `density' of $\Q$, we may take $a'\in \Q \cap (a-x,b-x)$.  Then, $a'+x\in (a,b)$, and furthermore, $a'+x\eqqcolon q$ must be irrational, because if it were rational, then $x=q-a'$ would be rational:  a contradiction.
\end{proof}
\end{thm}

\subsubsection{Counter-examples}

In this small subsubsection, we present a couple of counter-examples, which might seem quite surprising if you've never thought too much about these things before.
\begin{exm}{}{exm3.3.73}
For $m,n\in \Z ^+$, define\footnote{This is an example of a case where we would have to reindex by $1$, so that we don't divide by $0$, were we forced to take $m,n\in \N$---see the first remark in our definition of a sequence, \cref{dfn3.3.4}.}
\begin{equation}
x_{m,n}\coloneqq \frac{\tfrac{1}{m}\tfrac{1}{n}}{\tfrac{1}{m^2}+\tfrac{1}{n^2}}.
\end{equation}
Then, for \emph{fixed} $n\in \Z ^+$,
\begin{equation}
\lim _mx_{m,n}=\frac{0}{0+\tfrac{1}{n^2}}=0.
\end{equation}
Similarly, for fixed $m\in \Z ^+$,
\begin{equation}
\lim _nx_{m,n}=0.
\end{equation}
On the other hand, if we set $m=n$ and \emph{then} take a limit, we get
\begin{equation}
\lim _mx_{m,m}=\lim _m\left( \frac{\tfrac{1}{m^2}}{\tfrac{1}{m^2}+\tfrac{1}{m^2}}\right) =\lim _m(\tfrac{1}{2})=\tfrac{1}{2}.
\end{equation}
Thus:
\begin{important}
Even if there is some number $x_\infty$ such that (i)~\emph{for all} $\lambda$, $\lim _\mu x_{\lambda ,\mu}=x_\infty$, and (ii)~\emph{for all} $\mu$, $\lim _\lambda x_{\lambda ,\mu}=x_\infty$, it need \emph{not} be the case that $\lim _\lambda x_{\lambda ,\lambda}=x_\infty$.
\end{important}
This can sort of be fixed, however---see \cref{prp3.3.154}.
\end{exm}
\begin{exm}{Iterated limits need not agree}{}
For $m,n\in \Z ^+$, define
\begin{equation}
x_{m,n}\coloneqq \frac{\tfrac{1}{m}}{\tfrac{1}{m}+\tfrac{1}{n}}.
\end{equation}
Then,
\begin{equation}
\lim _mx_{m,n}=\frac{0}{0+\tfrac{1}{n}}=0,
\end{equation}
and hence
\begin{equation}
\lim _n\left( \lim _mx_{m,n}\right) =0.
\end{equation}
On the other hand,
\begin{equation}
\lim _nx_{m,n}=\frac{\tfrac{1}{m}}{\tfrac{1}{m}+0}=1,
\end{equation}
and hence
\begin{equation}
\lim _m\left( \lim _nx_{m,n}\right) =1.
\end{equation}
Thus:
\begin{displayquote}
It is possible for both iterated limits, $\lim _\lambda \left( \lim _\mu x_{\lambda ,\mu}\right)$ and $\lim _\mu \left( \lim _\lambda x_{\mu ,\lambda}\right)$, to exist and \emph{not} agree.
\end{displayquote}
\begin{rmk}
This is actually incredibly important because it is often really tempting to interchange limits, especially if they might be hidden implicitly in something like a derivative, but this is in general \emph{just plain wrong}.
\end{rmk}
\end{exm}

\subsection{Series}\label{sbs3.3.5}

As you probably know from calculus, a series is an `infinite sum'.  In other words, it is a limit of a finite sum.
\begin{dfn}{Series}{Series}
Let $m\mapsto a_m$ be a sequence and define $s_m\coloneqq \sum _{k=0}^ma_k$.  Then,
\begin{equation}
\sum _{k\in \N}a_k\coloneqq \sum _{k=0}^\infty a_k\coloneqq \lim _m\sum _{k=0}^ma_k
\end{equation}
is a \term{series}\index{Series} and the $s_m$s are the \term{partial sums}\index{Partial sums} of this series.\footnote{Of course this limit need not exist.  In that case, the notation $\sum _{k\in \N}a_k$ is meaningless, or at least, doesn't represent any real number.}  The series is said to \term{converge} iff the sequence $m\mapsto s_m$ converges and similarly for \term{diverge to $\pm \infty$} and \term{diverge}.  The series \term{converges absolutely}\index{Converges absolutely} iff $\sum _{k=0}^m\abs{a_k}$ converges and similarly for \term{diverges absolutely}\index{Diverges absolutely}\footnote{Of course, there is no ``diverges to $\pm \infty$ absolutely'' because this can only diverge to $+\infty$.  Also note that ``diverges absolutely'' is the same as ``doesn't converge absolutely''.}.  The series \term{converges conditionally}\index{Converges conditionally} iff it converges but not absolutely, and similarly for \term{diverges conditionally}\index{Diverges conditionally}.
\begin{rmk}
Of course, absolute convergence implies convergence, and similarly divergence to $\pm \infty$ implies divergence implies absolute divergence.
\end{rmk}
\begin{rmk}
Note that conditional convergence implies conditional divergence,\footnote{Why?} but that the converse is false---see \cref{exm2.4.95}.
\end{rmk}
\end{dfn}
\begin{exm}{}{}
While we don't have the tools to prove these statements yet, we present a couple of examples to help illustrate the differences between these types of convergence.

\begin{enumerate}
\item $\sum _{m\in \Z ^+}\frac{1}{m^2}$ converges absolutely.
\item $\sum _{m\in \Z ^+}\frac{(-1)^{m+1}}{m}$ converges, but not absolutely (that is, is conditionally convergent).
\item $\sum _{m\in \Z ^+}m$ diverges to $+\infty$.
\item $\sum _{m\in \Z ^+}(-2)^{m+1}$ diverges, but not to $\pm \infty$.
\item $\sum _{m\in \Z ^+}\frac{(-1)^{m+1}}{m}$ diverges absolutely, but doesn't diverge (that is, is conditionally divergent).
\end{enumerate}
\end{exm}
\begin{exm}{A series which is conditionally divergent but not conditionally convergent}{exm2.4.95}
$\sum _{m\in \Z ^+}(-1)^m$ is conditionally divergent but not conditionally convergent.  Of course, $\sum _{m\in \Z ^+}\abs{(-1)^m}$ diverges, but on the other hand the original series does not diverge (it is only nonconvergent).  On the other hand, the original series doesn't converge, and so certainly can't be conditionally convergent.
\end{exm}

\begin{exr}{Geometric series}{GeometricSeries}\index{Geometric series}
Let $\abs{a}<1$.  Show that $\sum _{m\in \N}a^m=\frac{1}{1-a}$.
\end{exr}

You'll probably recall several tests from calculus that allow us to determine whether or not a given series converges.  One of the primary goals of this section is to prove several of these tests.

Perhaps the first thing you should check is whether or not the terms of the series go to $0$, if only because it (often) takes no time to check at all.
\begin{prp}{}{}
Let $m\mapsto a_m$ be a sequence.  Then, if $\sum _{k\in \N}a_k$ converges, then $\lim _ma_m=0$.
\begin{rmk}
As just mentioned, this is often used in the contrapositive:  if $\lim _ma_m\neq 0$, then $\sum _{k\in \N}a_k$ doesn't converge.
\end{rmk}
\begin{proof}
Suppose that $\sum _{k\in \N}a_k$ converges.  Let $\varepsilon >0$.  Then, as the sequence of partial sums is Cauchy, there is some $m_0\in \N$ such that, whenever $m_0\leq m\leq n$, it 
\begin{equation}
\left| \sum _{k=m+1}^na_k\right| =\left| \sum _{k=0}^na_k-\sum _{k=0}^ma_k\right| <\varepsilon .
\end{equation}
As this hold \emph{for all} $m\leq n$, we may take $n\coloneqq m+1$, in which case this inequality reduces to
\begin{equation}
\abs{a_{m+1}-0}=\abs{a_{m+1}}<\varepsilon .
\end{equation}
Hence, by definition, $\lim _ma_m=0$.
\end{proof}
\end{prp}
Thus, if the terms do not go to $0$, you can immediately conclude that the series does not converge.  There is a similar result about the entire `tail-end' of a series, though it is probably more useful in proofs than as a test for convergence.
\begin{exr}{}{exr2.4.99}
Let $m\mapsto a_m$ be a sequence.  Show that if $\sum _{k\in \N}a_k$ converges, then $\lim _m\sum _{k=m}^{\infty}a_k=0$.
\end{exr}
\begin{prp}{Absolute convergence implies convergence}{}
Let $m\mapsto a_m$ be a sequence and suppose that $\sum _{k\in \N}\abs{a_k}$ exists.  Then, $\sum _{k\in \N}a_k$ exists.
\begin{proof}
To show that $\sum _{k\in \N}a_k$ exists, we show that the sequences of partials sums $m\mapsto s_m\coloneqq \sum _{k=0}^ma_k$ is Cauchy.  Take $m\leq n$.  Then,
\begin{equation}\label{3.3.61}
\begin{split}
\left| \sum _{k=0}^na_k-\sum _{k=0}^ma_k\right| & =\left| \sum _{k=m+1}^na_k\right| \leq \sum _{k=m+1}^n\abs{a_k} \\
& =\left| \sum _{k=0}^n\abs{a_k}-\sum _{k=0}^m\abs{a_k}\right| .
\end{split}
\end{equation}
Now we are essentially done.  Do you see why?  This is important as you eventually you want to get to the point where the above is all you need to convince yourself that the statement is true.\footnote{Don't `cheat' though---if you don't yet know how to fill in the details, keep filling them in:  eventually, after writing down the details sufficiently many times, you will be able to look at the above and confidently declare the rest as ``obvious''.}

Let $\varepsilon >0$ and choose $m_0$ so that whenever $m_0\leq m\leq n$ it follows that
\begin{equation}
\left| \sum _{k=0}^n\abs{a_k}-\sum _{k=0}^m\abs{a_k}\right| <\varepsilon .
\end{equation}
Then, it follows from \eqref{3.3.61} that whenever $m_0\leq m\leq n$ that
\begin{equation}
\left| \sum _{k=0}^na_k-\sum _{k=0}^ma_k\right| <\varepsilon ,
\end{equation}
which shows that the sequence of partial sums is Cauchy, and hence the series converges.
\end{proof}
\end{prp}

The next test we present is the \term{Alternating Series Test}.
\begin{dfn}{Alternating series}{}
A series $\sum _{k\in \N}a_k$ is \term{alternating}\index{Alternating series} iff $\sgn (a_{k+1})=-\sgn (a_k)$ for all $k\in \N$.
\begin{rmk}
In this case, we may write $a_k=\pm (-1)^kb_k$ where $b_k\geq 0$ and the $\pm$ is determined depending on the sign of the first term.
\end{rmk}
\end{dfn}
\begin{prp}{Alternating Series Test}{}
Let $m\mapsto a_m$ be a nonincreasing sequence that converges to $0$.  Then, $\sum _{m\in \N}(-1)^ma_m$ converges.
\begin{proof}
The first thing to notice is that
\begin{equation*}
\begin{split}
\MoveEqLeft
\sum _{k=m}^n(-1)^{k-m}a_k \\
& =a_m-(a_{m+1}-a_{m+2})-(a_{m+3}-a_{m+4})-\cdots \\ & \quad -(a_{n-1}-a_n) \\
& \leq a_m
\end{split}
\end{equation*}
because each $a_k-a_{k+1}\geq 0$.\footnote{Depending on whether $m-n$ is even or odd, the last term might have instead just be $-a_n$ instead of $-(a_{n-1}-a_n)$.  Either way, the same inequality holds.}

Using this, we see that
\begin{equation}
\begin{split}
\left| \sum _{k=0}^n(-1)^ka_k-\sum _{k=0}^m(-1)^ka_k\right| & =\left| \sum _{k=m+1}^n(-1)a_k\right| \\
& \leq a_{m+1}.
\end{split}
\end{equation}
Now that the partial sums are Cauchy follows from the fact that $\lim _ma_m=0$.
\end{proof}
\end{prp}
\begin{prp}{Comparison Test}{ComparisonTest}\index{Comparison Test}
Let $m\mapsto a_m$ and $m\mapsto b_m$ be sequences such that eventually $\abs{a_m}\leq \abs{b_m}$.  Then,
\begin{enumerate}
\item \label{enm3.3.87.i}if $\sum _{m\in \N}a_m$ diverges absolutely, then $\sum _{m\in \N}b_m$ diverges absolutely; and
\item \label{enm3.3.87.ii}if $\sum _{m\in \N}b_m$ converges absolutely, then $\sum _{m\in \N}a_m$ converges absolutely.
\end{enumerate}
\begin{wrn}
Warning:  \cref{enm3.3.87.i} will fail if $\sum _{m\in \N}b_m$ is only conditionally convergent---see the following exercise (\cref{exr2.4.109}).
\end{wrn}
\begin{wrn}
Warning:  \cref{enm3.3.87.ii} will fail if $\sum _{m\in \N}b_m$ only \emph{conditionally} converges---see the following exercise (\cref{exr2.4.109}).
\end{wrn}
\begin{proof}
\cref{enm3.3.87.i} follows from the fact that the operation of taking limits (in this case, applied to the partial sums) is nondecreasing---see \cref{exr3.3.30}.
\begin{exr}[breakable=false]{}{}
Prove \cref{enm3.3.87.ii}.
\end{exr}
\end{proof}
\end{prp}
\begin{exr}{}{exr2.4.109}
\begin{enumerate}
\item Find an example of sequences $m\mapsto a_m$ and $m\mapsto b_m$ with (i)~$\abs{a_m}\leq \abs{b_m}$, (ii)~$\sum _{m\in \N}a_m$ divergent, but (iii)~$\sum _{m\in \N}b_m$ not divergent.
\item Find an example of sequences $m\mapsto a_m$ and $m\mapsto b_m$ with (i)~$\abs{a_m}\leq \abs{b_m}$, (ii)~$\sum _{m\in \N}b_m$ convergent, but (iii)~$\sum _{m\in \N}a_m$ not convergent.
\end{enumerate}
\end{exr}
\begin{prp}{Limit Comparison Test}{prpLimitComparisonTest}\index{Limit Comparison Test}
Let $m\mapsto a_m$ and $m\mapsto b_m$ be eventually nonnegative sequences such that $\lim _m\left| \frac{a_m}{b_m}\right|$ converges to a nonzero number.  Then, $\sum _{m\in \N}a_m$ converges iff $\sum _{m\in \N}b_m$ converges.
\begin{proof}
Define $L\coloneqq \lim _m\left| \frac{a_m}{b_m}\right| >0$.  Let $\varepsilon >0$ be less than $L$, and choose $m_0\in \N$ such that, whenever $m\geq m_0$, it follows that\footnote{The absolute values signs went away because the sequences are eventually \emph{nonnegative}---we have implicitly chosen $m_0\in \N$ large enough so that $a_m,b_m\geq 0$ for all $m\geq m_0$.}
\begin{equation}
\left| \tfrac{a_m}{b_m}-L\right| <\varepsilon ,
\end{equation}
that is,
\begin{equation}
-\varepsilon <\tfrac{a_m}{b_m}-L<\varepsilon ,
\end{equation}
or rather
\begin{equation}
b_m(L-\varepsilon )<a_m<b_m(L+\varepsilon ).
\end{equation}
Note that $L-\varepsilon >0$.  It follows from the Comparison Test applied to the first inequality that, if $\sum _{m\in \N}a_m$ converges, then $\sum _{m\in \N}b_m$ converges.  Likewise, it follows from the second inequality that, if $\sum _{m\in \N}b_m$ converges, then $\sum _{m\in \N}a_m$ converges.
\end{proof}
\end{prp}
\begin{prp}{Ratio Test}{Ratio Test}\index{Ratio Test}
Let $m\mapsto a_m$ be a sequence.  Then, if $\lim _m\abs{\frac{a_{m+1}}{a_m}}<1$,\footnote{It is implicit in this assumption that we also assume that $m\mapsto a_m$ is eventually nonzero, so that this hypothesis actually makes sense.} then $\sum _{m\in \N}a_m$ converges absolutely; if $\lim _m\abs{\frac{a_{m+1}}{a_m}}>1$, then $\sum _{m\in \N}a_m$ diverges.
\begin{proof}
Suppose that $r\coloneqq \lim _m\abs{\frac{a_{m+1}}{a_m}}<1$.  Let $\varepsilon >0$ be less than $1-r$.  Then, there is some $m_0$ such that, whenever $m\geq m_0$, it follows that
\begin{equation}
\abs{\tfrac{a_{m+1}}{a_m}-r}\leq \abs{\abs{\tfrac{a_{m+1}}{a_m}}-r}<\varepsilon ,
\end{equation}
so that $\abs{\frac{a_{m+1}}{a_m}}<r+\varepsilon \eqqcolon C<1$, so that $\abs{a_{m+1}}<C\abs{a_m}$, so that $\abs{a_{m_0+l}}<C^l\abs{a_{m_0}}$ for $l\geq 0$.  As $\abs{C}<1$, it follows from the \namerefpcref{ComparisonTest} that $\sum _{k=m_0}^{\infty}\abs{a_k}$ converges, and hence that $\sum _{m\in \N}a_m$ converges absolutely.

\begin{exr}[breakable=false]{}{}
Prove the case where $\lim _m\abs{\frac{a_{m+1}}{a_m}}>1$.
\end{exr}
\end{proof}
\end{prp}
\begin{prp}{Root Test}{RootTest}\index{Root Test}
Let $m\mapsto a_m$ be a sequence.  Then, if $\limsup _m\abs{a_m}^{\frac{1}{m}}<1$, then $\sum _{m\in \N}a_m$ converges absolutely; if $\limsup _m\abs{a_m}^{\frac{1}{m}}>1$, then $\sum _{m\in \N}a_m$ doesn't converge.
\begin{rmk}
My impression is that this test is generally less frequently taught in calculus classes than most of the other ones we discuss.  Despite this, I think it's relatively important, certainly more so than AP Calculus might have one believe.  For one thing, it's the only convergence test we present that gives a sufficient condition for convergence for general series that doesn't make reference to other series---no clever `test' series required---just `mindlessly' compute the limit superior, and unless you get `unlucky' with a result of $1$, you have your answer.  Another reason for its importance is the \namecref{prp6.4.48} (\cref{prp6.4.48}), which gives a formula for the radius of convergence of a power series---this theorem is a nearly immediate corollary of the Root Test.
\end{rmk}
\begin{wrn}
Warning:  The $\limsup _m\abs{a_m}^{\frac{1}{m}}$ case fails if you replace ``doesn't converge'' with ``diverges''.  For example, take the sequence of terms
\begin{equation}
\coord{1,-1,2,-2,4,-4,8,-8,\ldots}.
\end{equation}
The partial sums alternate between $0$ and a power of $2$, and so, while this doesn't converge, it also doesn't diverge.
\end{wrn}
\begin{proof}
Suppose that $u\coloneqq \limsup _m\abs{a_m}^{\frac{1}{m}}<1$.  Let $\varepsilon >0$ be less than $1-u>0$.  Then, there is some $m_0$ such that, whenever $m\geq m_0$, it follows that
\begin{equation}
\sup _{n\geq m}\{ \abs{a_n}^{\frac{1}{n}}\} -u<\varepsilon ,
\end{equation}
so that $\sup _{n\geq m}\{ \abs{a_n}^{\frac{1}{n}}\} <\varepsilon +u<1$, so that $\abs{a_n}^{\frac{1}{n}}<r\coloneqq \varepsilon +u<1$ for all $n\geq m_0$, so that $\abs{a_n}<r^n$ for all $n\geq m_0$.  It follows that $\sum _{m\in \N}a_m$ converges absolutely by the comparison test.

\begin{exr}[breakable=false]{}{}
Prove the case where
\begin{equation}
\limsup _m\abs{a_m}^{\frac{1}{m}}>1.
\end{equation}
\end{exr}
\end{proof}
\end{prp}
\begin{exm}{Harmonic Series}{HarmonicSeries}
Consider the series $\sum _{m\in \Z ^+}(-1)^{m+1}\frac{1}{m}$ with terms $a_m\coloneqq (-1)^{m+1}\frac{1}{m}$.  This is called the \term{Alternating Harmonic Series}\index{Alternating Harmonic Series}, whereas the series of absolute values $\sum _{m\in \Z ^+}\frac{1}{m}$ is the \term{Harmonic Series}\index{Harmonic Series}.  We see immediately from the Alternating Series Test that the alternating harmonic series converges.  In fact, it converges to $\ln (2)$,\footnote{See \cref{exr6.4.76}.} though we don't know that yet (we don't even know what $\ln (2)$ is!).  The harmonic series itself though diverges (so that the alternating harmonic series is conditionally convergent).  To see this, we apply the Comparison Test:
\begin{equation*}
\begin{split}
\sum _{m\in \Z ^+}\tfrac{1}{m} & =1+\tfrac{1}{2}+\tfrac{1}{3}+\tfrac{1}{4}+\tfrac{1}{5}+\tfrac{1}{6}+\tfrac{1}{7}+\tfrac{1}{8}+\cdots \\
& \geq 1+\tfrac{1}{2}+\left( \tfrac{1}{4}+\tfrac{1}{4}\right) +\left( \tfrac{1}{8}+\tfrac{1}{8}+\tfrac{1}{8}+\tfrac{1}{8}+\tfrac{1}{8}\right) \\
& =1+\tfrac{1}{2}+\tfrac{1}{2}+\tfrac{1}{2}+\cdots =\infty .
\end{split}
\end{equation*}
\begin{rmk}
If you take a string and fix its endpoints, then there are only countably many ``fundamental'' frequencies that the string can vibrate at (fundamental in the sense that any way in which the string might vibrate can be written as a sum of the fundamental frequency solutions).  These are called the \emph{harmonics} and the wavelength of every harmonic is of the form $\frac{1}{m}2L$, where $L$ is the length of the string.  This is where the term ``harmonic'' comes from (or so it seems).
\end{rmk}
\end{exm}
In fact, there is a test that will tell us that the Harmonic Series diverges immediately, called the \emph{$p$-series Test}.  It says that $\sum _{n\in \Z ^+}\frac{1}{n^p}$ converges (absolutely) iff $p>1$.  While I suppose it is possible to prove it at the moment,\footnote{See \cite{Yang}, for example.} I think the most natural proof makes use of the integral, and so we postpone the proof---see \cref{pSeriesTest}.  More specifically, this proof will make use of the \emph{Integral Test}, which, obviously, first requires we introduce the integral, and so we likewise postpone this as well---see \cref{IntegralTest}.

\subsubsection{Decimal expansions}\label{sssDecimalExpansions}

We now return to the issue left unfinished from \cref{chp1} having to do with your ``naive'' idea of the real numbers, namely, as decimal expansions.  In mathematics, it is almost never a good idea to prove something about the real numbers using decimal expansions.  On the other hand, I suppose it's nice to know that this new object does in fact correspond to what you are familiar with.

In any case, without further ado, here is the precise statement of what is meant by ``decimal expansions''.
\begin{thm}{Decimal expansions}{DecimalExpansions}\index{Decimal expansions}
Let $r\in \Z ^+$ be at least $2$ and let $x\in \R$.  Then, there is a $x_0\in \N$ and $x_k\in \{ 0,\ldots ,r-1\}$ for $k\in \Z ^+$ such that
\begin{enumerate}
\item \label{DecimalExpansions.i}
\begin{equation}
x=\sgn (x)\left( x_0+\sum _{k=1}^{\infty}\frac{x_k}{r^k}\right) ;
\end{equation}
and
\item \label{DecimalExpansions.ii}$k\mapsto x_k$ is not eventually the constant $r-1$.
\end{enumerate}
\begin{rmk}
It is then customary to write $x=\sgn (x)x_0.x_1x_2x_3\ldots$, possibly using separate set of symbols to represent the numbers in $\{ 0,1,\ldots ,r-1\}$ (for example, in hexadecimal, it is customary to use the symbol ``$\text{A}$'' to represent the number $10\in \Z ^+$, the symbol ``$\text{B}$'' to represent the number $11\in \Z ^+$, and so on).
\end{rmk}
\begin{rmk}
$r$ is the \term{radix}\index{Radix} of the number system, and it correspond to the number of distinct symbols one uses to represent a number in a `decimal expansion' (though, strictly speaking I suppose, ``decimal expansion'' really refers to the case $r=10$).  Of course, you cannot distinguish between numbers using a single symbol alone (at least certainly not all real numbers), and so we need at least $r\geq 2$.  $r=2$ corresponds to \term{binary}\index{Binary}, $r=3$ corresponds to \term{ternary}\index{Ternary}, $r=8$ correspond to \term{octal}\index{Octal}, $r=10$ corresponds to \term{decimal}\index{Decimal}, $r=16$ corresponds to \term{hexadecimal}\index{Hexadecimal}, and those are the only names I am aware of.
\end{rmk}
\begin{rmk}
The condition \cref{DecimalExpansions.ii} is needed to ensure uniqueness.  For example, we have $1=1.00\ldots$ as well as $1=0.99\ldots$ for $r=10$.
\end{rmk}
\begin{rmk}
The exact same statement without the ``$\sgn (x)$'' is still true, but it's not the standard way of writing numbers.  For example, in this case, we would be writing $-3.6=-4+0.4$, whereas the `usual' decimal expansion corresponds to writing $-3.6=-3-0.6=-(3+0.6)$.
\end{rmk}
\begin{proof}
\Step{Replace $x$ with $\abs{x}$}
We always have that $x=\sgn (x)\abs{x}$.  Thus, what we actually want to do is find a unique $x_0\in \N$ and $x_k\in \{ 0,\ldots ,r-1\}$ for $k\in \Z ^+$ such that
\begin{enumerate}
\item
\begin{equation}
\abs{x}=x_0+\sum _{k=1}^{\infty}\frac{x_k}{r^k};
\end{equation}
and
\item $k\mapsto x_k$ is not eventually the constant $r-1$.
\end{enumerate}
Thus, it suffices to prove the result in the case $x\geq 0$.  We assume this throughout.

\Step{Define $x_0$}
First of all, let $x_0\in \N$ be the largest integer less-than-or-equal to $x$.\footnote{By the Archimedean Property, there is some integer larger than $x$.  As $\N$ is well-ordered, there is a smallest integer strictly larger than $x$.  That integer minus one will be the largest integer less-than-or-equal to $x$.}  Define $y_0\coloneqq x-x_0\in [0,1)$.

\Step{Define $x_1$}
Note that $\frac{d}{r}\in [0,1)$ for $d\in \{ 0,\ldots ,r-1\}$.  As the set $\left\{ d\in \{ 0,\ldots ,r-1\} :\frac{d}{r}\leq y_0\right\}$ is nonempty and finite, it has a largest element.  Call that element $x_1$ and define $y_1\coloneqq y_0-\frac{x_1}{r}\in [0,\frac{1}{r})$.
\begin{exr}{}{}
We have claimed that $y_1\in [0,\frac{1}{r})$.  Prove this.
\end{exr}

\Step{Define $x_k$ for $k\in \Z ^+$ inductively}
Note again that $\frac{d}{r^2}\in [0,\frac{1}{r})$ for $d\in \{ 0,\ldots ,r-1\}$, and so, similarly as before, there is a largest $x_2\in \{ 0,\ldots ,r-1\}$ such that $\frac{x_2}{r^2}\leq y_1$.

Continue this process inductively defining $x_k$ for all $k\in \Z ^+$.
\begin{exr}[breakable=false]{}{}
Show that in fact
\begin{equation}
x=x_0+\sum _{k=1}^{\infty}\frac{x_k}{r^k}.
\end{equation}
\end{exr}

\Step{Verify \cref{DecimalExpansions.i} and \cref{DecimalExpansions.ii}}
We now check that $k\mapsto x_k$ is not eventually the constant $r-1$.  To see this, note that
\begin{equation}
\sum _{k=k_0}^{\infty}\frac{r-1}{r^k}=\frac{1}{r^{k_0-1}}
\end{equation}
for $l\in \Z ^+$.  Thus, if we had $x_k=r-1$ for $k\geq k_0$ but $x_{k_0-1}<r-1$, then $x_{k_0-1}\in \{ 0,\ldots ,r-1\}$ would no longer be the larges element such that $\frac{x_{k_0-1}}{r^{k_0-1}}\leq y_{k_0-2}$---$x_{k_0-1}+1\in \{ 0,\ldots ,r-1\}$ would be a larger element which satisfies the same inequality:  a contradiction.  Therefore, it cannot be the case that $k\mapsto x_k$ is eventually the constant $r-1$.

\Step{Show that $x_0=x_0'$ in uniqueness}
It remains to check uniqueness.  So, let $x_0'\in \Z$ and $x_k'\in \{ 0,\ldots ,r-1\}$ be such that
\begin{equation}
x=x_0'+\sum _{k=1}^{\infty}\frac{x_k'}{r^k}
\end{equation}
and $k\mapsto x_k'$ is not eventually the constant $r-1$.

As both these `decimal expansions' are equal to $x$, we have
\begin{equation}
x_0-x_0'=\sum _{k=1}^{\infty}\frac{x_k'-x_k}{r^k},
\end{equation}
and hence
\begin{equation}
\abs{x_0-x_0'}\leq \sum _{k=1}^{\infty}\frac{\abs{x_k'-x_k}}{r^k}.
\end{equation}
Now, we certainly have that $\abs{x_k'-x_k}\leq r-1$, the `worst case scenario' being when one is $0$ and the other is $r-1$.  We wish to show that this can't \emph{always} be the case, that is, for at least some $k$ we have $\abs{x_k'-x_k}$ is \emph{strictly} less than $r-1$.  If we can show that, then we will have
\begin{equation}\label{eqn2.4.129}
\abs{x_0-x_0'}<\sum _{k=1}^{\infty}\frac{r-1}{r^k}=1,
\end{equation}
forcing $x_0=x_0'$ (by a corollary of \cref{exr1.2.14}).  So, we now show that there is at least some $k$ for which $\abs{x_k'-x_k}<r-1$.

We proceed by contradiction:  suppose that $\abs{x_k'-x_k}=r-1$ for all $k\in \Z ^+$.  This means that, for each $k$, either $x_k'=r-1$ and $x_k=0$, or vice-versa. Furthermore, note that regardless, the inequality \eqref{eqn2.4.129} is nonstrict, which still forces $\abs{x_0k-x_0'}=0$ or $\abs{x_0,x_0'}=1$.  In the former case we are done, so we can assume without loss of generality that $x_0'=x_0+1$.  This implies that we must have
\begin{equation}
\sum _{k=1}^{\infty}\frac{x_k}{r^k}=1+\sum _{k=1}^{\infty}\frac{x_k'}{r^k}.
\end{equation}
We wish to show that this forces $x_k=r-1$ for all $k\in \Z ^+$, which will contradict our hypothesis.  We prove that inductively.  We already know that $x_1=r-1$ and $x_1'=0$, or vice-versa.  If the ``vice-versa'' were true, then the above equation gives
\begin{equation}
1+\frac{1}{r}=\sum _{k=2}^{\infty}\frac{x_k-x_k'}{r^k}\leq \sum _{k=2}^{\infty}\frac{r-1}{r^k}=\frac{1}{r}:
\end{equation}
a contradiction.  Thus, we must have that $x_1=r-1$ and $x_1'=0$.
\begin{exr}{}{}
Complete the induction argument, using our proof for the $k=1$ case as guidance.
\end{exr}

\Step{Show that $x_1=x_1'$ in uniqueness}
Now that we know $x_0=x_0'$,
\begin{equation}
x_0+\sum _{k=1}^{\infty}\frac{x_k}{r^k}x=x_0'+\sum _{k=1}^{\infty}\frac{x_k'}{r^k}
\end{equation}
simplifies to
\begin{equation}
x_1-x_1'=\sum _{k=2}^{\infty}\frac{x_k'-x_k}{r^{k-1}}
\end{equation}
This is exactly the same setup as in the previous step, and so the previous step gives us $x_1=x_1'$.

\Step{Show that $x_k=x_k'$ for $k\in \Z ^+$ in uniqueness}
Applying this argument inductively, we find that $x_k=x_k'$ for all $k\in \Z ^+$, finishing the proof of uniqueness.
\end{proof}
\end{thm}

\subsubsection{The uncountability of the real numbers}\label{sss2.4.3}

We mentioned at the end of \crefnameref{CardinalityAndCountability} that $\abs{\R}=2^{\aleph _0}$ but at the time we did not know enough to prove it.  We now return to this.
\begin{thm}{}{thm2.4.118}
$\abs{\R}=2^{\aleph _0}$.
\begin{proof}
To prove this, we apply the Bernstein-Cantor-Schr\"{o}der Theorem (\cref{thm1.1.26}).  Thus, we wish to construct in injection from $2^{\N}$ to $\R$ and an injection from $\R$ to $2^{\N}$.  By \cref{exrA.1.26x}, we may replace $2^{\N}$ with $\{ 0,1\} ^{\N}$ in this statement, and hence in return with $\{ 0,2\} ^{\N}$ (you will see why we do this momentarily).

The set $\{ 0,2\} ^{\N}$ is just the collection of sequences $m\mapsto a_m$ with $a_m \in \{ 0,2\}$.  We thus define $\phi \colon \{ 0,2\} ^{\N}\rightarrow \R$ by
\begin{equation}
\phi \left( m\mapsto x_m\right) \coloneqq \sum _{m\in \N}\tfrac{a_m}{3^m}.
\end{equation}
Intuitively, the sequence $m\mapsto a_m$ is thought of as the ternary expansion of a real number.   Switching from $\{ 0,1\}$ to $\{ 0,2\}$ was tantamount to changing from binary to ternary and not including any numbers with $1$ in their ternary expansion.  The reason for this is because, in binary, we have
\begin{equation}
1=.\bar{1}\coloneqq .111\cdots ,
\end{equation}
and so the resulting function would not be injective.  Of course, in ternary, we still have things like
\begin{equation}
1=.\bar{2}=.222\cdots ,
\end{equation}
but $1$ corresponds to the sequence $\coord{1,0,0,0,\ldots}$, which is not an element of $\{ 0,2\} ^{\N}$, and so injectivity works out.
\begin{exr}[breakable=false]{}{}
Show that $\phi$ is injective.
\end{exr}
It follows that $2^{\aleph _0}\leq \abs{\R}$.

As $\abs{\Q}=\aleph _0$, it suffices to replace $2^{\N}$ by $2^{\Q}$ above, and so it suffices to produce an injection from $\R$ to $2^{\Q}$, the power-set of $\Q$.  Define $\psi \colon \R \rightarrow 2^{\Q}$ by
\begin{equation}
\psi (x)\coloneqq \left\{ q\in \Q :q\leq x\right\} .
\end{equation}
\begin{exr}[breakable=false]{}{}
Show that $\psi$ is injective.
\end{exr}
It follows that $\abs{\R}\leq 2^{\aleph _0}$, and hence that $\abs{\R}=2^{\aleph _0}$.
\end{proof}
\end{thm}

\subsubsection{Addition of infinitely many terms is `noncommutative'}

We will make the title of this subsubsection precise in a moment, but for the moment we will settle for something imprecise:  there exists two convergent series $\sum _{m\in \N}a_m$ and $\sum _{m\in \N}b_m$ which converge to different values, but yet $\{ a_m:m\in \N \} =\{ b_m:m\in \N \}$.  That is, the terms themselves are the same (though in different order of course), but yet the series converge to different values!  If this is your first time studying `rigorous' mathematics, this is probably one of these ``WTF!? moments'' when you realize that sometimes mathematics can be so counter-intuitive so as to demand rigor---if we didn't require a proof, it would be very easy to dismiss `commutativity' of infinite series as ``obvious''.  In fact, it's not usually a good idea to use the word ``obvious'' in proofs at all---at best it's lazy, and at worst, it's just plain wrong.

Despite the fact that this crazy sort of `noncommutativity' can happen, it never happens for \emph{absolutely} convergent series.
\begin{thm}{}{}
Let $m\mapsto a_m$ be a sequence such that $\sum _{m\in \N}a_m$ converges absolutely and let $\phi \colon \N \rightarrow \N$ be a bijection.  Then, $\sum _{m\in \N}a_{\phi (m)}$ converges absolutely and $\sum _{m\in \N}a_m=\sum _{m\in \N}a_{\phi (m)}$.
\begin{rmk}
That $\phi$ is a bijection is the way we make precise the idea that $b_m\coloneqq a_{\phi (m)}$ is just a `rearrangement' of the original terms.  Thus, this theorem says that the rearrangement of any absolutely convergent series converges to the same value.
\end{rmk}
\begin{proof}
Define $S\coloneqq \sum _{m\in \N}a_m$.  Let $\varepsilon >0$ and choose $m_0$ such that, whenever $m\geq m_0$, it follows that $\left| \sum _{k=0}^ma_k-S\right| <\varepsilon$.  Choose $m_1$ such that, whenever $m\geq m_1$, it follows that $\sum _{k=m+1}^\infty \abs{a_k}<\varepsilon$ (we may do this because of the absolute convergence---see \cref{exr2.4.99}).  Replace $m_0$ by $\max \{ m_0,m_1\}$, so that, whenever $m\geq m_0$, it follows that both of these inequalities hold.  Define
\begin{equation}
n_0\coloneqq \max \left( \phi ^{-1}\left( \{ m\in \N :m\leq m_0\} \right) \right) .
\end{equation}
The set $\{ m\in \N :m\leq n_0\}$ is finite, and so $\phi$ of that set is finite, and so the definition of $n_0$ makes sense.  This definition guarantees that
\begin{equation}\label{3.3.109}
\{ 0,1,\ldots ,m_0-1,m_0\} \subseteq \phi \left( \{ 0,1,\ldots ,n_0-1,n_0\} \right) .
\end{equation}
Suppose that $n\geq n_0$.  Then,
\begin{equation}
\begin{split}
\left| \sum _{k=0}^na_{\phi (k)}-S\right| & \leq \left| \sum _{k=0}^na_{\phi (k)}-\sum _{k=0}^{m_0}a_k\right| +\left| \sum _{k=0}^{m_0}a_k-S\right| \\
& <\footnote{In the first term, because of our choice of $n_0$, every $a_k$ for $0\leq k\leq a_{m_0}$ appears somewhere in $a_{\phi (k)}$ for $0\leq k\leq n$.  Thus, the difference only contains terms with index at least $m_0+1$.  We then apply the triangle inequality again.}\sum _{k=m_0+1}^\infty \abs{a_k}+\varepsilon <2\varepsilon .
\end{split}
\end{equation}
\end{proof}
\end{thm}
And now we turn to the precise statement of the idea that ``addition of infinitely many real numbers is `noncommutative'.''.
\begin{thm}{}{thm2.4.160}
Let $m\mapsto a_m$ be a sequence and define
\begin{equation}
a_m^+\coloneqq \tfrac{1}{2}(\abs{a_m}+a_m)\text{ and }a_m^-\coloneqq \tfrac{1}{2}(\abs{a_m}-a_m).
\end{equation}
Then,
\begin{enumerate}
\item \label{thm2.4.160.i}if $\sum _{m\in \N}a_m$ converges conditionally, then
\begin{equation}
\sum _{m\in \N}a_m^+=\infty =\sum _{m\in \N}a_m^-;
\end{equation}
and
\item \label{thm2.4.160.ii}if
\begin{equation}
\sum _{m\in \N}a_m^+=\infty =\sum _{m\in \N}a_m^-,
\end{equation}
then for every $x,y\in [-\infty ,\infty ]$ with $x\leq y$, there exists a bijection $\phi \colon \N \rightarrow \N$ such that $\liminf _m\sum _{k=0}^ma_{\phi (k)}=x$ and $\limsup _m\sum _{k=0}^ma_{\phi (k)}=y$.
\end{enumerate}
In particular, taking $x=y$, there is a bijection $\phi \colon \N \rightarrow \N$ such that $\sum _{m\in \N}a_{\phi (m)}=x$.
\begin{rmk}
Note that $a_m=a_m^+-a_m^-$ (and $\abs{a_m}=a_m^++a_m^-$) with $a_m^{\pm}\geq 0$.  Furthermore, $a_m^+>0$ iff $a_m>0$ and $a_m^->0$ iff $a_m<0$ (and are $0$ otherwise).  This trick of defining the `nonnegative part' and `nonpositive' part of a function is not that uncommon,\footnote{For example, see \cref{dfn5.2.38}.} and it would do you well to remember it.
\end{rmk}
\begin{rmk}
The `point' of this theorem, at least for us now, is the case in which $\sum _{m\in \N}a_m$ is conditionally convergent.  However, it holds more generally, and so we may as well state it more generally (especially as this will eventually be of use to us---see the proof of \cref{prp5.2.229}).
\end{rmk}
\begin{rmk}
This theorem says something really quite surprising:  Given a series that converges conditionally, you can rearrange the terms to obtain a series which converges to \emph{any real number you choose whatsoever}.\footnote{And in fact it says \emph{even more} than this---what we are referring to in this remark is just the special case.}
\end{rmk}
\begin{proof}\footnote{Adapted from \cite{Rudin}.}
\cref{thm2.4.160.i} Suppose that $\sum _{m\in \N}a_m$ converges conditionally.  As $\abs{a_m}=a_m^++a_m^-$, we must have that
\begin{equation}
\infty =\sum _{m\in \N}\abs{a_m}=\sum _{m\in \N}[a_m^++a_m^-],
\end{equation}
so that at least one of $\sum _{m\in \N}a_m^+$ and $\sum _{m\in \N}a_m^-$ must diverge.  On the other hand, as $a_m=a_m^+-a_m^-$,
\begin{equation}
\infty >\sum _{m\in \N}a_m=\sum _{m\in \N}[a_m^+-a_m^-],
\end{equation}
and so we cannot just have one of $\sum _{m\in \N}a_m^+$ and $\sum _{m\in \N}a_m^-$ diverge.  Thus, we must have that
\begin{equation}\label{3.3.116}
\sum _{m\in \N}a_m^+=\infty =\sum _{m\in \N}a_m^-.
\end{equation}

\blankline
\noindent
\cref{thm2.4.160.ii} Suppose that
\begin{equation}
\sum _{m\in \N}a_m^+=\infty =\sum _{m\in \N}a_m^-.
\end{equation}
Let $x,y\in [-\infty ,\infty ]$ with $x\leq y$.

Note that $a_m^+=a_m$ and $a_m^-=0$ if $a_m\geq 0$, and $a_m^+=0$ and $a_m^-=-a_m$ if $a_m\leq 0$.  Thus, modulo the existence of zero terms which make no difference, every term in both of the series of \eqref{3.3.116} is a term in the original series $\sum _{m\in \N}a_m$ (up to a minus sign in the latter case).  We will build up a rearrangement of the original series $\sum _{m\in \N}a_m$ using $a_m^+$ and $-a_m^-$.

The intuition is this:  because both of the series of \eqref{3.3.116} diverge, I can move as `far to the right' as I like by choosing terms of the form $a_m^+$, and likewise, I can move as `far to the left' as I like by choosing terms of the form $-a_m^-$.  We make this precise as follows.

First, suppose that $x,y$ are finite.\footnote{Note in the statement that we are allowed to take $x=-\infty$ and $y=+\infty$!}

Let $m_0$ be the smallest natural number such that
\begin{equation}
\sum _{k=0}^{m_0}a_k^+\geq y.
\end{equation}
Such an $m_0$ exists because the first series in \eqref{3.3.116} diverges.  Note that, because $m_0$ is the \emph{smallest} such number, we must have that 
\begin{equation}
\sum _{k=0}^{m_0-1}a_k^+<y,
\end{equation}
so that
\begin{equation}\label{3.3.119}
0\leq \sum _{k=0}^{m_0}a_k^+-y<a_{m_0}^+.
\end{equation}
Then, let $n_0$ be the smallest natural number such that
\begin{equation}
\sum _{k=0}^{m_0}a_k^+-\sum _{k=0}^{n_0}a_k^-\leq x.
\end{equation}
Similarly as before, we now have that
\begin{equation}\label{3.3.121}
0\leq x-\left( \sum _{k=0}^{m_0}a_k^+-\sum _{k=0}^{n_0}a_k^-\right) <a_{n_0}^-.
\end{equation}
Do this again:  let $m_1$ and $n_1$ be the smallest natural numbers such that
\begin{equation}\label{3.3.130}
\sum _{k=0}^{m_0}a_k^+-\sum _{k=0}^{n_0}a_k^-+\sum _{k=m_0+1}^{m_1}a_k^+\geq y
\end{equation}
and
\begin{equation}\label{3.3.131}
\sum _{k=0}^{m_0}a_k^+-\sum _{k=0}^{n_0}a_k^-+\sum _{k=m_0+1}^{m_1}a_k^+-\sum _{k=n_0+1}^{n_1}a_k^-\leq x
\end{equation}
respectively.  (Of course, inequalities analogous to \eqref{3.3.119} and \eqref{3.3.121} hold here as well.)  Continue this process inductively.  The series
\begin{equation}
\begin{multlined}
\sum _{k=0}^{m_0}a_k^+-\sum _{k=0}^{n_0}a_k^-+\sum _{k=m_0+1}^{m_1}a_k^+-\sum _{k=n_0+1}^{n_1}a_k^- \\ +\sum _{k=m_1+1}^{m_2}a_k^+-\sum _{k=n_1+1}^{n_2}a_k^-+\cdots
\end{multlined}
\end{equation}
is a rearrangement of the original series.  Denote the partial sums of this series by $S_m$.  Thus, the inequalities \eqref{3.3.119} and \eqref{3.3.121}, in terms of $S_m$, look like\footnote{Note that I can replace $m_0$ with $m\leq m_0$ on the \emph{left-hand} side of \eqref{3.3.119} and the inequality still remains valid.  Similarly, I can replace $n_0$ with $m\leq n_0$ on the \emph{left-hand} side of \eqref{3.3.121} and the inequality still remains valid.}
\begin{equation}
0\leq S_m-y<a_{i_m}^+\text{ and }0\leq x-S_m<a_{j_m}^-,
\end{equation}
where $i,j:\N \rightarrow \N$ are strictly increasing functions of $m$ (these are the $m_k$ and $n_k$s).  Hence,
\begin{subequations}\label{3.3.134}
\begin{align}
0\leq \sup _{m\geq m_0}\{ S_m\}-y< & \sup _{m\geq m_0}\{ a_{i_m}^+\} \\
0\leq x-\inf _{m\geq m_0}\{ S_m\} < & \inf _{m\geq m_0}\{ a_{j_m}^-\}
\end{align}
\end{subequations}
Recall however that $\sum _{m\in \N}a_m$ converges.  It follows that $\lim _ma_m=0$, and so in turn $\lim _ma_m^+=0=\lim _ma_m^-$.  Thus, taking the limit of \eqref{3.3.134} with respect to $m_0$, we obtain $\liminf _mS_m=x$ and $\limsup _mS_m=y$.

\begin{exr}[breakable=false]{}{}
Modify this argument to prove the case where at least one of $x$ or $y$ is not finite.
\end{exr}
\end{proof}
\end{thm}

\subsection{Subnets and subsequences}

The concept of a subnet is \emph{almost} what you think it should be.  To help understand the concept before we go to the precise definition, let's think of what subnets of \emph{sequences} should be.

Let $x\colon \N \rightarrow \R$ be a sequence and let $S\subseteq \N$.  When should $\restr{x}{S}$ be a \emph{subsequence} of the original $m\mapsto x_m$?  Well certainly if $S$ is finite, we should not consider $\restr{x}{S}$ to be a subsequence---we need the indices to get arbitrarily large.  Moreover, whatever our definition of subsequence is, it should have the property that, if $m\mapsto x_m$ converges to $x_\infty$, then every subsequence of $m\mapsto x_m$ should converge to $x_\infty$ as well.  If $S$ is allowed to be finite, then of course this will not be the case.  For example, if we allowed this, $(0)$ would be a subsequence of $\coord{0,1,1,1,\ldots}$.  This is just silly.  Thus, a key requirement is that elements of $S$ have to be able to become arbitrarily large.

Now let $\Lambda$ be a general directed set and let $\Lambda '\subseteq \Lambda$ be a subset whose elements are arbitrarily large.  (Precisely, this means that, for all $\lambda \in \Lambda$, there is some $\mu \in \Lambda '$ such that $\mu \geq \lambda$.)  One way to see we need to make this requirement is because, without this requirement, $\Lambda '$ would not itself be a directed set in general.  However, if we do require the elements of $S$ to be arbitrarily large, then $\Lambda '$ will be a directed set, and so $\restr{a}{S}$ will indeed be a net, and certainly it will turn-out that $\restr{a}{\Lambda '}$ is a subnet of $a$.  However, it is \emph{not} the case that every subnet of $\lambda \mapsto x_\lambda$ is of this form.  The reason for this is ultimately because, if we don't allow for more general subnets, then theorems we want to be true will fail to be true (see, for example, the proofs of \cref{prp3.4.56,KelleysConvergenceTheorem}).
\begin{dfn}{Subnet}{dfnSubnet}
Let $x\colon \Lambda \rightarrow \R$ be a be a net.  Then, a \term{subnet}\index{Subnet} of $a$ is a net $b:\Lambda '\rightarrow \R$ such that
\begin{enumerate}
\item \label{enmSubnet.i}for all $\mu \in \Lambda '$, $y_\mu =x_{\lambda _\mu}$ for some $\lambda _\mu \in \Lambda$; and
\item \label{enmSubnet.ii}whenever $U\subseteq \R$ eventually contains $x$, it eventually contains $y$.
\end{enumerate}
A \term{subsequence}\index{Subsequence} is a subnet that is a sequence.
\begin{rmk}
In words, a subnet of a net is a net whose terms are all terms from the original net and is eventually contained in every set that eventually contains the original net.
\end{rmk}
\begin{rmk}
See the following proposition (\cref{prp2.4.162}) for an equivalent way to state this, that could possibly be more intuitive for you.
\end{rmk}
\begin{rmk}
Recall (see the paragraphs preceding this definition) that our definition should have the property that, if $\lambda \mapsto x_{\lambda}$ converges to $x_{\infty}$, then all subnets should likewise converge to $x_{\infty}$.  \cref{enmSubnet.ii} is precisely the condition that guarantees this.  See \cref{prp3.3.61} for the precise statement and proof.
\end{rmk}
\begin{rmk}
There are at least two definitions in the literature that are distinct from this one.  Our definition is strictly weaker than both of them (see \cref{prp3.3.92,exm3.3.93,prp3.3.93,exr3.3.94}).  These definitions are not so good because they do not correspond precisely to the notion of filterings and filters (see \crefnameref{sct4.4}).\footnote{You are neither supposed to know what these are yet nor why this is significant.}  This definition also makes a few proofs easier (see, for example, the proofs of \cref{prp3.4.56,KelleysConvergenceTheorem}).  I also personally find this definition easier to understand than either of the ones given in \cref{prp3.3.92} or \cref{prp3.3.93}.  As nets seem not to be taught very often at least in part because of the fact that the definition of a subnet is a bit tricky,\footnote{Or at least, this is the impression that I have gotten.} I think it is quite important to make this definition in particular as clean as possible.
\end{rmk}
\begin{rmk}
If $y\colon \Lambda '\rightarrow \R$ is a subnet of a net $x\colon \Lambda \rightarrow \R$, then, by \cref{enmSubnet.i}, it follows that there is some function $\iota :\Lambda '\rightarrow \Lambda$ such that $y=x\circ \iota$.  However, \emph{in general there will not be a unique such function}.  This almost never matters, and it is customary to write $y_{\mu}=x_{\iota (\mu)}\coloneqq x_{\lambda _\mu}$ for some noncanonically chosen $\iota$.
\end{rmk}
\begin{rmk}
Notwithstanding the fact that our definition of subnet is nonstandard, our definition of \emph{subsequence} would still be slightly different than most authors.  The primary reason for this is because typically people do not introduce nets in a first analysis course, in which case the `naive' definition of subsequence, i.e., a subnet of the form $\restr{x}{S}$, works.  For them, subsequences are what we would call \emph{cofinal subnets} of a sequence (see \cref{StrictSubnet}).  In particular, we allow for `repeats' whereas most authors will not, e.g.,~we consider $\coord{1,1,1,2,3,\ldots}$ to be a subsequence of $\coord{1,2,3,4,5,\ldots}$.
\end{rmk}
\end{dfn}
\begin{prp}{}{prp2.4.162}
Let $\lambda \mapsto x_{\lambda}\in \R$ be a net.  Then, $\mu \mapsto x_{\lambda _{\mu}}$ is a subnet of $\lambda \mapsto x_{\lambda}$ iff for all $\lambda _0$ there is some $\mu _0$ such that
\begin{equation}
\left\{ x_{\lambda _{\mu}}:\mu \geq \mu _0\right\} \subseteq \left\{ x_{\lambda}:\lambda \geq \lambda _0\right\} .
\end{equation}
\begin{proof}
$(\Rightarrow )$ Suppose that $\mu \mapsto x_{\lambda _{\mu}}$ is a subnet of $\lambda \mapsto x_{\lambda _{\mu}}$.  Let $\lambda _0$ be arbitrary.  Of course, $\{ x_{\lambda}:\lambda \geq \lambda _0\}$ eventually contains $\lambda \mapsto x_{\lambda}$, and so by the definition of a subnet, it eventually contains $\mu \mapsto x_{\lambda _{\mu}}$.  That is, there is some $\mu _0$ such that, whenever $\mu \geq \mu _0$, it follows that $x_{\mu}\in \{ x_{\lambda}:\lambda \geq \lambda _0\}$.  In other words,
\begin{equation}
\left\{ x_{\lambda _{\mu}}:\mu \geq \mu _0\right\} \subseteq \left\{ x_{\lambda}:\lambda \geq \lambda _0\right\} .
\end{equation}

\blankline
\noindent
Suppose that for all $\lambda _0$ there is some $\mu _0$ such that
\begin{equation}
\left\{ x_{\lambda _{\mu}}:\mu \geq \mu _0\right\} \subseteq \left\{ x_{\lambda}:\lambda \geq \lambda _0\right\} .
\end{equation}
Let $U\subseteq \R$ eventually contain $\lambda \mapsto x_{\lambda}$.  This means that there is some $\lambda _0$ such that, whenever $\lambda \geq \lambda _0$, it follows that $x_{\lambda}\in U$.  In other words, $\{ x_{\lambda}:\lambda \geq \lambda _0\} \subseteq U$.  By hypothesis, there is some $\mu _0$ such that $\left\{ x_{\lambda _{\mu}}:\mu \geq \mu _0\right\} \subseteq \{ x_{\lambda}:\lambda \geq \lambda _0\}$.  Hence, $\left\{ x_{\lambda _{\mu}}:\mu \geq \mu _0\right\} \subseteq U$, that is, whenever $\mu \geq \mu _0$, it follows that $x_{\lambda _{\mu}}\in U$.  In other words, $\mu \mapsto x_{\lambda _{\mu}}$ is eventually contained in $U$.  By definition then, $\mu \mapsto x_{\lambda _{\mu}}$ is a subnet of $\lambda x_{\lambda}$.
\end{proof}
\end{prp}
As subnets of the more `naive type' are still quite important, we do given them a special name.
\begin{dfn}{Cofinal subset}{}
Let $S$ be a subset of a preordered set $X$.  Then, $S$ is \term{cofinal}\index{Cofinal} iff for every $x\in X$ there is some $s\in S$ such that $s\geq x$.
\begin{rmk}
Of course, saying that a subset is cofinal is just our fancy-schmancy way of saying that the elements are arbitrarily large.
\end{rmk}
\end{dfn}
One way to see that we need elements to grow arbitrarily large is because, in a directed set, the subset will itself be directed.
\begin{dfn}{Cofinal subnet}{StrictSubnet}
A \term{cofinal subnet}\index{Cofinal subnet} of a net $x\colon \Lambda \rightarrow \R$ is a subnet of the form $\restr{a}{\Lambda '}:\Lambda '\rightarrow \R$ for $\Lambda '\subseteq \Lambda$ cofinal.
\begin{exr}[breakable=false]{}{}
Let $a\colon \Lambda \rightarrow \R$ be a net and let $\Lambda '\subseteq \R$.  Show that if $\Lambda '$ is cofinal, then $\restr{a}{\Lambda '}\colon \Lambda '\rightarrow \R$ is a subnet of $a\colon \Lambda \rightarrow \R$.
\begin{rmk}
Thus, our definition does in fact make sense.
\end{rmk}
\end{exr}
\begin{rmk}
If $\Lambda '\subseteq \Lambda$ is cofinal, we say that $\Lambda '$ \emph{defines} the cofinal subnet $\restr{x}{\Lambda '}\colon \Lambda '\rightarrow \R$.
\end{rmk}
\begin{rmk}
One key difference between the definition of a subnet and the more `naive' definition (that is, if one were only to allow \emph{cofinal} subnets) is that you are allowed to repeat elements in a subnet, for example, $\coord{0,0,1,2,3,\ldots}$ is a subnet of $\coord{0,1,2,3,\ldots}$, but \emph{not} a cofinal subnet.
\end{rmk}
\end{dfn}

We mentioned in the definition of a subnet, \cref{dfnSubnet}, that there are at least two definitions of subnets in the literature that are distinct from ours.  Our definition is strictly weaker than both of these, as we now show.
\begin{prp}{}{prp3.3.92}
Let $a\colon \Lambda \rightarrow \R$ and $b\colon \Lambda '\rightarrow \R$ be nets.  Then, if there is a function $\iota \colon \Lambda '\rightarrow \Lambda$ such that (i)~$b=a\circ \iota$ and (ii)~for all $\lambda \in \Lambda$ there is some $\mu _0\in \Lambda '$ such that, whenever $\mu \geq \mu _0$, it follows that $\iota (\mu )\geq \lambda$, then $b$ is a subnet of $a$.
\begin{rmk}
Thus, in different notation, a subnet of $\lambda \mapsto x_\lambda$ is a net of the form $\mu \mapsto x_{\lambda _\mu}$, where the function $\mu \mapsto \lambda _\mu$ has the property that, for all $\lambda _0$, there is some $\mu _0$ such that, whenever $\mu \geq \mu _0$, it follows that $\lambda _\mu \geq \lambda _0$.  Note that, of course, in this case, there \emph{is} a canonically chosen $\iota :\Lambda '\rightarrow \Lambda $ (confer the remarks in the definition of a subnet, \cref{dfnSubnet}).
\end{rmk}
\begin{rmk}
This is sometimes taken as the definition of a subnet (for example, see \cite[pg.~70]{Kelley}).  Our definition is strictly weaker than this one as the following example shows.  This definition is more or less perfectly okay for almost all purposes.  The reason we have chosen the definition we have over this one (aside from the fact that it makes some proofs slightly easier), is that it is more natural in the sense that our definition is the one that corresponds to the analogous notion with filters (see \crefnameref{sct4.4}).
\end{rmk}
\begin{proof}
Suppose that there is a function $\iota :\Lambda '\rightarrow \Lambda$ such that (i)~$b=a\circ \iota$ and (ii)~for all $\lambda \in \Lambda$ there is some $\mu _0\in \Lambda '$ such that, whenever $\mu \geq \mu _0$, it follows that $\iota (\mu )\geq \lambda$. Let $U\subseteq \R$ eventually contain $a$.  Then, there is some $\lambda _0\in \Lambda$ such that, whenever $\lambda \geq \lambda _0$, it follows that $x_\lambda \in U$.  Let $\mu _0\in \Lambda '$ be such that, whenever $\mu \geq \mu _0$, it follows that $\iota (\mu )\eqqcolon \lambda _\mu \geq \lambda _0$.  Thus, whenever $\mu \geq \mu _0$, it follows that $x_{\lambda _\mu}\in U$, so that $\mu \mapsto x_{\lambda _\mu}$ is eventually contained in $U$, and hence is a subnet of $\lambda \mapsto x_\lambda$.
\end{proof}
\end{prp}
\begin{exm}{A subnet that would not be a subnet in the sense of \cref{prp3.3.92}}{exm3.3.93}
Consider the constant sequence $m\mapsto x_m\coloneqq 0$ and define $\iota :\N \rightarrow \N$ by $\iota (n)\coloneqq 0$.  Then, $n\mapsto x_{\iota (n)}=0$, and so is certainly eventually contained in every set which eventually contains $a$, and so is a subnet.  On the other hand, $\iota$ definitely does not satisfy (ii)~of \cref{prp3.3.92}.  Thus, this is an example of a subnet which would not be considered a subnet if we had taken the conditions in \cref{prp3.3.92} as our definition of a subnet.
\end{exm}
And now we come to the second definition that is sometimes in the literature.
\begin{prp}{}{prp3.3.93}
Let $a\colon \Lambda \rightarrow \R$ and $b\colon \Lambda '\rightarrow \R$ be nets.  Then, if there is a function $\iota \colon \Lambda '\rightarrow \Lambda$ such that (i)~$b=a\circ \iota$, (ii)~is nondecreasing, and (iii)~has cofinal image, then $b$ is a subnet of $a$.
\begin{rmk}
This is sometimes taken as the definition of a subnet (for example, this is currently\footnote{6 July 2015} the definition given on Wikipedia).  The definition that is sometimes given as described in \cref{prp3.3.92} is strictly weaker than this definition (see the next exercise), and so in turn our definition is strictly weaker than this definition.  This definition also fails to exactly correspond to the analogous notion with filters, but, unlike the `definition' of \cref{prp3.3.92}, using this definition can actually make things quite a bit more difficult.\footnote{Or maybe even impossible?  I don't know because I don't use this definition.}  The problem is that we often construct a subnet of the form in \cref{prp3.3.92}, and then showing that $\iota$ is nondecreasing is an extra step at best and requires modification of the subnet at worst.  As far as I am aware, there is just no good reason to use as a definition.
\end{rmk}
\begin{proof}
We apply the previous proposition.  Let $\lambda \in \Lambda$ be arbitrary.  Then, because $\iota$ has cofinal image, there is some $\mu _0\in \Lambda '$ such that $\iota (\mu _0)\geq \lambda$.  Because $\iota$ is nondecreasing, it follows that, if $\mu \geq \mu _0$, then $\iota (\mu )\geq \iota (\mu _0)\geq \lambda$.
\end{proof}
\end{prp}
\begin{exr}{A subnet that would not be a subnet in the sense of \cref{prp3.3.93}}{exr3.3.94}
Find a net $x\colon \Lambda \rightarrow \R$, a directed set $\Lambda '$ with function $\iota :\Lambda '\rightarrow \Lambda$ that has the property that, for all $\lambda \in \Lambda$, there is some $\mu _0\in \Lambda '$ such that, whenever $\mu \geq \mu_0$, it follows that $\iota (\mu )\geq \lambda$, but yet either (i)~is not nondecreasing or (ii)~does not have cofinal image.
\begin{rmk}
That is to say, the conditions of \cref{prp3.3.92} are strictly weaker than the conditions of \cref{prp3.3.93}.
\end{rmk}
\end{exr}

\begin{exm}{}{}
\begin{enumerate}
\item $\coord{0,0,0,\ldots}$ is a subsequence of $\coord{0,1,0,1,0,1,\ldots}$.
\item $\coord{0,2,4,,\ldots}$ is a subsequence of $\coord{0,1,2,\ldots}$.
\item $\coord{1,1,1,\ldots}$ is \emph{not} a subsequence of $\coord{0,1,2,\ldots}$.
\item $\coord{0,1,4,9,16,\ldots}$ is a subsequence of $\coord{0,1,2,\ldots}$.
\end{enumerate}
\end{exm}

We mentioned when we defined the terminology ``frequently XYZ'' (\cref{FrequentlyXYZ}) that one of its uses is that in a net which is frequently XYZ, the terms which are actually XYZ define a cofinal subnet.  Now that we know what a cofinal subnet is, we can return to this.
\begin{mpr}{}{mpr2.4.194}
Let $\Lambda \ni \lambda \mapsto x_{\lambda}$.  Then, if $\lambda \mapsto x_{\lambda}$ is frequently XYZ, then
\begin{equation}
\left\{ \lambda \in \Lambda :x_{\lambda}\text{ is XYZ.}\right\}
\end{equation}
defines a cofinal subnet of $\lambda \mapsto x_{\lambda}$.
\begin{proof}
We must show that $\Lambda '\ceqq \left\{ \lambda \in \Lambda :x_{\lambda}\text{ is XYZ.}\right\}$ is cofinal in $\Lambda$.  So, let $\lambda _0\in \Lambda$.  By the definition of frequently (\cref{FrequentlyXYZ}), this means that there is some $x_{\lambda}$ that is XYZ, for $\lambda \geq \lambda _0$.  That is, $\lambda \in \Lambda '$, as desired.
\end{proof}
\end{mpr}

The following two results, while neither particularly difficult nor deep, are important because they become an axioms of the convergence definition of topological spaces---see \namerefpcref{KelleysConvergenceTheorem}.
\begin{prp}{}{prp3.3.61}
Let $\mu \mapsto x_{\lambda _\mu}$ be a subnet of a net $\lambda \mapsto x_\lambda$.  Then, if $\lim _{\lambda}x_{\lambda}=x_{\infty}$, then $\lim _{\mu}x_{\lambda _{\mu}}=x_{\infty}$.
\begin{proof}
Suppose that $\lim _{\lambda}x_{\lambda}=x_{\infty}$.  Let $\varepsilon >0$.  Then, as $\lim _{\lambda}x_{\lambda}=x_{\infty}$, $B_{\varepsilon}(x_{\infty})$ eventually contains $\lambda \mapsto x_{\lambda}$.  By the definition of a subnet, $B_{\varepsilon}(x_{\infty})$ then eventually contains $\mu \mapsto x_{\lambda _{\mu}}$.  By definition, this means that $\lim _{\mu}x_{\lambda _{\mu}}=x_{\infty}$.
\end{proof}
\end{prp}
\begin{prp}{}{prp3.3.95}
Let $\lambda \mapsto x_\lambda$ be a net.  Then, if every cofinal subnet $\mu \mapsto x_{\lambda _\mu}$ has in turn a subnet itself $\nu \mapsto x_{\lambda _{\mu _\nu}}$ such that $\lim _\nu x_{\lambda _{\mu _\nu}}=x_\infty$, then $\lim _\lambda x_\lambda =x_\infty$.
\begin{rmk}
The converse is true too of course by the previous result.
\end{rmk}
\begin{proof}
Suppose that every cofinal subnet $\mu \mapsto x_{\lambda _\mu}$ has in turn a subnet itself $\nu \mapsto x_{\lambda _{\mu _\nu}}$ such that $\lim _\nu x_{\lambda _{\mu _\nu}}=x_\infty$.  We proceed by contradiction:  suppose that it is not the case that $\lim _\lambda x_\lambda =x_\infty$.  Then,
\begin{textequation}[3.3.63]
there is some $\varepsilon _0>0$ such that for all $\lambda$ there is some $\mu _\lambda \geq \lambda$ such that $\abs{x_{\mu _\lambda}-x_\infty}\geq \varepsilon _0$.
\end{textequation}
Define $S\coloneqq \left\{ \mu _\lambda :\lambda \right\}$ and denote by $\lambda \mapsto x_{\mu _\lambda}$ the corresponding cofinal subnet, so that $\abs{x_{\mu _\lambda}-x_\infty}\geq \varepsilon _0$ for all $\lambda$.\footnote{Note that the index here $\mu _{\lambda}$ is not the usual.  Of course, it makes no difference---$\lambda$ and $\mu$ are just letters---we point this out simply because this can be an easy-to-miss detail if you're reading fast.}

We wish to show that $\lambda \mapsto x_{\mu _\lambda}$ has no subnet which converges to $x_\infty$.  This will be a contradiction, thereby proving the result.  To show this itself, we again proceed by contradiction:  suppose there is some subnet $\nu \mapsto x_{\mu _{\lambda _\nu}}$ of $\lambda \mapsto x_{\mu _\lambda}$ that converges to $x_\infty$.  By \eqref{3.3.63}, we have that $\abs{x_{\mu _{\lambda _{\nu}}}-x_{\infty}}\geq \varepsilon _0$, or rather, $x_{\mu _{\lambda _{\nu}}}\in B_{\varepsilon _0}(x_{\infty})^{\comp}$, for all $\nu$.  But then $\nu \mapsto x_{\mu _{\lambda _{\nu}}}$ is certainly not eventually contained in $B_{\varepsilon _0}(x_{\infty})$:  a contradiction.
\end{proof}
\end{prp}
The following result, together with \cref{prp3.3.61,prp3.3.95} (and the fact that constant nets converge to that constant) turn out to be sufficient to characterize the topology on a topological space.  Before we present it, however, we must first define an order on a product of preordered sets.
\begin{dfn}{Product order}{ProductOrder}
Let $\collection{P}$ be a collection of preordered sets and let $x,y\in \prod _{P\in \mcal{P}}P$.  Then, we define $x\leq _{\mcal{P}}y$ iff $x_P\leq _Py_P$ for all $P\in \mcal{P}$.
\begin{exr}[breakable=false]{}{}
Show that $\leq _{\mcal{P}}$ is a preorder.
\end{exr}
\begin{rmk}
In any category, there is a notion of a \emph{product}.  It turns out that this is in fact the product in the category of preordered sets $\Pre$.
\end{rmk}
\end{dfn}
\begin{exr}{}{}
If $\collection{P}$ is a collection of directed sets, show that $\coord{\prod _{P\in \collection{P}}P,\leq _{\collection{P}}}$ is a directed set.
\end{exr}
\begin{prp}{}{prp3.3.154}
Let $I$ be a directed set\footnote{``$I$'' is for ``index''.} and for each $i\in I$ let $x^i:\Lambda ^i\rightarrow \R$ be a convergent net.  Then, if $(x^\infty )_\infty \coloneqq \lim _i\lim _\lambda (x^i)_\lambda$ exists, then $I\times \prod _{i\in I}\Lambda ^i\ni \coord{i,\lambda}\mapsto (x^i)_{\lambda ^i}$ converges to $(x^\infty )_\infty$.
\begin{rmk}
In other words, if you have a `net\textquotesingle s worth of nets' such that the iterated limit converges, then you can write this limit as a limit of a single net (which itself is formed from the `net\textquotesingle s worth of nets').
\end{rmk}
\begin{rmk}
Note that if you insist upon working with only sequences, you haven't a chance in hell to make something like this work.\footnote{Okay, perhaps I'm not being completely fair to the sequence-loyalists out there.  In general, I imagine you \emph{should} be able to conjure up a single sequence from the $(a^n)_m$s which converges to $(x^{\infty})_{\infty}$; however, there's no way you're going to get an \emph{explicit formula} for that sequence that's going to work all the time---this result is nice in that it gives a very explicit expression for \emph{any} `net\textquotesingle s worth of nets' you might come up with.}  In fact, recall our counter-example (\cref{exm3.3.73}), in which we had $\lim _m (a^n)_m=0$ for all $n\in \N$, and so of course we had that $\lim _n\left( \lim _m(x^n)_m\right)$ existed (and was equal to $0$).  In fact, the same was true with $m$ and $n$ reversed.  We then hoped that $\lim _m(x^m)_m=0$, but found that this was not in fact the case.  This result tells us that you can indeed form a net from a nets worth of convergent nets that converges to the thing you would like to, the catch being that the answer is not as nice as one might have liked.
\end{rmk}
\begin{rmk}
As messy as this answer might seem, in some sense, it's the best we could do.  Can you write down any other net at all formed from just the $(x^\lambda )_{\lambda ^\mu}$s?  As all the directed sets $\Lambda ^i$ are in general distinct, this is essentially the simplest thing we can write down.
\end{rmk}
\begin{proof}
Suppose that $(x^\infty )_\infty \coloneqq \lim _i\lim _\lambda (x^i)_\lambda$ exists.  Let $\varepsilon >0$.  Let $i_0\in I$ be such that, whenever $i\geq i_0$, it follows that
\begin{equation}
\left| \lim _\lambda (x^i)_\lambda -(x^\infty )_\infty \right| <\varepsilon .
\end{equation}
Define
\begin{equation}
(x^i)_\infty \coloneqq \lim _\lambda (x^i)_\lambda .
\end{equation}
Let $\lambda ^i_0\in \Lambda ^i$ be such that whenever $\lambda ^i\geq \lambda ^i_0$ it follows that
\begin{equation}
\left| (x^i)_{\lambda ^i}-(x^i)_\infty \right| <\varepsilon .
\end{equation}
Then, whenever $\coord{i,\lambda }\geq \coord{i_0,\lambda _0}$, by definition of the product order, $i\geq i_0$ and $\lambda ^i\geq \lambda ^i_0$ for all $i\in I$, and so
\begin{equation}
\begin{split}
\left| (x^i)_{\lambda ^i}-(x^\infty )_\infty \right| \leq & \left| (x^i)_{\lambda ^i}-(x^i)_\infty \right| \\ & \qquad +\left| (x^i)_\infty -(x^\infty )_\infty \right| \\
& <\varepsilon +\varepsilon =2\varepsilon .
\end{split}
\end{equation}
\end{proof}
\end{prp}

We end this section with a result on the relationship of limit superiors and limit inferiors of nets to their subnets.
\begin{prp}{}{}
Let $\mu \mapsto x_{\lambda _\mu}$ be a subnet of a net $\lambda \mapsto x_\lambda$.  Then,
\begin{equation*}
\liminf _\lambda x_\lambda \leq \liminf _\mu x_{\lambda _\mu}\leq \limsup _\mu x_{\lambda _\mu}\leq \limsup _\lambda x_\lambda .
\end{equation*}
\begin{proof}
We already know the middle inequality holds from \cref{exr3.3.50}.  We prove the $\limsup$ inequality holds; the other is similar.

Let $\lambda _0$ be an arbitrary index.  Then, by \cref{prp2.4.162}, there is some $\mu _0$ such that
\begin{equation}
\left\{ x_{\lambda _\mu}:\mu \geq \mu _0\right\} \subseteq \left\{ x_\lambda :\lambda \geq \lambda _0\right\} .
\end{equation}
Hence
\begin{equation}
\sup _{\mu \geq \mu _0}\{ x_{\lambda _\mu}\} \leq \sup _{\lambda \geq \lambda _0}\left\{ x_\lambda \right\} .
\end{equation}
It follows from the definition of the limit superior \eqref{3.3.50} and the \namerefpcref{exr3.3.30} that $\limsup _\mu x_{\lambda _\mu}\leq \lim _\lambda x_\lambda$.
\end{proof}
\end{prp}

\section{Basic topology of the Euclidean space}

Though we have not defined it yet (and will not do so until the next chapter), a \emph{topological space} is the most general context in which one can make precise the notion of \emph{continuity}.\footnote{This is arguably a slight lie, but in any case, I think it's fair to say that continuity is really the point of working with topological spaces.}  The point is, if continuity is something you care about, then topology in turn is something you should also care about.

In this last section, we work in not $\R$ itself, but rather $\R ^d\coloneqq \underbrace{\R \times \cdots \times \R}_d$, \term{Euclidean space}\index{Euclidean space}.  The reason we do so is because (i)~basically everything works verbatim if you just replace $\R$ with $\R ^d$ and (ii)~I think picturing these concepts in $\R ^2$ or $\R ^3$ is more enlightening than picturing them in $\R$.  There are a couple of places in which we will need to reference old concepts that we have not technically defined in $\R ^d$ (e.g.~limits of nets).  However, the correct definition in all such cases, unless otherwise stated, is given simply by replacing ``$\R$'' with ``$\R ^d$''.  If you're still bothered by this, just pretend $d=1$.

Most of the structure of $\R$ generalizes to $\R ^d$ in a straightforward manner.\footnote{Though upon generalization we lose a lot of the nice properties we had in $\R$!}.  For example, for $x,y\in \R ^d$,
\begin{equation}
x+y\coloneqq \coord{x_1+y_1,\ldots ,x_d+y_d}\in \R ^d,
\end{equation}\index[notation]{$x+y$}
where $x_k$ and $y_k$ are the \emph{coordinates} of $x$ and $y$ respectively---see \cref{CartesianProductCollection}.  You can define multiplication similarly if you like, but this is not common.  Upon doing so, we obtain a cring that is \emph{not even integral} (e.g.~$\coord{1,0}\cdot \coord{0,1}=\coord{0,0}$), much less a field.  We can also equip $\R ^d$ with the product order, but this will only given us a \emph{partially-ordered} cring, not a \emph{totally-ordered} cring (e.g.~$\coord{1,0}$ and $\coord{0,1}$ are not comparable).

There is also the issue of the \namerefpcref{AbsoluteValue}.  For $x\in \R ^d$, we define
\begin{equation}
\abs{x}\coloneqq \sqrt{\abs{x_1}^2+\cdots +\abs{x_d}^2}.\index[notation]{$B_{\varepsilon}(x_0)$}\footnote{It is common for people to write $\| x\|$\index[notation]{$\| x \|$} when working in higher dimensions.  I'm not sure why, as this agrees with the usual absolute value when $d=1$, and so there isn't any ambiguity in writing $\abs{x}$.}\footnote{The absolute values here around the $x_k$s aren't strictly necessary here, though they will be when you work in $\C ^d$, and so we put them here anyways.}
\end{equation}
The motivation for such a definition comes of course from the Pythagorean Theorem\index{Pythagorean Theorem}, though it is certainly not the only choice we could have made---see, for example, the \namerefpcref{LpNorm}.  The definitions of $B_{\varepsilon}(x_0)$ and $D_{\varepsilon}(x_0)$ (given in the remark of \cref{AbsoluteValue}) generalize accordingly.

\subsection{Continuity}

Roughly speaking, a continuous function is a function which `preserves limits', that is, if you take the limit as $x$ goes to $a$, then $f(x)$ goes to $f(a)$.  Thus, to formulate the definition of continuity, we must first say what we mean by limit of a function (so far, we have only said what it means to take the limit of a \emph{net}).  In turn, the notion of a \emph{limit point} will make this definition slightly easier to formulate (it's also quite an important concept in its own right).
\begin{dfn}{Limit point}{LimitPoint}
Let $S\subseteq \R ^d$ and let $x_0\in \R ^d$.  Then, $x_0$ is a \term{limit point}\index{Limit point} of $S$ iff there exists a net $\lambda \mapsto x_\lambda \in S$ with $x_\lambda \neq x_0$ such that $\lim _\lambda x_\lambda =x_0$.
\begin{rmk}
The reason for the requirement $x_\lambda \neq x_0$ is two-fold:  first of all, we need to make this same requirement in the definition of a limit of a function for reasons explained there, and second of all, we need this condition for the notion of a limit point to agree with the notion of an accumulation point---see \cref{prp3.4.21}.  (See the respective definitions, \cref{dfn3.4.1,dfn3.4.20}, for an explanation of why we make the corresponding conditions in these definitions.)
\end{rmk}
\end{dfn}
\begin{dfn}{Limit (of a function)}{dfn3.4.1}
Let $D\subseteq \R ^d$,\footnote{``$D$'' is for ``domain''.} let $x_0\in \R ^d$ be a limit point of $D$, let $f\colon D\rightarrow \R ^e$, and let $L\in \R ^e$.  Then, $y$ is a \term{limit}\index{Limit (of a function)} of $f$ at $x_0$ iff for every net $\lambda \mapsto x_\lambda \in D$ such that (i)~$x_\lambda \neq x_0$ and (ii)~$\lim _\lambda x_\lambda =x_0$ we have $\lim _\lambda f(x_\lambda )=y$.
\begin{exr}{}{}
Let $y,y'\in \R ^e$ be limits of the function $f\colon D\rightarrow \R ^e$ at $x_0\in \R ^d$.  Show that $y=y'$.
\end{exr}
\begin{rmk}
If $y$ is the limit of $f$ at $x_0$, then we write $\lim _{x\to x_0}f(x)=y$\index[notation]{$\lim _{x\to x_0}f(x)=y$}.  Note that this is unambiguous by the previous exercise.
\end{rmk}
\begin{rmk}
That $x_0\in \R ^d$ is a limit point of $D$ guarantees that there is at least one such net.  This rules out the possibility of the condition being fulfilled vacuously.  For example, in the stupid case in which the domain $S$ is a point, if we didn't require that $a$ be a limit point,\footnote{Note that, for $D=\{ x_0\}$, $a$ is \emph{not} a limit point of $D$.} then (vacuously) every $y\in \R ^e$ would be a limit of $f$ at $x_0$, which in particular would destroy uniqueness.
\end{rmk}
\begin{rmk}
Consider the function $f\colon \R \rightarrow \R$ defined by
\begin{equation}
f(x)\coloneqq \begin{cases}0 & \text{if }x\neq 0 \\ 1 & \text{if }x=0.\end{cases}
\end{equation}
Then we \emph{would like} to say that $\lim _{x\to 0}f(x)=0$.  This is the motivation for imposing the constraint $x_\lambda \neq x_0$.  Because, for example, the constant net $\lambda \mapsto x_\lambda \coloneqq x_0$ does \emph{not} satisfy $\lim _\lambda f(x_\lambda )=0$.
\end{rmk}
\begin{rmk}
There are several equivalent ways to state the definition of a limit of a function (see the following exercise, for example), and it's quite possible that this is not the one you've seen taken as a definition before.  Our motivation for taking this as our definition is that it makes it quite easy to carry over our knowledge about limits of nets to limits of functions.
\end{rmk}
\end{dfn}
The following equivalent condition is also commonly taken as the definition of a limit, the so-called ``$\varepsilon$-$\delta$''\index{$\varepsilon$-$\delta$ definition of a limit} definition of a limit.
\begin{exr}{}{}
Let $D\subseteq \R ^d$, let $x_0\in \R ^d$ be a limit point of $D$, let $f\colon D\rightarrow \R ^e$, and let $y\in \R ^e$.  Show that $\lim _{x\to x_0}f(x)=y$ iff for every $\varepsilon >0$ there is some $\delta >0$ such that, whenever $0<\abs{x-x_0}<\delta, x\in D$, it follows that $\abs{f(x)-y}<\varepsilon$.
\begin{rmk}
The intuition is as follows:  No matter how close we want to make $f(x)$ to $y$, we can do so by making $x$ sufficiently close to $x_0$.
\end{rmk}
\begin{rmk}
The motivation for the condition $0<\abs{x-x_0}$ is the same as the motivation for the condition $x_\lambda \neq x_0$ in the previous definition.
\end{rmk}
\end{exr}
Of course, a lot of the results we had for limits of nets have analogues for limits of functions.  In particular, there are versions of the \nameref{AlgebraicLimitTheorems} and the \nameref{exr3.3.30} for limits of functions.
\begin{prp}{Algebraic Limit Theorems (for \\ functions)}{AlgebraicLimitTheoremsForFunctions}\index{Algebraic Limit Theorems (for functions)}
Let $D\subseteq \R ^d$, let $x_0\in \R ^d$ be a limit point of $D$, and let $f,g\colon D\rightarrow \R ^e$ both have limits at $x_0$.  Then,
\begin{enumerate}
\item $\lim _{x\to x_0}[f(x)+g(x)]=\lim _{x\to x_0}f(x)+\lim _{x\to x_0}g(x)$;
\item $\lim _{x\to x_0}[f(x)g(x)]=\left( \lim _{x\to x_0}f(x)\right) \left( \lim _{x\to x_0}g(x)\right)$;
\item $\lim _{x\to x_0}\frac{1}{f(x)}=\frac{1}{\lim _{x\to x_0}f(x)}$ if $\lim _{x\to x_0}f(x)\in [\R ^e]^{\times}$\footnote{Recall that (\eqref{eqnA.1.34}) the notation $[\R ^e]^{\times}$ means the set of invertible elements in $\R ^e$.  Concretely, this means that every component of $\lim _{x\to x_0}f(x)$ is nonzero.}; and
\item $\lim _{x\to x_0}[\alpha f(x)]=\alpha \lim _{x\to x_0}f(x)$ for $\alpha \in \R$.
\end{enumerate}
\begin{rmk}
The product is defined component-wise.
\end{rmk}
\begin{proof}
We leave this as an exercise.
\begin{exr}[breakable=false]{}{}
Prove this yourself, using the \nameref{AlgebraicLimitTheorems} (for nets).
\end{exr}
\end{proof}
\end{prp}
\begin{exr}{Order Limit Theorem (for functions)}{OrderLimitTheoremFunctions}\index{Order Limit Theorem (for functions)}
Let $D\subseteq \R ^d$, let $x_0\in \R ^d$ be a limit point of $D$, and let $f,g\colon D\rightarrow \R ^e$ both have limits at $x_0$.  Then, if $f(x)\leq g(x)$ for all $x\in D$, then $\lim _{x\to x_0}f(x)\leq \lim _{x\to x_0}g(x)$.
\begin{rmk}
$\R ^e$ is equipped with the \nameref{ProductOrder}.
\end{rmk}
\end{exr}
\begin{exr}{Squeeze Theorem (for functions)}{SqueezeTheoremForFunctions}
Let $D\subseteq \R ^d$, let $x_0\in \R ^d$ be a limit point of $D$, and let $f,g,h\colon D\rightarrow \R ^e$.  Show that if (i)~$f(x)=h(x)$ in an $\varepsilon$-ball centered at $x_0$ and (ii)~$\lim _{x\to x_0}f(x)=\lim _{x\to x_0}h(x)$, then $\lim _{x\to x_0}f(x)=\lim _{x\to x_0}g(x)=\lim _{x\to x_0}h(x)$.
\begin{rmk}
As before, the content of this theorem is the the limit $\lim _{x\to x_0}g(x)$ \emph{exists}.  We already knew that (by the Order Limit Theorem), if it existed, it would have to be equal to the common value $\lim _{x\to x_0}f(x)=\lim _{x\to x_0}h(x)$.
\end{rmk}
\end{exr}
Before we finally turn to continuity, we first define limit superiors and limit inferiors of \emph{functions}.
\begin{dfn}{Limit superior and limit inferior (of a function)}{}
Let $D\subseteq \R ^d$, let $x_0\in \R ^d$ be a limit point of $D$, and let $f\colon D\rightarrow \R ^e$.  Then, the \term{limit superior}\index{Limit superior (of a function)} and \term{limit inferior}\index{Limit inferior (of a function)} of $f$ at $x_0$, $\limsup _{x\to x_0}f(x)$ and $\liminf _{x\to x_0}f(x)$ respectively, are defined by
\begin{subequations}
\begin{align*}
\limsup _{x\to x_0}f(x) & \ceqq \lim _{\varepsilon \to 0^+}\sup \left\{ f(x):x\in B_{\varepsilon}(x_0)\right\} \\
\liminf _{x\to x_0}f(x)& \ceqq \lim _{\varepsilon \to 0^+}\inf \left\{ f(x):x\in B_{\varepsilon}(x_0)\right\} .
\end{align*}
\end{subequations}\index[notation]{$\limsup \limits _{x\to x_0}f(x)$}\index[notation]{$\liminf \limits _{x\to x_0}f(x)$}
\begin{rmk}
Note that, the same as before, the \namerefpcref{MonotoneConvergenceTheorem} guarantees that these limits always exist, though they may be $\pm \infty$.
\end{rmk}
\begin{rmk}
Also note that, as suprema and infima are quite special to $\R$ (as opposed to $\R ^d$ for $d\geq 2$), this is quite specific for functions with values in $\R$.\footnote{I suppose for functions with values in $\R ^d$ for $d\geq 2$, you could take the $\limsup$ and $\liminf$ `componentwise', but that I think is a bit awkward.}  On the other hand, we can replace the domain with any topological space---see \cref{LimitSuperiorAndLimitInferiorOfAFunction}.
\end{rmk}
\end{dfn}

Having defined limits of functions, we can present the definition of continuity itself.
\begin{dfn}{Continuous (real) function}{}
Let $D\subseteq \R ^d$, let $x_0\in \R ^d$ be a limit point of $D$, and let $f\colon D\rightarrow \R ^e$.  Then, $f$ is \term{continuous}\index{Continuous (at a point)} at $x_0$ iff $\lim _{x\to x_0}f(x)=f(x_0)$.  $f$ is \term{continuous}\index{Continuous} iff it is continuous at $x_0$ for all $x_0\in D$.
\begin{rmk}
Note how this can be written instead as $\lim _{x\to x_0}f(x)=f(\lim _{x\to x_0}x)$.  Thus, you might say that continuous functions are precisely the functions which `commute' with, or `preserve', limits.
\end{rmk}
\end{dfn}
\begin{exm}{Dirichlet Function}{DirichletFunction}
Define $f\colon \R \rightarrow \R$ by
\begin{equation}
f(x)\coloneqq \begin{cases}1 & \text{if }x\in \Q \\ 0 & \text{if }x\in \Q ^{\comp}.\end{cases}
\end{equation}
This is the \term{Dirichlet Function}\index{Dirichlet Function}.
\begin{exr}[breakable=false]{}{}
Where is the Dirichlet Function continuous?
\end{exr}
\end{exm}
\begin{exm}{Thomae Function}{}
Define $f\colon \R \rightarrow \R$ by
\begin{equation*}
f(x)\coloneqq \begin{cases}\tfrac{1}{n} & \text{if }x=\tfrac{m}{n}\in \Q \text{ with }\gcd (m,n)=1,\ n>0 \\ 0 & \text{if }x\in \Q ^{\comp}.\end{cases}
\end{equation*}
This is the \term{Thomae Function}\index{Thomae Function}.
\begin{exr}[breakable=false]{}{}
Show that the Thomae Function is continuous at $x\in \R$ iff $x\in \Q ^{\comp}$.  Hint:  For a fixed $n\in \Z ^+$, how many rational numbers are there in the interval $[0,1]$ with denominator smaller than $n$?
\end{exr}
\end{exm}
\begin{exr}{}{exr3.4.5}
Let $f\colon \R ^d\rightarrow \R ^e$ be a function and let $x_0\in \R ^d$.  Show that the following are equivalent.
\begin{enumerate}
\item \label{enm3.4.5.i}$f$ is continuous at $x_0$.
\item \label{enm3.4.5.ii}For every net $\lambda \mapsto x_\lambda$ such that (i)~$x_\lambda \neq x_0$ and (ii)~$\lim _\lambda x_\lambda =x_0$ we have $\lim _\lambda f(x_\lambda )=f(x_0)$.
\item \label{enm3.4.5.iii}For every $\varepsilon >0$ there is some $\delta >0$ such that, whenever $0<\abs{x-x_0}<\delta$, it follows that $\abs{f(x)-f(x_0)}<\varepsilon$.\footnote{Note that as $\abs{f(x_0)-f(x_0)}<\varepsilon$ for all $\varepsilon >0$, the ``$<$'' in ``$0<\abs{x-x_0}<\delta$'' is not strictly necessary anymore.}
\item \label{enm3.4.5.iv}For every $\varepsilon >0$ there is some $\delta >0$ such that $f(B_\delta (x_0))\subseteq B_\varepsilon (f(x_0))$.
\item \label{enm3.4.5.v}For every $\varepsilon >0$ there is some $\delta >0$ such that $B_\delta (x_0)\subseteq f^{-1}(B_\varepsilon (f(x_0)))$.
\end{enumerate}
\end{exr}
\begin{exr}{}{exr3.4.12}
Let $D\subseteq \R ^d$, let $x_0\in \R ^d$ be a limit point of $D$, and let $f,g\colon D\rightarrow \R ^e$ be continuous at $x_0\in \R ^d$.
\begin{enumerate}
\item Show that $f+g$ is continuous at $x_0\in \R^d$.
\item Show that $fg$ is continuous at $x_0\in \R ^d$. 
\item Show that $\frac{1}{f}$ is continuous at $x_0\in \R ^d$ if $f(x_0)\in [\R ^e]^{\times}$.
\item Show that $\alpha f$ is continuous at $x_0\in \R ^d$ for $\alpha \in \R$.
\end{enumerate}
\end{exr}
The equivalences of \cref{exr3.4.5} are nice, but they all somehow have the `disadvantage' that they make reference to the points of $\R ^d$.\footnote{Admittedly, at this point, it should probably not be clear as to why this would be a disadvantage at all.}  There is, however, a way to characterize continuity without making reference to points at all.  This is done by the introduction of \emph{open sets}.  Before we get there however, we take an aside to define something that ideally we could have done a long time ago, but first needed continuity to discuss.

\subsubsection{Exponentials}

Despite all that we've done, there is still something you thought you knew how to do with numbers since grade school, but we have yet to define:  exponentials.\footnote{Did you really think you knew what something like $\sqrt{2}^{\sqrt{3}}$ was?  What are you going to do?  Multiply $\sqrt{2}$ by itself $\sqrt{3}$ times?  Good luck with that.}  Now that we know what it means to be continuous, we can finally define exponentials.
\begin{thm}{Exponentials}{Exponentials}
Let $a\in \R ^+$.  Then, there is a unique continuous function, $\R \ni x\mapsto a^x\in \R$\index[notation]{$a^x$}, such that
\begin{enumerate}
\item $a^{x+y}=a^xa^y$; and
\item $a^1=a$.
\end{enumerate}
Functions of this form are called \term{exponential functions}\index{Exponential}.  $a$ is the \term{base}\index{Base (exponential functions)} and $x$ is the \term{exponent}\index{Exponent (exponential functions)}.
\begin{rmk}
We also define:  $0^a\coloneqq 0$ for $a>0$, and $0^0\coloneqq 1$---see \cref{00}.\footnote{The reason for this definition is \emph{nontrivial}, and we will have to wait for \emph{Lebesgue measure} to justify it.  That being said, you can see intuitively why we make this definition simply by graphing the function $\coord{x,y}\mapsto x^y$ on $\R ^+\times \R ^+$.  Despite our reasons, I should probably note that I imagine most simply leave $0^0$ undefined.}  $0^a$ is undefined for $a<0$.
\end{rmk}
\begin{proof}
\Step{Define $a^m$ for $m\in \N$}
For $m\in \N$, $a^m$ is defined by
\begin{equation}
a^m\coloneqq \underbrace{a\cdot \cdots \cdot a}_m.
\end{equation}

\Step{Define $a^{1/n}$ for $n\in \Z ^+$}
For $n\in \Z ^+$, $a^{\frac{1}{n}}$ is defined to be the unique positive real number that has the property that $(a^{\frac{1}{n}})^n=a$---see \cref{prp3.3.66}.

\Step{Define $a^r$ for $r\in \Q$}
For $\frac{p}{q}\in \Q$ with $p\neq 0$, $q\in \Z ^+$, and $\gcd (p,q)=1$, $a^{\frac{p}{q}}$ is defined by
\begin{equation}
a^{\frac{p}{q}}\coloneqq (a^p)^{\tfrac{1}{q}}.
\end{equation}

\Step{Define $a^x$ for $x\in \R$}
For $x\in \R$, let $\lambda \mapsto x_\lambda \in \Q$ be some net converging to $x$.\footnote{Note that some such net exists by \nameref{thm3.2.14}.}  Then, $a^x$ is defined by
\begin{equation}
a^x\coloneqq \lim _\lambda a^{x_\lambda}.
\end{equation}
\begin{exr}[breakable=false]{}{}
Show that this is well-defined in the sense that if $\mu \mapsto b_\mu \in \Q$ also converges to $a$, then $\lim _\lambda x^{a_\lambda}=\lim _\mu x^{b_\mu}$.
\end{exr}

\Step{Check properties}
$a^1=a$ by definition.
\begin{exr}[breakable=false]{}{}
Show that $a^{x+y}=a^xa^y$ for all $x,y\in \R$.
\end{exr}

\Step{Show uniqueness}
\begin{exr}[breakable=false]{}{}
Show that this is the unique continuous function that is $a$ at $1$ and takes sums to products.
\end{exr}
\end{proof}
\end{thm}
\begin{exr}{}{exr2.5.27}
Show that $(a^x)^y=a^{xy}$.
\end{exr}

\subsection{Open and closed sets}

\begin{dfn}{Open set}{OpenSetInR}
Let $U\subseteq \R ^d$.  Then, $U$ is \term{open}\index{Open (in $\R ^d$)} iff for every $x\in U$ there is some $\varepsilon >0$ such that $B_\varepsilon (x)\subseteq U$.
\begin{rmk}
The intuition is that there is `wiggle room' around every point.
\end{rmk}
\end{dfn}
\begin{exr}{}{exr3.4.13}
Explain why $\emptyset$ is open.
\begin{rmk}
When we generalize to topological spaces, we will require that the empty-set is open.
\end{rmk}
\end{exr}
\begin{exr}{}{exr3.4.14}
Show that $\R ^d$ is open.
\begin{rmk}
Likewise, when we generalize to topological spaces, we will also require that the entire set be open.
\end{rmk}
\end{exr}
\begin{exr}{}{exr3.4.7}
Let $x\in \R ^d$ and $\varepsilon >0$.  Show that $B_\varepsilon (x)$ is open.
\end{exr}
The following result is incredibly important.  It is neither particularly deep nor particularly difficult, but it is what is taken as the \emph{definition} of continuity when we generalize to topological spaces,\footnote{Okay, so this is \emph{almost} true, but not quite---see \cref{ContinuousFunction,exr3.1.27}.} even though it might not be particularly intuitive at first.
\begin{thm}{}{thm3.4.16}
Let $f\colon \R ^d \rightarrow \R ^d$.  Then, $f$ is continuous iff the preimage of every open set is open (i.e.~iff $U\subseteq \R ^d$ open implies $f^{-1}(U)\subseteq \R ^d$ is open).
\begin{proof}
$(\Rightarrow )$ Suppose that $f$ is continuous.  Let $U\subseteq \R ^d$ be open and let $x\in f^{-1}(U)$.  Then, $f(x)\in U$, so because $U$ is open, there is some $\varepsilon >0$ such that $B_\varepsilon (f(x))\subseteq U$.  Then, by \cref{exr3.4.5}.\cref{enm3.4.5.iv}, there is some $\delta >0$ such that $f(B_\delta (x))\subseteq B_\varepsilon (f(x))\subseteq U$.  It follows that $B_\delta (x)\subseteq f^{-1}(U)$, and so $f^{-1}(U)$ is open.

\blankline
\noindent
$(\Leftarrow )$ Suppose that the preimage of every open set is open.  Let $x\in \R ^d$ and $\varepsilon >0$.  $B_\varepsilon (f(x))$ is open by \cref{exr3.4.7}, and so $f^{-1}(B_\varepsilon (f(x)))$ is open.  As this set contains $x$, there is some $\delta >0$ such that $B_\delta (x)\subseteq f^{-1}(B_\varepsilon (f(x)))$, and so $f(B_\delta (x))\subseteq B_\varepsilon (f(x))$.  Thus, $f$ is continuous by \cref{exr3.4.5}.\cref{enm3.4.5.iv}.
\end{proof}
\end{thm}

`Dual' (but not \emph{opposite}!)\footnote{Don't make the mistake Hitler did---see \url{https://www.youtube.com/watch?v=SyD4p8_y8Kw}.} to the notion of an open set is that of a \emph{closed} set.
\begin{dfn}{Closed set}{dfn3.4.17}
Let $C\subseteq \R ^d$.  Then, $C$ is \term{closed}\index{Closed (in $\R ^d$)} iff $C^{\comp}$ is open.
\end{dfn}
\begin{exm}{$\R ^d$ and the empty-set}{}
Hopefully you saw in \cref{exr3.4.13,exr3.4.14} that both $\emptyset$ and $\R ^d$ are open.  As $\emptyset ^{\comp}=\R ^d$ and $[\R ^d]^{\comp}=\emptyset$, it follows that both $\emptyset$ and $\R ^d$ are closed \emph{as well}.  In particular, it is possible for sets to be \emph{both open and closed}---sometimes such sets are referred to as \term{clopen}\index{Clopen}.  This is what we meant when we said that openness and closedness are ``dual'' but not ``opposite''.
\end{exm}
\begin{exr}{}{exr2.5.33}
Let $f\colon \R ^d\rightarrow \R ^e$.  Show that $f$ is continuous iff the preimage of every closed set is closed.
\end{exr}
Showing that $C^{\comp}$ is open to show that $C$ is closed can in fact be a very efficient way of doing so.  Nevertheless, it would be nice to have a direct way of checking that $C$ is closed, which is why we introduce the notion of \emph{accumulation point}.
\begin{dfn}{Accumulation point}{dfn3.4.20}
Let $S\subseteq \R ^d$ and let $x_0\in \R ^d$.  Then, $x_0$ is an \term{accumulation point}\index{Accumulation point} of $S$ iff for every $\varepsilon >0$, $B_\varepsilon (x_0)$ intersects $S$ at a point \emph{distinct from} $x_0$.
\begin{rmk}
You might think of the accumulation points of $S$ as being `infinitely close' to $S$ in some sense.
\end{rmk}
\begin{rmk}
The reason we require that it intersect at a point \emph{distinct} from $x_0$ is because some results would just fail to be true without it (for example, \cref{prp3.4.27}).  A similar problem which would arise is that the \namerefpcref{BolzanoWeierstrassTheorem} would be trivial (and hence have no content) without this extra condition.  This condition should be seen as exactly analogous to the condition in the definition of a limit point (\cref{LimitPoint}) that $x_{\lambda}\neq x_0$.  Indeed, we will see momentarily that accumulation points are the same as limit points (\cref{prp3.4.21}), and the proof will make plain that these conditions correspond to one another.
\end{rmk}
\end{dfn}
The condition ``distinct from $x_0$'' is very important as, as mentioned in the remark, some results are just not true without it.  That said, there are cases in which it can be slightly annoying to worry about, which leads us to the following definition.
\begin{dfn}{Adherent point}{AdherentPointR}
Let $S\subseteq \R ^d$ and let $x_0\in \R ^d$.  Then, $x_0$ is an \term{adherent point}\index{Adherent point} of $S$ iff for every $\varepsilon >0$, $B_{\varepsilon}(x_0)$ intersects $S$.
\begin{rmk}
Note of course that this is exactly the same as the definition of accumulation point without the condition ``distinct from $x_0$''.
\end{rmk}
\begin{rmk}
Note that automatically every point of $S$ is an adherent point of $S$, which is one convenience adherent points have over accumulation points.  In contrast, many nonempty sets have no accumulation points at all.\footnote{Example?}
\end{rmk}
\begin{rmk}
While certainly not useless, I think it is unquestionably fair to say, that of the two, the concept of an accumulation point is quite a bit more useful (certainly, anyways, it is the concept that we will make primary use of).
\end{rmk}
\end{dfn}
\begin{exr}{}{exr3.4.22}
Let $S\subseteq \R ^d$ and let $x_0\in \R ^d$.  Show that $x_0$ is an accumulation point of $S$ iff for every $\varepsilon >0$, $B_\varepsilon (x_0)$ intersects $S$ at \emph{infinitely many points}.
\begin{wrn}
Warning:  Many of the results we prove in this section generalize perfectly to the case of a general topological space.  This is not one of them!  For example, obviously this cannot be true in a topological space which only has finitely many points.
\end{wrn}
\begin{wrn}
Warning:  This fails, even in $\R ^d$, if you replace ``accumulation point'' with ``adherent point''.  Counter-example?
\end{wrn}
\end{exr}
\begin{prp}{}{prp3.4.21}
Let $S\subseteq \R ^d$ and let $x_0\in \R ^d$.  Then, $x_0$ is an accumulation point of $S$ iff it is a limit point of $S$.
\begin{rmk}
If you replace ``net'' with ``sequence'' in the definition of a limit point, then this result will be \emph{false} in general!  Sequences are just fine if we restrict ourselves to $\R ^d$, but when we generalize, this result would fail to hold if we constrained ourselves to only work with sequences.
\end{rmk}
\begin{proof}
$(\Rightarrow )$ Suppose that $x_0$ is an accumulation point of $S$.  Let $\varepsilon >0$.  Then, $B_\varepsilon (x_0)\cap S$ contains some element $x_\varepsilon$ distinct from $x_0$.  Note that the positive-real numbers $\coord{\R ^+,\preceq}$ equipped with the \emph{reverse} of the usual ordering $\preceq$ (i.e.,$x\preceq y$ is defined to be true iff $y\leq x$) is a directed set, so that $\varepsilon \mapsto x_\varepsilon$ is a net with $x_\varepsilon \neq x_0$.  We claim that $\lim _\varepsilon x_\varepsilon =x_0$, so that $x_0$ will then be a limit point of $S$.  Let $\varepsilon >0$.  Suppose that $\delta \succeq \varepsilon$ (we are taking our `$\lambda _0$' in the definition of the limit of a net, \cref{dfn3.3.8}, to be $\varepsilon$ itself).  Then, $x_\delta \in B_\delta (x_0)$, and so $\abs{x_\delta -x_0}<\delta$.  As $\delta \succeq \varepsilon$, we have $\delta \leq \varepsilon$, and so $\abs{x_\delta -x_0}<\varepsilon$, which shows that $\lim _\varepsilon x_\varepsilon =x_0$, and so $x_0$ is a limit point of $S$.

\blankline
\noindent
$(\Leftarrow )$ Suppose that $x_0$ is a limit point of $S$.  Then, there is some net $\lambda \mapsto x_\lambda \in S$ with $x_\lambda \neq x_0$ such that $\lim _\lambda x_\lambda =x_0$.  Let $\varepsilon >0$.  Then, there is some $x_{\lambda _0}\neq x_0$ such that $\abs{x_{\lambda _0}-x_0}<\varepsilon$.  In other words, $x_{\lambda _0}\in B_\varepsilon (x_0)\cap S$, so that $x_0$ is an accumulation point of $S$.
\end{proof}
\end{prp}
There is of course an analogous result for adherent points.
\begin{prp}{}{prp2.5.39}
Let $S\subseteq \R ^d$ and let $x_0\in \R ^d$.  Then, $x_0$ is an adherent point of $S$ iff there is a net $\lambda \mapsto x_{\lambda} \in S$ such that $x_0=\lim _{\lambda}x_{\lambda}$.
\begin{rmk}
Note that the condition ``there is a net $\lambda \mapsto x_{\lambda}\in S$ such that $x_0=\lim _{\lambda}x_{\lambda}$'' is \emph{almost} the definition of a limit point, the difference being that there is no requirement that $x_{\lambda}\neq x_0$.  To the best of my knowledge, there is no term for this condition.  I suppose to be completely parallel to the accumulation point-limit point equivalence, it would make sense to have one, but there is no standard term, and as it winds up being equivalent to adherent point anyways, there isn't that much benefit in creating one.
\end{rmk}
\begin{proof}
We leave this as an exercise.
\begin{exr}{}{}
Prove this yourself, using the proof of the previous result as guidance.
\end{exr}
\end{proof}
\end{prp}
Certainly, every accumulation point is an adherent point.  Additionally, there is a name (and characterization) of those accumulation points which are not adherent points, namely, ``\emph{isolated point}''.
\begin{dfn}{Isolated point}{IsolatedPointR}
Let $S\subseteq \R ^d$ and let $x_0\in \R ^d$.  Then, $x_0$ is an \term{isolated point}\index{Isolated point} iff there is some $\varepsilon _0>0$ such that $B_{\varepsilon _0}(x_0)\cap S=\{ x_0\}$.
\begin{rmk}
Note that it follows straight from the definition that $x_0\in S$.
\end{rmk}
\begin{rmk}
The name is a fitting one:  the intuition is that $x_0$ is ``isolated'' from the rest of $S$ by a distance of at least $\varepsilon _0$.
\end{rmk}
\end{dfn}
\begin{prp}{}{prp2.5.46}
Let $S\subseteq \R ^d$ and let $x_0\in \R ^d$.  Then, $x_0$ is an adherent point of $S$ iff either $x_0$ is an accumulation point of $S$ or $x_0$ is an isolated point of $S$.
\begin{rmk}
Note that this or is \emph{exclusive}.
\end{rmk}
\begin{rmk}
In particular, as every element of $S$ is an adherent point, every element of $S$ is either an accumulation point of $S$ or an isolated point of $S$.
\end{rmk}
\begin{proof}
$(\Rightarrow )$ Suppose that $x_0$ is an adherent point of $S$.  If $x_0$ is an accumulation point, we're done, so suppose this is not the case.  This means that there is some $\varepsilon _0>0$ such that it is \emph{not} the case that $B_{\varepsilon _0}(x_0)$ intersects $S$ at a point distinct from $x_0$.  This can fail for two reasons:  either it doesn't intersect $S$ at all or it intersects $S$ only at $x_0$.  However, the former cannot happen as $x_0$ is an adherent point, and so it must be the case that $B_{\varepsilon _0}(x_0)\cap S=\{ x_0\}$.  Thus, $x_0$ is an isolated point of $S$.  This shows that at least one of the statements is true.  It remains to show that at most one of them is true.

So, suppose that $x_0$ is an accumulation point of $S$.  This means that for every $\varepsilon >0$, $B_{\varepsilon}(x_0)\cap S$ contains some point besides $x_0$, and in particular, this intersection is not equal to $\{ x_0\}$.  Thus, $x_0$ is not an isolated point of $S$.

Now suppose that $x_0$ is an isolated point of $S$.  Then, there is some $\varepsilon _0>0$ such that $B_{\varepsilon _0}(x_0)\cap S=\{ x_0\}$, and so in particular, this $\varepsilon _0$-ball does not intersect $S$ at a point distinct from $x_0$.  Thus, $x_0$ is not an accumulation point.

\blankline
\noindent
$(\Leftarrow )$ Suppose that either $x_0$ is an accumulation point of $S$ or $x_0$ is an isolated point of $S$.  If $x_0$ is an accumulation point, then $x_0$ is certainly an adherent point (from the definition).  On the other hand, if $x_0$ is an isolated point, then in fact $x_0\in S$, and so certainly $B_{\varepsilon}(x_0)$ intersects $S$ for all $\varepsilon >0$ (namely at $x_0$), and so is likewise an adherent point.
\end{proof}
\end{prp}

The following result is an equivalent characterization of being a closed set and was our motivation for introducing accumulation points at all.
\begin{thm}{}{prp3.4.23}
Let $C\subseteq \R ^d$.  Then, the following are equivalent.
\begin{enumerate}
\item \label{prp3.4.23.i}$C$ is closed.
\item \label{prp3.4.23.ii}$C$ contains all its accumulation points.
\item \label{prp3.4.23.iii}$C$ contains all its limit points.
\item \label{prp3.4.23.iv}$C$ contains all its adherent points.
\item \label{prp3.4.23.v}$C$ contains all points $x_0\in \R ^d$ for which there is a net $\lambda \mapsto x_{\lambda}\in C$ such that $x_0=\lim _{\lambda}x_{\lambda}$.
\item \label{prp3.4.23.vi}$C$ is equal to its set of adherent points.
\item \label{prp3.4.23.vii}$C$ is equal to the set of all points $x_0\in \R ^d$ for which there is a net $\lambda \mapsto x_{\lambda}\in C$ such that $x_0=\lim _{\lambda}x_{\lambda}$.
\end{enumerate}
\begin{rmk}
In practice, you use this to to show a set is closed as follows:  given an arbitrary convergent net $\lambda \mapsto x_{\lambda}\in C$, you must show that $\lim _{\lambda}x_{\lambda}\in C$.  This is really an application of \cref{prp3.4.23.v}, as you don't have to worry about $x_{\lambda}$ being equal to the limit or not, which is arguably one reason why adherent points can be useful (being equivalent to this characterization by \cref{prp2.5.39}).
\end{rmk}
\begin{rmk}
I am quite confident this is not in fact the correct etymology of the term, but you might think of closed sets as being `closed under the operation of taking limits'.
\end{rmk}
\begin{proof}
$(\cref{prp3.4.23.ii}\Leftrightarrow \cref{prp3.4.23.iii})$ This follows from \cref{prp3.4.21}.

\blankline
\noindent
$(\cref{prp3.4.23.iv}\Leftrightarrow \cref{prp3.4.23.v})$ This follows from \cref{prp2.5.39}.

\blankline
\noindent
$(\cref{prp3.4.23.vi}\Leftrightarrow \cref{prp3.4.23.vii})$ This follows from \cref{prp2.5.39}.

\blankline
\noindent
$(\cref{prp3.4.23.i}\Rightarrow \cref{prp3.4.23.ii})$ Suppose that $C$ is closed.  Let $x\in \R ^d$ be an accumulation point of $C$.  We proceed by contradiction:  suppose that $x\in C^{\comp}$.  Then, because $C^{\comp}$ is open, there is some $\varepsilon _0>0$ such that $B_{\varepsilon _0}(x)\subseteq C^{\comp}$.  But then, $B_{\varepsilon _0}(x)\cap C$ is empty, a contradiction of the fact that $x$ is an accumulation point of $C$.  Thus, we must have had that $x\in C$.

\blankline
\noindent
$(\cref{prp3.4.23.ii}\Rightarrow \cref{prp3.4.23.i})$ Suppose that $C$ contains all its accumulation points.  Let $x\in C^{\comp}$.  Then, $x$ is not an accumulation point of $C$, and so there must be some $\varepsilon _0$ such that $B_{\varepsilon}(x)\cap C$ is empty (it cannot even contain $x$ because $x\notin C$).  In other words, it must be the case that $B_{\varepsilon}(x)\subseteq C^{\comp}$, so that $C^{\comp}$ is open.

\blankline
\noindent
$(\cref{prp3.4.23.iv}\Rightarrow \cref{prp3.4.23.ii})$ This is immediate as every accumulation point is an adherent point.

\blankline
\noindent
$(\cref{prp3.4.23.ii}\Rightarrow \cref{prp3.4.23.iv})$ Suppose that $C$ contains all its accumulation points.  Let $x_0\in \R ^d$ be an adherent point of $C$.  By the previous result, either $x_0$ is an accumulation point of $C$ or an isolated point of $C$.  In the former case, $x_0\in C$ by hypothesis, and in the latter case, $x_0\in C$ because isolated points of a set are always contained in that set.  Either way, $x_0\in C$, and so $C$ contains all its adherent points.

\blankline
\noindent
$(\cref{prp3.4.23.iv}\Rightarrow \cref{prp3.4.23.vi})$ Suppose that $C$ contains all its adherent points.  Then, as every set is contained in its set of adherent points,\footnote{See the remark in the definition \cref{AdherentPointR}.}, it follows that $C$ is equal to its set of adherent points.

\blankline
\noindent
$(\cref{prp3.4.23.vi}\Rightarrow \cref{prp3.4.23.iv})$ Immediate
\end{proof}
\end{thm}
Thus, while $C$ is closed iff it is equal to its set of adherent points, this is in general not true if you replace ``adherent'' with ``accumulation''.  In fact, this condition even has a name.
\begin{dfn}{Perfect set}{PerfectSet}
Let $C\subseteq \R ^d$.  Then, $C$ is \term{perfect}\index{Perfect} iff it is equal to its set of accumulation points.
\begin{rmk}
In particular, it contains its set of accumulation points, and perfect sets are necessarily closed.
\end{rmk}
\end{dfn}
There are several equivalent ways to state this condition.
\begin{prp}{}{prp2.5.48}
Let $C\subseteq \R ^d$.  Then, the following are equivalent.
\begin{enumerate}
\item \label{prp2.5.48.i}$C$ is perfect.
\item \label{prp2.5.48.ii}$C$ is closed and every element of $C$ is an accumulation point of $C$.
\item \label{prp2.5.48.iii}$C$ is closed and has no isolated points.
\end{enumerate}
\begin{proof}
$(\cref{prp2.5.48.i}\Rightarrow \cref{prp2.5.48.ii})$ Suppose that $C$ is perfect.  Then, by definition, $C$ is equal to its set of accumulation points, and so in particular it contains all its accumulation points, and so is closed by \cref{prp3.4.23}.  Likewise, every element of $C$ is an accumulation point of $C$.

\blankline
\noindent
$((\cref{prp2.5.48.ii}\Rightarrow \cref{prp2.5.48.iii})$ Suppose that $C$ is closed and every element of $C$ is an accumulation point of $C$.  By \cref{prp3.4.23}, $C$ contains its accumulation points, and hence is equal to its set of accumulation points.  By \cref{prp3.4.23} again, it follows that its set of accumulation points is equal to its set of adherent points.  By \cref{prp2.5.46}, an adherent point is either an accumulation point or an isolated point, whence it follows that $C$ has no isolated points.

\blankline
\noindent
$((\cref{prp2.5.48.iii}\Rightarrow \cref{prp2.5.48.i})$ Suppose that $C$ is closed and has no isolated points.  By \cref{prp3.4.23}, $C$ is equal to its set of adherent points, which, by \cref{prp2.5.46}, is simply its set of accumulation points.
\end{proof}
\end{prp}
Perfect sets will almost be of no use to use at all.  However, there is at least one interesting fact about perfect sets that we will make use of later on (when discussing generalized Cantor sets---see \cref{GeneralizedCantorSets}).
\begin{prp}{}{prp2.5.49}
Let $C\subseteq \R ^d$.  Then, if $C$ is perfect, then either $C$ is empty or uncountable.
\begin{wrn}
Warning:  This doesn't generalize to topological spaces.\footnote{This will be obvious as soon as you know what a topological space is---certainly subsets of finite topological spaces cannot be uncountable.}
\end{wrn}
\begin{proof}
We leave this as an exercise.
\begin{exr}{}{}
Prove this yourself.
\begin{rmk}
Hint:  See \cite[Theorem 3.4.3]{Abbott}.
\end{rmk}
\end{exr}
\end{proof}
\end{prp}

\begin{exr}{}{exr3.4.27}
Let $S\subseteq \R$ be closed and bounded above.  Show that $\sup (S)\in S$.
\begin{rmk}
Similarly, of course, if $S$ is closed and bounded below, then $\inf (S)\in S$.
\end{rmk}
\end{exr}
\begin{prp}{}{prp3.4.27}
Let $m\mapsto x_m$ be a sequence that is not eventually constant and let $x\in \R ^d$.  Then, $x$ is an accumulation point of $\{ x_m :m\in \Z ^+\}$ iff there is a subsequence of $m\mapsto x_m$ which converges to $x$.
\begin{rmk}
Note that this result would be \emph{false} if we did not require that $B_\varepsilon (x_0)$ intersect the set at a point \emph{distinct} from $x_0$ in the definition of an accumulation point (\cref{dfn3.4.20}).  For example, if we did not require this, $1\in \R$ would be an accumulation point of $\coord{1,\frac{1}{2},\frac{1}{3},\frac{1}{4},\ldots}$, but of course no subsequence converges to $1$.
\end{rmk}
\begin{wrn}
Warning:  This is \emph{not} true if you replace ``sequence'' with ``net''---see the following counter-example.
\end{wrn}
\begin{wrn}
Warning:  This is \emph{not} true in general topological spaces---see \cref{exm4.2.15}.
\end{wrn}
\begin{proof}
$(\Rightarrow )$ Suppose that $x$ is an accumulation point of $\{ x_m :m\in \Z ^+\}$.  We construct a subsequence $n\mapsto x_{m_n}$ inductively that has the property that $x_{m_n}\in B_{\frac{1}{n}}(x)$ with $x_{m_n}\neq x$.  If we can do so, then $n\mapsto x_{m_n}$ will converge to $x$ by the Archimedean Property (that is, because numbers of the form $\frac{1}{n}$ can be chosen to be arbitrarily small).  Because $x$ is an accumulation point, we have that $B_1(x)\cap \{ x_m:m\in \Z ^+\}$ contains some point distinct from $x$, and so we can take $x_{m_0}$ to be any such element.  Suppose now that we have constructed $x_{m_0},\ldots ,x_{m_n}$ and we wish to construct $x_{m_{n+1}}$.  By \cref{exr3.4.22}, not only is $B_{\frac{1}{n+1}}(x)\cap \{ x_m:m\in \Z \}$ nonempty, but it is infinite.  Therefore, $B_{\frac{1}{n+1}}\cap \{ x_m:m>m_n\}$ is nonempty,\footnote{Because this set is obtained from the \emph{infinite} set $B_{\frac{1}{n+1}}(x)\cap \{ x_m:m\in \Z \}$ by removing \emph{finitely} many points.} and so we can choose $x_{m_{n+1}}$ to be any element in this set distinct from $x$.
\begin{rmk}
Note that this direction of the proof fails for nets in general.  We are implicitly using the fact that infinite subsets of $\N$ are cofinal in $\N$ (and hence give us a (strict) subsequence), and this is not true for general directed sets.
\end{rmk}

\blankline
\noindent
$(\Leftarrow )$ Suppose that there is a subsequence of $m\mapsto x_m$ which converges to $x$.  Denote this subsequence by $n\mapsto x_{m_n}$.  Let $\varepsilon >0$.  Then, there is some $n_0$ such that, whenever $n\geq n_0$, it follows that $x_{m_n}\in B_\varepsilon (x)$.  In particular, $B_\varepsilon (x)\cap \{ x_m:m\in \Z ^+\}$ contains an element distinct from $x$ (because the sequence $m\mapsto x_m$ is not eventually constant), and so $x$ is an accumulation point of $\{ x_m:m\in \Z ^+\}$.
\end{proof}
\end{prp}
\begin{exm}{An accumulation point of a net to which no subnet converges}{exm3.4.29}\footnote{This example was inspired by a similar example showed to me by a student.}
Define $\R ^+\ni \lambda \mapsto x_\lambda \coloneqq \frac{1}{\lambda}$.  Then,
\begin{equation}
\{ x_\lambda :\lambda \in \R ^+\} =(0,\infty ).
\end{equation}
Thus, for example, $1\in (0,\infty )$ is an accumulation point of the net.  However, as $\lim _\lambda x_\lambda =0$, it follows that no subnet can converge to $1$.\footnote{Of course, there is nothing special about $1$---any element of $(0,\infty )$ would work just as well.}
\end{exm}

There is a `dual' (well, sort of) notion of accumulation point, though it is perhaps not quite as useful.
\begin{dfn}{Interior point}{}
Let $S\subseteq \R ^d$ and let $x_0\in \R ^d$.  Then, $x_0$ is an \term{interior point}\index{Interior point} of $S$ iff there is some $\varepsilon _0$ such that $B_{\varepsilon _0}(x_0)\subseteq S$.
\end{dfn}
The result dual to \cref{prp3.4.23} is in the following easy exercise.
\begin{exr}{}{exr3.4.26}
Let $U\subseteq \R ^d$.  Show that $U$ is open iff every point in $U$ is an interior point.
\end{exr}

The next couple of results are incredibly important for the same reason that \cref{thm3.4.16} (the characterization of continuity in terms of open sets) was:  they are neither deep nor difficult (in fact, they're quite trivial), but they will be defining requirements of open sets in the more general setting of topological spaces.
\begin{thm}{}{thm3.4.34}
Let $\collection{U}$ be a collection of open subsets of $\R ^d$.  Then,
\begin{equation}
\bigcup _{U\in \mcal{U}}U
\end{equation}
is open.
\begin{rmk}
In other words, an \emph{arbitrary} union of open sets is open.
\end{rmk}
\begin{proof}
Let $x\in \bigcup _{U\in \mcal{U}}U$.  Then, $x\in U$ for some $U\in \mcal{U}$.  Because $U$ is open, there is some $\varepsilon _0>0$ such that $B_{\varepsilon _0}(x)\subseteq U\subseteq \bigcup _{U\in \mcal{U}}U$, and so $\bigcup _{U\in \mcal{U}}U$ is open.
\end{proof}
\end{thm}
\begin{thm}{}{thm3.4.36}
Let $U_1,\ldots ,U_m\subseteq \R ^d$ be open.  Then,
\begin{equation}
\bigcap _{k=1}^mU_k
\end{equation}
is open.
\begin{rmk}
In other words, the \emph{finite} intersection of open sets is open.
\end{rmk}
\begin{proof}
Let $x\in \bigcap _{k=1}^mU_k$, so that $x\in U_k$ for $1\leq k\leq m$.  Let $\varepsilon _k>0$ be such that $B_{\varepsilon _k}(x)\subseteq U_k$.  Define $\varepsilon _0\coloneqq \min \{ \varepsilon _1,\ldots ,\varepsilon _m\} >0$.  Then, $B_{\varepsilon _0}(x)\subseteq B_{\varepsilon _k}(x)\subseteq U_k$ for all $k$, and so $B_{\varepsilon _0}(x)\subseteq \bigcap _{k=1}^mU_k$, so that $\bigcap _{k=1}^mU_k$ is open.
\end{proof}
\end{thm}
\begin{exr}{}{}
Find an infinite collection of open sets whose intersection is \emph{not} open.
\end{exr}
\begin{exr}{}{exr3.4.38x}
Let $\collection{C}$ be a collection of closed subsets of $\R ^d$.  Show that
\begin{equation}
\bigcap _{C\in \collection{C}}C
\end{equation}
is closed.
\end{exr}
\begin{exr}{}{exr3.4.40}
Let $C_1,\ldots ,C_m\subseteq \R ^d$ be closed.  Show that
\begin{equation}
\bigcup _{k=1}^mC_k
\end{equation}
is closed.
\end{exr}

The closure of a set is the `smallest' closed set which contains it.  Likewise, the interior of a set is the `largest' open set which it contains.  The sense in which these are respectively ``smallest'' and ``largest'' is made precise by the following results.
\begin{prp}{Closure}{prp3.4.34}
Let $S\subseteq \R ^d$.  Then, there exists a unique set $\Cls (S)\subseteq \R ^d$\index[notation]{$\Cls (S)$}, the \term{closure}\index{Closure} of $S$, that satisfies
\begin{enumerate}
\item \label{enm3.4.38.i}$\Cls (S)$ is closed;
\item \label{enm3.4.38.ii}$S\subseteq \Cls (S)$; and
\item \label{enm3.4.38.iii}if $C$ is any other closed set which contains $S$, then $\Cls (S)\subseteq C$.
\end{enumerate}
Furthermore, explicitly, we have
\begin{equation}
\Cls (S)=\bigcap _{\substack{C\subseteq \R ^d\text{ closed} \\ S\subseteq C}}C.
\end{equation}
\begin{rmk}
Compare this with the definition of the integers, rationals, and reals (\cref{Integers,RationalNumbers,RealNumbers}).
\end{rmk}
\begin{rmk}
See \cref{prp2.5.59} for another description of the closure, as the union of the set with its accumulation points.
\end{rmk}
\begin{rmk}
Sometimes people denote the closure by $\bar{S}$.  We prefer the notation $\Cls (S)$ because (i)~it is less ambiguous (the over-bar is used to denote many things in mathematics) and (ii)~the notation $\Cls (S)$ is just slightly more descriptive.
\end{rmk}
\begin{proof}
Define
\begin{equation}\label{3.4.39}
\Cls (S)\coloneqq \bigcap _{\substack{C\subseteq \R \text{ closed } \\ S\subseteq C}}C.
\end{equation}
$\Cls (S)$ is closed because the intersection of an arbitrary collection of closed sets is closed.  $S\subseteq \Cls (S)$ because, by definition, $S$ is contained in every subset in the intersection \eqref{3.4.39}.  Let $C$ be some other closed set which contains $S$.  Then, $C$ itself appears in the intersection of \eqref{3.4.39}, and so $\Cls (S)\subseteq C$.

If $C$ is some other subset of $\R ^d$ which satisfies \cref{enm3.4.38.i}--\cref{enm3.4.38.iii}, then, by \cref{enm3.4.38.iii} applied to $\Cls (S)$, we have that $\Cls (S)\subseteq C$.  On the other hand, by \cref{enm3.4.38.iii} applied to $C$, we have that $C\subseteq \Cls (S)$.  Thus, $\Cls (S)=C$.
\end{proof}
\end{prp}
We have a dual result for the interior.
\begin{prp}{Interior}{}
Let $S\subseteq \R ^d$.  Then, there exists a unique set $\Int (S)\subseteq \R ^d$\index[notation]{$\Int (S)$}, the \term{interior}\index{Interior} of $S$, that satisfies
\begin{enumerate}
\item $\Int (S)$ is open;
\item $\Int (S)\subseteq S$; and
\item if $U$ is any other open set which is contained in $S$, then $U\subseteq \Int (S)$.
\end{enumerate}
Furthermore, explicitly, we have
\begin{equation}
\Int (S)=\bigcup _{\substack{U\subseteq \R \text{ open} \\ U\subseteq S}}U.
\end{equation}
\begin{rmk}
See \cref{prp2.5.60} for another description of the interior, as the set of (surprise, surprise) interior points.
\end{rmk}
\begin{rmk}
Sometimes people denote the interior by $S\degree$.  We prefer the notation $\Int (S)$ for essentially the same reasons as we prefer the notation $\Cls (S)$.
\end{rmk}
\begin{proof}
We leave this as an exercise.
\begin{exr}[breakable=false]{}{}
Complete this proof by yourself, using the dual proof for the closure as guidance.
\end{exr}
\end{proof}
\end{prp}
One might ask ``What about the smallest \emph{open} set which contains $S$?'' (and ``dually'').  In general, however, there is no smallest open set that contains $S$.
\begin{exm}{A set for which there is not a smallest open set which contains $S$}{}
Define $S\coloneqq \{ 0\} \subseteq \R$.  Suppose there is a smallest open set $U$ which contains $0$.  As $U$ is open, there is some $\varepsilon >0$ such that $B_{\varepsilon}(0)\subseteq U$.  On the off chance we have $B_{\varepsilon}(0)$, use $\frac{\varepsilon}{2}$ instead:  $B_{\frac{\varepsilon}{2}}(0)$ will be a \emph{proper} subset of $U$.  Thus, there can be no smallest open set which contains $\{ 0\}$.
\end{exm}
\begin{exr}{}{}
Find a set $S\subseteq \R ^d$ for which there is no largest set closed set contained in $S$.
\end{exr}

There is a relatively concrete description of the closure.
\begin{prp}{}{prp2.5.59}
Let $S\subseteq \R ^d$.  Then, $\Cls (S)$ is the union of $S$ and its set of accumulation points.
\begin{proof}
Let $C$ be the union of $S$ and its accumulation points.  We simply have to verify that it satisfies the axioms of the definition of the closure in \cref{prp3.4.34}.

To show that it is closed, we must show that it contains all its accumulation points.  So, let $x$ be an accumulation point of $C$.  If $x\in S$, we are done, so we may as well assume that $x\notin S$.  We show that $x$ is an accumulation point of $S$, so that $x\in C$.  Let $\varepsilon >0$.  We wish to show that $B_\varepsilon (x)$ intersects $S$ (as $x\notin S$, we know the point of intersection must be distinct from $x$).  As $x$ is an accumulation point of $C$, we know, however, that $B_\varepsilon (x)$ contains either a point of $S$ or an accumulation point of $S$ distinct from $x$.  In the former case, we are done, so let $x_\varepsilon \in B_\varepsilon (x)$ be an accumulation point of $S$ distinct from $x$.  Because $x_\varepsilon$ is an accumulation point of $S$, it must be the case that $B_\varepsilon (x_\varepsilon )$ intersects $S$ at a point $x_\varepsilon \in S$ distinct from $x_\varepsilon$.  But then, by the triangle inequality, $x_\varepsilon \in B_{2\varepsilon}(x)$.  Thus, $x$ is an accumulation point of $S$ ($x_\varepsilon$ and $x$ must be distinct because one is in $S$ and the other is not), and hence an element of $C$.  Thus, $C$ is closed.

Because any closed set must contain all its accumulation points (\cref{prp3.4.23}), it follows that $C$ must be contained in any closed set which contains $S$, and so $C=\Cls (S)$.
\end{proof}
\end{prp}
There is a dual concrete description of the interior.
\begin{prp}{}{prp2.5.60}
Let $S\subseteq \R ^d$.  Then, $\Int (S)$ is the set of interior points of $S$.
\begin{proof}
Because of the dual result to \cref{prp3.4.23}, namely \cref{exr3.4.26} (a set is open iff all of its points are interior points), just as in the dual proof above, it suffices to show that the set of interior points of $S$ is open.

So, let $x\in \R ^d$ be an interior point of $S$.  Then, there is some $\varepsilon _0>0$ such that $B_{\varepsilon _0}(x)\subseteq S$.  To show that the set of interior points is open, we need to show that in fact every point of $B_{\varepsilon _0}(S)$ is an interior point of $S$.  This however follows from the fact that balls are open (\cref{exr3.4.7}).
\end{proof}
\end{prp}
\begin{exr}{}{exr2.5.61}
Let $S\subseteq \R ^d$.
\begin{enumerate}
\item Show that $S$ is closed iff $S=\Cls (S)$.
\item Show that $S$ is open iff $S=\Int (S)$.
\end{enumerate}
\end{exr}

In the next chapter, we will define a topological space as a set equipped with a choice of open sets.  The choice of open sets will be called a \emph{topology}.  Of course, it turns out that there are many equivalent ways to specify a topology, and one way to do this is by defining what the closure (or interior) of each set is.  The following result is important because, when we go to generalize, it will play the role of axioms which a closure (or interior) `operator' must satisfy.
\begin{thm}{Kuratowski Closure Axioms}{KuratowskiClosureAxioms}\index{Kuratowski Closure Axioms}
Let $S,T\subseteq \R ^d$.  Then,
\begin{enumerate}
\item \label{enm3.4.39.i}$\Cls (\emptyset) =\emptyset$;
\item \label{enm3.4.39.ii}$S\subseteq \Cls (S)$;
\item \label{enm3.4.39.iii}$\Cls (S)=\Cls \left( \Cls (S)\right)$; and
\item \label{enm3.4.39.iv}$\Cls (S\cup T)=\Cls (S)\cup \Cls (T)$.
\end{enumerate}
\begin{rmk}
Careful:  The closure of a \emph{finite} union is the union of the closures, but this does not hold in general---see \cref{exr3.4.53} below.
\end{rmk}
\begin{proof}
The empty-set is closed, contains the empty-set, and is contained in every closed set which contains the empty-set, and hence, by definition, $\Cls (\emptyset )=\emptyset$.

\cref{enm3.4.39.ii} follows from the definition.

\cref{enm3.4.39.iii} follows from the fact that the closure of a set is closed (by definition) and the fact that the closure of a closed set is itself (\cref{exr2.5.61}).

We now prove \cref{enm3.4.39.iv}.  We show that $\Cls (S)\cup \Cls (T)$ satisfies the axioms of the closure of $S\cup T$.  $\Cls (S)\cup \Cls (T)$ is a closed set which contains $S\cup T$, and so it therefore contains $\Cls (S\cup T)$.  Let $C$ be some other closed set which contains $S\cup T$.  $C$ therefore contains $S$, and so it must contain $\Cls (S)$.  Likewise, it must contain $\Cls (T)$, and so $C$ must contain $\Cls (S)\cup \Cls (T)$.  Therefore, by definition, $\Cls (S)\cup \Cls (T)=\Cls (S\cup T)$.
\end{proof}
\end{thm}
Of course, we have the dual result for interior.
\begin{thm}{Kuratowski Interior Axioms}{KurawoskiInteriorAxioms}\index{Kuratowski Interior Axioms}
Let $S,T\subseteq \R ^d$.  Then,
\begin{enumerate}
\item $\Int (\R )=\R ^d$;
\item $\Int (S)\subseteq S$;
\item $\Int (S)=\Int \left( \Int (S)\right)$; and
\item $\Int (S\cap T)=\Int (S)\cap \Int (T)$.
\end{enumerate}
\begin{proof}
We leave this as an exercise.
\begin{exr}{}{}
Complete this proof yourself, using the dual proof for the closure as guidance.
\end{exr}
\end{proof}
\end{thm}
\begin{exr}{}{exr3.4.53}
Let $\collection{S}\subseteq 2^{\R ^d}$ be a collection of subsets of $\R ^d$.  Show that the following are true.
\begin{enumerate}
\item \label{enm3.4.53.i}$\bigcap _{S\in \collection{S}}\Cls (S)\supseteq \Cls \left( \bigcap _{S\in \collection{S}}S\right)$.
\item \label{enm3.4.53.ii}$\bigcup _{S\in \collection{S}}\Int (S)\subseteq \Int \left( \bigcup _{S\in \collection{S}}S\right) $.
\item \label{enm3.4.53.iii}$\bigcup _{S\in \collection{S}}\Cls (S)\subseteq \Cls \left( \bigcup _{S\in \collection{S}}S\right)$.
\item \label{enm3.4.53.iv}$\bigcap _{S\in \collection{S}}\Int (S)\supseteq \Int \left( \bigcup _{S\in \collection{S}}S\right)$.
\end{enumerate}
Find examples to show that we need not have equality in general.  In fact, show that \cref{enm3.4.53.i,enm3.4.53.ii} can fail even in the case where $\collection{S}$ is \emph{finite}.\footnote{Of course, in this case, you can find counter-examples for $\abs{\collection{S}}=2$.}
\end{exr}

\subsection{Quasicompactness}

You will find with experience that closed intervals on the real line are particularly nice to work with.  For example, you'll probably recall from calculus that, on a closed interval, every continuous function \emph{attains} a maximum and minimum (the Extreme Value Theorem (\cref{ClassicalExtremeValueTheorem})).  In particular, continuous functions are bounded on closed intervals.  This is not just true of all closed intervals, however, but is in fact true about any closed bounded subset of $\R ^d$.

The objective then is to characterize closed bounded sets in such a way that will generalize to arbitrary topological spaces.  The  characterization we are looking for is what is called \emph{quasicompactness}.
\begin{dfn}{Cover}{}
Let $S\subseteq \R ^d$ and let $\cover{U}\subseteq 2^{\R ^d}$.  Then, $\cover{U}$ is a \emph{cover}\index{Cover} of $S$ iff $S\subseteq \bigcup _{U\in \cover{U}}U$.  $\cover{U}$ is an \emph{open cover}\index{Open cover} iff every $U\in \cover{U}$ is open.  A \emph{subcover} of $\cover{U}$ is a subset $\cover{V}\subseteq \cover{U}$ that is still a cover of $S$.
\end{dfn}
\begin{dfn}{Quasicompact}{}
Let $S\subseteq \R ^d$.  Then, $S$ is \term{quasicompact}\index{Quasicompact} iff every open cover of $S$ has a finite subcover.
\begin{rmk}
For most authors, and in fact, for probably all authors of introductory analysis books, my definition of quasicompact for them will be called just \emph{compact}.  Instead, I reserve the term \emph{compact} for spaces which are both quasicompact \emph{and} $T_2$---see \cref{Compact}.\footnote{You are not expected to know what $T_2$ means---see the next chapter for details (specifically, \cref{T2}), though keep in mind, the details don't matter at the moment.}  As $\R ^d$ is $T_2$, the notions of compact and quasicompact are the same for the real numbers, but in general they will differ.  To the best of my knowledge,\footnote{Actually, since first writing this remark, I found that Bourbaki \cite[pg.~83]{Bourbaki} uses exactly this terminology, so perhaps the root of this terminology goes back further.} the term quasicompact originated in algebraic geometry because it was felt that things that should \emph{not} intuitively be thought of as compact nevertheless satisfied the defining condition above.  Thus, it was decided that such spaces should be referred to as quasicompact and that instead the term compact should be reserved for spaces which are both quasicompact and $T_2$.  I prefer this terminology for two reasons:  (i)~the terminology is more precise, that is, I have two terms to work with instead of just one; and (ii)~I strongly feel that it is a bad idea for the terminology we use to be dependent on the mathematics we happen to be doing at the moment---``compact'' should not mean one thing today and something else tomorrow just because I decided to work on something different.  Terminology should be as consistent as possible across all of mathematics.
\end{rmk}
\end{dfn}

And now of course we had better check that this condition properly characterizes ``closed and bounded'' in the real numbers, as desired.
\begin{thm}{Heine-Borel Theorem}{HeineBorelTheorem}\index{Heine-Borel Theorem}
Let $S\subseteq \R ^d$.  Then, $S$ is closed and bounded iff it is quasicompact.
\begin{rmk}
This fails in general topological spaces (for one thing, ``boundedness'' does not make sense in arbitrary topological spaces).  In fact, it will not even hold topological spaces in which the notion of boundedness \emph{does} make sense.  That said, there is something closely resembling a generalization for uniform spaces---see \cref{thm4.4.8}.
\end{rmk}
\begin{wrn}
Warning:  A common mistake I have seen is for students to assert that, for example, $[0,1]\subseteq \R$ is quasicompact \emph{for other topologies besides the usual one}.  In the next chapter, we will start equipping $\R$ with other new topologies, and this result \emph{fails} for other topologies.  For example, $[0,1]$ is \emph{not} quasicompact in the cocountable topology on $\R$---see \cref{exm3.6.45}.
\end{wrn}
\begin{proof}\footnote{Proof adapted from \cite[pg.~87--89]{Abbott}.}
$(\Rightarrow )$
\Step{Assume hypotheses}
Suppose that $S$ is closed and bounded.  Let $\cover{U}$ be an open cover of $S$.  We proceed by contradiction:  suppose that $\cover{U}$ has no finite subcover.

\Step{Construct a sequence of nonincreasing closed rectangles whose side lengths go to $0$ and whose intersection with $S$ has no finite subcover}
We do this step for $d=1$ as it makes the proof clearer.  We will mention in footnotes where things must be modified for $d\geq 2$.

Let $M>0$ be such that $S\subseteq [-M,M]$.\footnote{In general, we want $S\subseteq [-M,M]\times \cdots \times [-M,M]$.}  Then, at least one of $[-M,0]\cap S$ and $[0,M]\cap S$ must not have a finite subcover of $\mcal{U}$, because if they both did, then the cover of $[-M,0]\cap S$ together with the cover of $[0,M]\cap S$ would comprise a finite subcover of $S$.  Without loss of generality, assume that $[0,M]\cap S$ has no finite subcover.\footnote{In higher dimensions, $[-M,M]\times \cdots \times [-M,M]$ breaks up into not $2$ cases, but $2^d$, all with side length $M$.  The same logic gives us that $S$ intersected with at least one of these `quadrants' has no finite subcover.}  Then, by exactly the same logic, either $\left[ 0,\frac{M}{2}\right] \cap S$ or $\left[ \frac{M}{2},M\right] \cap S$ has no finite subcover.  Proceeding inductively, we obtain a nonincreasing sequence of closed rectangles $R_0\supseteq R_1\supseteq R_2\supseteq \cdots$ such that (i)~$I_k\cap S$ has no finite subcover and (ii)~the lengths of the intervals converges to $0$.

\Step{Construct an element in $S\cap \bigcap _{k\in \N}R_k$}
Note that, as $R_k\cap S$ has no finite subcover, in particular, it cannot be empty (otherwise any subset of $\mcal{U}$ would cover it).  So, let $x_k\in R_k\cap S$.  Because the side lengths of the rectangles go to $0$, $m\mapsto x_m$ is a Cauchy sequence, and hence converges to some $x_\infty \in \R ^d$.  As $S$ is closed, we have in addition that $x_\infty \in S$.  We wish to show that in addition $x_\infty \in I_k$ for all $k$.  Write $R_k=[a_{k,1},b_{k,1}]\times \cdots \times [a_{k,d},b_{k,d}]$ for $a_{k,l}\leq b_{k,l}$.  We wish to show that $a_{k,l}\leq x_\infty \leq b_{k,l}$.  We show just $[x_\infty] _l\leq b_{k,l}$ (the other inequality is similar).  We proceed by contradiction:  suppose that there is some $b_{k_0,l_0}$ such that $[x_\infty ]_{l_0}>b_{k_0,l_0}$.  Then, there is some $m_0$ such that, whenever $m\geq m_0$, it follows that $[x_m]_{l_0}>b_{k_0,l_0}$.  However, for $m$ at least as large as $k_0$, we need $x_m\in R_m\subseteq I_{k_0}$, so that $x_{m,l_0}\leq b_{k_0,l_0}$:  a contradiction.  Therefore, $x_\infty \leq b_k$ for all $k$, and so $x_\infty \in R_k$ for all $k$.

\Step{Deduce the contradiction}
As $x_\infty \in S$, there is some $U\in \cover{U}$ such that $x_\infty \in U$.  Then, because the side lengths of the rectangles converge to $0$ and $U$ is open, there must be some $I_{m_0}$ such that $x_\infty \in I_{m_0}\subseteq U$.  But then, $\{ U\}$ is a finite open cover of $I_{m_0}\cap S$:  a contradiction.  Therefore, $S$ is quasicompact.

\blankline
\noindent
$(\Leftarrow )$
\Step{Assume hypotheses}
Suppose that $S$ is quasicompact.

\Step{Show that $S$ is bounded}
The cover
\begin{equation}
\left\{ B_M(0):M>0\right\}
\end{equation}
covers all of $\R ^d$, and so certainly covers $S$.  Therefore, there is a finite subcover
\begin{equation}
\left\{ B_{M_1}(0),\ldots ,B_{M_m}(0):M_1,\ldots ,M_m>0\right\} .
\end{equation}Define $M\coloneqq \max \{ M_1,\ldots ,M_m\}$.  Then, $B_M(0)$ contains each $B_{M_k}(0)$, and so contains $S$.  Therefore, $S$ is bounded.

\Step{Show that $S$ is closed}
Let $\lambda \mapsto x_\lambda \in S$ be a net converging to $x_\infty \in \R ^d$.  We must show that $x_\infty \in S$---see \cref{prp3.4.23}.  We proceed by contradiction:  suppose that $x_\infty \notin S$.  Then, for each $s\in S$, because $s\neq x_\infty$, there is some $\varepsilon _s>0$ such that $x_\infty \notin B_{\varepsilon _s}(s)$.\footnote{For what it's worth, this step does not work in general.}  The collection $\left\{ B_{\varepsilon _s}(s):s\in S\right\}$ is certainly an open cover of $S$, and so there is some finite subcover $\{ B_{\varepsilon _{s_1}}(s_1),\ldots ,B_{\varepsilon _{s_m}}(s_m)\}$.  Define $\varepsilon _0\coloneqq \min _{1\leq k\leq m}\left\{ \abs{s_k-x_\infty}-\varepsilon _k\right\}$.  As $\lim _{\lambda}x_{\lambda}=x_{\infty}$, there is some $x_{\lambda _0}\in B_{\varepsilon _0}(x_\infty)$.  However, by the Reverse Triangle Inequality (\cref{exr3.1.4}.\cref{enm3.3.v}),
\begin{equation}
\begin{split}
\abs{x_{\lambda _0}-s_k} & =\left| (x_{\lambda _0}-x_\infty )+(x_\infty -s_k)\right| \\
& \geq \left| \left| x_{\lambda _0}-x_\infty \right| -\left| x_\infty -s_k\right| \right| \\
& \geq \left| x_\infty -s_k\right| -\left| x_{\lambda _0}-x_\infty \right| \\
& >\abs{x_{\infty}-s_k}-\varepsilon _0 \\
& \geq \abs{x_{\infty}-s_k}-\left( \abs{s_k-x_{\infty}}-\varepsilon _k\right) \\
& =\varepsilon _k,
\end{split}
\end{equation}
and so, $x_{\lambda _0}\notin B_{\varepsilon _{s_k}}(s_k)$, a contradiction of the fact that $\{ B_{\varepsilon _{s_1}}(s_1),\ldots ,B_{\varepsilon _{s_m}}(s_m)\}$ is a cover of $S$.  Therefore, $\lim _\lambda x_\lambda =x_\infty \in S$, and we are done.
\end{proof}
\end{thm}

\subsubsection{Equivalent formulations of quasicompactness}

If for some reason you find the definition of quasicompactness in terms of open covers off-putting, there are a couple of other equivalent formulations of the concept that we present in this section.  In contrast to the \nameref{HeineBorelTheorem}, these characterizations of quasicompactness do generalize to arbitrary topological spaces.

The first characterization we come to is essentially the characterization you get by `taking complements' in the definition of quasicompactness, and rewording things in terms of closed sets.  To phrase this characterization, it will be convenient to have a term that makes the statement less verbose.
\begin{dfn}{Finite-intersection property}{FiniteIntersectionPropertyInR}
Let $X$ be a set, let $S\subseteq X$, let $\collection{C}\subseteq 2^X$ be a collection of subsets of $X$.  Then, $\collection{C}$ has the \term{finite-intersection property}\index{Finite-intersection property} with $S$ iff every finite subset $\{ C_1,\ldots ,C_m\} \subseteq \collection{C}$ intersects $S$:  $(C_1\cap \cdots \cap C_m)\cap S\neq \emptyset$.  For $S=X$, we simply say that $\collection{C}$ has the \term{finite-intersection property}.\footnote{That is, in this case we omit the phrase ``with $X$''.}
\end{dfn}
\begin{prp}{}{prp3.4.58}
Let $K\subseteq \R ^d$ and let $\collection{C}$ be a collection of closed subsets of $\R ^d$.  Then, $K$ is quasicompact iff whenever $\collection{C}$ has the finite-intersection property with $K$, the entire intersection $\bigcap _{C\in \collection{C}}C$ also intersects $K$.
\begin{proof}
$(\Rightarrow )$ Suppose that $K$ is quasicompact.  Let $\collection{C}$ be a collection of closed subsets of $\R ^d$ that has the property that the intersection of any finite number of elements of $\collection{C}$ intersects $K$.  We proceed by contradiction:  suppose that $\bigcap _{C\in \collection{C}}C$ does not intersect $K$.  Then, $\left( \bigcap _{C\in \collection{C}}C\right) ^{\comp}=\bigcup _{C\in \collection{C}}C^{\comp}$ contains $K$,\footnote{Recall De Morgan's Laws (\cref{DeMorgansLaws}).} and therefore the collection $\cover{U}:=\left\{ C^{\comp}:C\in \collection{C}\right\}$ constitutes an open cover of $K$.  Because $K$ is quasicompact, it follows that there is some finite subcover $C_1^{\comp}\cup \cdots \cup C_m^{\comp}\supseteq K$.  But then $C_1\cap \cdots \cap C_m$ does not intersect $K$:  a contradiction.  Therefore, $\bigcap _{C\in \collection{C}}C$ intersects $K$.

\blankline
\noindent
$(\Leftarrow )$ Suppose that $\collection{C}$ has the finite-intersection property with $K$.
\begin{exr}[breakable=false]{}{}
Prove the converse.
\end{exr}
\end{proof}
\end{prp}
\begin{thm}{}{prp3.4.56}
Let $K\subseteq \R ^d$.  Then, $K$ is quasicompact iff every net $\lambda \mapsto x_\lambda \in K$ has a subnet that converges to a limit in $K$.
\begin{rmk}
This is yet another result that will not hold in general if you replace the word ``net'' with ``sequence'' (though it will hold in $\R ^d$).
\end{rmk}
\begin{proof}
$(\Rightarrow )$ Suppose that $K$ is quasicompact.  Let $\Lambda \ni \lambda \mapsto x_\lambda \in K$ be a net.  Define
\begin{equation}
C_\lambda \ceqq \Cls \left( \left\{ x_\mu :\mu \geq \lambda \right\} \right) 
\end{equation}
and $\collection{C}\ceqq \left\{ C_\lambda :\lambda \in \Lambda \right\}$.
\begin{exr}[breakable=false]{}{}
Show that $\collection{C}$ has the finite-intersection property with $K$.
\end{exr}
By the previous characterization of quasicompactness, it follows that $\bigcap _{\lambda \in \Lambda}C_\lambda$ intersects $K$, so let $x\in K$ be in this intersection.

Then,
\begin{textequation}[3.4.62]
for every $\varepsilon >0$ and for every $\mu$, there is some $x_{\lambda _{\varepsilon ,\mu}}\in B_\varepsilon (x)$ with $\lambda _{\varepsilon ,\mu}\geq \mu$.\footnote{This is the statement that $x$ is an accumulation point of $\{ x_{\lambda}:\lambda \geq \mu \}$ for each $\mu$.}
\end{textequation}
Define
\begin{equation*}
\Lambda '\coloneqq \left\{ \coord{\varepsilon ,\lambda}:\varepsilon \in \R ^+,\lambda \in \Lambda \text{ such that }x_\lambda \in B_{\varepsilon}(x)\right\} .
\end{equation*}
We order $\R ^+$ with the reverse of the usual ordering.  Then, $\R ^+\times \Lambda$ is directed.  We verify that $\Lambda '$ is likewise directed.  Let $\varepsilon _1,\varepsilon _2>0$ and let $\lambda _1,\lambda _2\in \Lambda$ be such that $x_{\lambda _k}\in B_{\varepsilon _k}(x)$.  Take $\varepsilon \coloneqq \min \{ \varepsilon _1,\varepsilon _2\}$ and let $\lambda _3$ be at least as large as $\lambda _1$ and $\lambda _2$.  By \eqref{3.4.62}, there is some $\lambda \geq \lambda _3$ such that $x_\lambda \in B_\varepsilon (x)$.  Then, $\coord{\varepsilon ,\lambda}\in \Lambda '$ and $\coord{\varepsilon ,\lambda}\geq \coord{\varepsilon _k,\lambda _k}$, so that $\Lambda '$ is directed.

Now, for $\coord{\varepsilon ,\mu}\in \Lambda '$, pick some $\lambda _{\varepsilon ,\mu}$ such that (i)~$x_{\lambda _{\varepsilon ,\mu}}\in B_\varepsilon (x)$ and (ii)~$\lambda _{\varepsilon ,\mu}\geq \mu$.  Of course $\coord{\varepsilon ,\mu}\mapsto x_{\varepsilon ,\mu}$ converges to $x$, but we still need to check that it is a subnet.

Let $\lambda _0$ be an arbitrary index.  Then, if $\coord{\varepsilon ,\mu}\geq (1,\lambda _0)$, it follows that $\lambda _{\varepsilon ,\mu}\geq \mu \geq \lambda _0$, and so, by \cref{prp3.3.92}, this is indeed a subnet.

\blankline
\noindent
$(\Leftarrow )$ Suppose that every net $\lambda \mapsto x_\lambda$ has a subnet that converges to a limit in $K$.  Let $\collection{C}$ be a collection of closed sets such that the intersection of finitely many elements of $\collection{C}$ intersects $K$.  Let $\tilde{\collection{C}}$ be the collection of all finite subsets of $\collection{C}$ ordered by inclusion, so that it is indeed a directed set.  For each element $\tilde{C}\in \tilde{\collection{C}}$, let $x_{\tilde{C}}\in \left( \bigcap _{C\in \tilde{C}}C\right) \cap K$, which by assumption is nonempty.  Then, $\tilde{\collection{C}}\ni \tilde{C}\mapsto x_{\tilde{C}}\in K$ is a net, and so by hypothesis, this has a subnet $\mu \mapsto x_{\tilde{C}_\mu}$ that converges to $x\in K$.  We wish to show that $x\in \left( \bigcap _{C\in \collection{C}}C\right) \cap K$.  We already know that $x\in K$, so to show this, it suffices to show that $x\in C$ for all $c\in \collection{C}$.  To show this, it suffices to show that $\mu \mapsto x_{\tilde{C}_\mu}$ is eventually in each $C\in \collection{C}$ (we will then have that $x\in C$ because each $C$ is closed).  By the definition of a subnet, it suffices to show that each $C$ eventually contains $\tilde{C}\mapsto x_{\tilde{C}}$.  However, for $C_0\in \collection{C}$, $\{ C_0\} \in \tilde{\mcal{C}}$, and so whenever $\tilde{C}\geq \{ C_0\}$, i.e.~$C_0\in \tilde{C}$, certainly $x_{\tilde{C}}\in C_0$ as in fact $x_{\tilde{C}}\in \left( \bigcap _{C\in \tilde{C}}C\right) \cap K$.
\end{proof}
\end{thm}
\begin{crl}{Bolzano-Weierstrass Theorem}{BolzanoWeierstrassTheorem}\index{Bolzano-Weierstrass Theorem}
Every eventually bounded net in $\R ^d$ has a convergent subnet.
\begin{proof}
Every eventually bounded net is eventually contained in some closed rectangle interval $[-M,M]\times \cdots \times [-M,M]$, and so, every eventually bounded net has a subnet which is contained (not \emph{eventually} contained) in $[-M,M]\times \cdots \times [-M,M]$.  $[-M,M]\times \cdots \times [-M,M]$ is quasicompact by the \namerefpcref{HeineBorelTheorem}.  By the subnet characterization of quasicompactness (the previous theorem), it follows that this net has a convergent subnet.
\end{proof}
\end{crl}
\begin{crl}{}{}
Bounded infinite subsets of $\R ^d$ have accumulation points.
\begin{rmk}
This is sometimes also called the Bolzano-Weierstrass Theorem.
\end{rmk}
\begin{proof}
Any infinite subset of $\R ^d$ will have a sequence of distinct points contained in it.  If the set is bounded, this sequence will be bounded, and so by the Bolzano-Weierstrass Theorem has a convergent subnet.  The limit of this subnet is an accumulation point of the set (\cref{prp3.4.27}).
\end{proof}
\end{crl}

\section{Summary}

This has been a rather long chapter and we have covered many different properties of the real numbers.  For convenience, we summarize here some of the main points we have covered.
\begin{enumerate}
\item The real numbers are the unique (up to isomorphism of totally-ordered fields) nonzero Dedekind-complete totally-ordered field.
\item The real numbers are Cauchy-complete (\cref{CompletenessOfR}).
\item The real numbers have cardinality $2^{\aleph _0}>
\aleph _0$ (\cref{CantorsCardinalityTheorem,thm2.4.118}).
\item The real numbers are Archimedean (the natural numbers are unbounded)---see \cref{thm3.2.3}.
\item Nondecreasing/nonincreasing nets bounded above/bounded below converge (\nameref{MonotoneConvergenceTheorem}).
\item A subset of $\R$ is closed and bounded iff it is quasicompact (\nameref{HeineBorelTheorem}).
\item Every bounded net in $\R$ has a convergent subnet (\nameref{BolzanoWeierstrassTheorem}).
\end{enumerate}