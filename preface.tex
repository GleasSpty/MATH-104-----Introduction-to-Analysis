\chapter*{Preface}

\addtocontents{toc}{\protect\setcounter{tocdepth}{-1}}

I started writing these notes in order to prepare for the `lectures'\footnote{This is in quotes because they weren't really lectures in the traditional sense of the word.} I would be giving for MATH 104 Introduction to Analysis at the University of California, Berkeley during Summer 2015.  My obsessive-compulsive perfectionism and completionism turned them into what they are today.

At first, these notes were really just for me---I wanted to be sure I was ready to teach the class.  As I'm sure you're aware if you've ever taught before, there is much more to being able to teach well than simply knowing all the material.  For example, it is not enough to simply know Theorems 1 and 2.  Among other things, for example, you have to know the order in which they come in the theory.  Most of the motivation for starting these notes was to make sure I got all of that straight in my mind before I went up in front of a class.

It almost immediately became apparent, however, that the book I was assigned to use to teach the course (which shall remain unnamed\footnote{Lest you think I am referring to Pugh's book \cite{Pugh}, let me clarify that I am not.  (Pugh is at Berkeley, and so it would be natural for you to think that they made this the required text, but alas, they did not.)  Indeed, Pugh's book is one of my favorite real analysis books, and it's likely that had I been assigned to use his book, these notes would not exist.}\textellipsis ) was not sufficient for what I wanted to do.  The target audience for the course (or so I thought) was students who were at least considering going to graduate school in mathematics, but also who had never seen any analysis before, and the book I was supposed to use was just simply too elementary to serve this purpose, especially for the caliber of students at one of the world's top mathematics departments.  What really surprised me, however, is that I could not find a single mathematical analysis book that served \emph{all} of my needs.  For example, I wanted to teach the Lebesgue integral in place of the Riemann integral, and nearly all of the mathematical analysis books that teach the Lebesgue integral as the \emph{primary} definition of the integral assume that the student has seen analysis before.  Thus, within a week or so after started the course,\footnote{The course ran 8 weeks in total.} the notes switched from just being a device I would use to prepare, to what would serve as the effective text for the course.

I now briefly discuss several of the topics / ways of treating topics that I had difficultly finding in standard analysis texts.

\subsubsection{Analysis with an eye towards category theory}

Given that this is a first course in analysis, I didn't want to go super heavy on the category theory---I don't even use functors, for example---but on the other hand I felt as if it would be a sin to say nothing of it.  For example, $\R$ as a set, a field, a topological space, a uniform space, a metric space, an ordered-field, blah blah etc. etc.\textellipsis all of these are \emph{very} different things, and the appropriate way to make these distinctions is of course via the use of morphisms (because morphisms determine the meaning of isomorphism).  In $\Set$, the mathematics cannot tell the difference between $\R$ and $2^{\N}$, whereas in $\Top$ they are very different:  one is compact and the other is not.  On the other hand, in $\Top$, the mathematics cannot tell between $(0,1)$ and $\R$, whereas in $\Uni$ they are very different:  one is complete and the other is not.  I can understand one making the argument that you can make these distinctions without using categories per se.  On the other hand, the budding mathematician \emph{has} to learn categories at some point, and so they may as well learn the absolute basics now.

I also implicitly made use of other more advanced categorical ideas, most notably of `pseudo-universal' properties.\footnote{I say ``pseudo'' because I usually phrase things as ``$A$ is the `smallest' things that satisfies XYZ and contains $B$.'' instead of ``There is a universal morphism from $B$ to $A$ in category ABC.''.  Maybe I will change this in the future, but I felt as if this level of mathematical sophistication in Chapter 1 of a first analysis course could lose a lot of people.}  For example, the integers are defined as the unique\footnote{Up to isomorphism in the category of preordered rings.} totally-ordered integral cring which (i)~contains $\N$ and (ii)~is contained in any other totally-ordered integral cring which contains $\N$.  This is just one example on how I tried to stress that \emph{mathematical objects are defined by the properties that uniquely specify them}, instead of the nuts and bolts which happen to make them up.  The right way to think of the real numbers is as the unique nonzero\footnote{It is common to require that $0\neq 1$ in fields, but I find this condition awkward and unnecessary---I'm not aware of any serious benefit this has other than to remove the necessity of having to say ``nonzero'' in places (like here).} Dedekind-complete totally-ordered field, not as Dedekind cuts or equivalence classes of Cauchy sequences or whatever---these are merely tools used to prove the existence of such an object.

\subsubsection{Construction of the reals from scratch}

Many books go from $\N$ to $\R$; what I actually had difficulty finding was sources that constructed $\N$ from scratch.  I very much wanted to say at the end of the course that everything we proved could be reduced to nothing more than pure logic\footnote{In a naive sense.  Not in the sense of mathematical logic.}; and the idea that if you have a bunch of things, you can give all those things a single name ($X$, for example), and now you have a new thing.\footnote{That is, the naive notion of a set.}  With these basic tools in place, we can construct $\N$ from the ground-up, and so in turn we can construct $\R$ from the ground-up.

\subsubsection{Nets}

I really can't believe how many peers of mine, and even professional mathematicians, don't know nets.  There is no really getting around them:  if you want to do general topology, you \emph{have} to use nets. \footnote{Okay, so that's not quite true.  There are alternatives to nets (e.g.~filter bases).  The point is that sequences themselves are not enough---if not nets, you will need some other generalization/replacement for sequences.} Sequences are just not enough.  For example, the real numbers with the discrete topology and the cocountable topology provide an example of a set equipped with two nonhomeomorphic topologies for which the notions of sequential convergence agree.  Thus, you cannot uniquely specify a topology by saying what i means for sequences to converge---if you ever want to define a topology using convergence, you \emph{must} use nets.  Moreover, the definition of a uniform space is the way it is because then the uniformity itself, ordered by reverse star-refinement, is a directed set, so that one can conveniently index nets by the uniformity itself.  And even if you do only care about $\R$, however, nets can make some arguments a tad bit easier.  For example, $\R ^+$ both with the usual ordering and reverse ordering provide examples of directed sets, and so you can interpret $\lim _{\varepsilon \to 0^+}$ and $\lim _{x\to \infty}$ both as limits of nets, which can be used to slightly simplify some proofs.

\subsubsection{Subnets}

I imagine that there are at least some sources that shy away from using nets because subnets are notoriously more difficult than subsequences.  Despite how determined I was to use nets, even I struggled with the tedium of the definition of subnets for awhile.  Eventually I realized that this difficult was a consequence of using the `wrong' definition of subnet.  There are at least two distinct definitions of subnet in the literature that I was aware---see \cref{prp3.3.92,prp3.3.93}---and both of these were `wrong' in the sense that they did not agree with what the analogous notion for filter bases (filterings---see \cref{Filtering}) was.\footnote{There wasn't really any uncertainly about what the `right' notion was for filter bases.}  By examining the definition of a filtering, I was able to formulate a definition of subnet (\cref{dfnSubnet}) that agreed (\cref{prp4.4.8}) with the definition of filter bases.  That definition is as follows.
\begin{displayquote}
A \emph{subnet}\index{Subnet} of a net $x:\Lambda \rightarrow X$ is a net $y:\Lambda '\rightarrow X$ such that
\begin{enumerate}
\item for all $\mu \in \Lambda '$, $y_\mu =x_{\lambda _\mu}$ for some $\lambda _\mu \in \Lambda$; and
\item whenever $U\subseteq X$ eventually contains $x$, it eventually contains $y$.
\end{enumerate}
\end{displayquote}
That is, it is a net made up from terms of the original net that has the property that any set which eventually contains the original net eventually contains the subnet.  I personally find this to be a much cleaner definition than the ones involving awkward conditions on the indices of the nets:  The indices themselves don't matter---it is only what \emph{eventually happens} that matters.

\subsubsection{Uniform spaces}

My inclusion of uniform spaces was essentially a replacement for a treatment of metric spaces (which, for some reason, seems standard).  Unfortunately, metric spaces are just not enough.  For example, if you care about continuous functions on the real line,\footnote{If you're a mathematician or planning on becoming one, chances are you do, at least a little.} you cannot get away with metric spaces.  You could generalize to semimetric spaces, but I find this to not be as clean as uniform spaces.  Uniform spaces also include \emph{all} topological groups, and so by generalizing to uniform spaces, you include another hugely important example of topological spaces.  For some reason, the theory of uniform spaces seems to be very obscure in standard undergraduate and graduate curricula, and I myself was never formally taught it in my own education.  Despite this, I've found knowledge of uniform spaces to be incredibly useful,\footnote{For example, there is a generalization of Haar measure to uniform spaces (see the \nameref{HaarHowesTheorem}), which, among other things, gives isometric invariance of Lebesgue measure for free.} in particular in functional analysis, and they deserve more attention than they get in courses in analysis and general topology.

\subsubsection{\texorpdfstring{$\sigma$}{sigma}-algebras}

In general, I try to place a relatively large amount of emphasis on motivation, and so when I first started writing down my development of measure theory, I decided from the beginning that I would first introduce the notion of an outer-measure, and then introduce the notion of an abstract $\sigma$-algebra once I had shown that Carath\'{e}odory's Theorem (\cref{CaratheodorysTheorem}) says that the measurable sets form a $\sigma$-algebra.  That is, you don't have to pull the definition out of thin air---the theory gives it to you.  As I continued to develop the theory, however, I found that I never really needed to make use of the notion---working with measure spaces (sets equipped with a measure) was sufficient:  all I had to do was add in the hypothesis ``measurable'' in certain places.  I found that this made the theory quite a bit cleaner.  For one thing, it was easier to teach, but even in terms of the mathematics itself, it made some things a bit easier.  For example, when discussing product measures, you run into the issue of whether or not the product $\sigma$-algebra agrees with the $\sigma$-algebra of measurable sets on the product.  If I recall, in general this fails, but that doesn't matter:  if you don't use $\sigma$-algebras, it's just not an issue---Mr.~Carath√©odory's \emph{always} tells you what the measurable sets are.

\subsubsection{Measure theory and the Lebesgue integral}

Since the first day you learned the `definition' of the integral in your first calculus class,\footnote{Or when self-teaching, for the particularly precocious amongst you.} you have almost certainly thought of the integral as the `(signed) area under the curve'.  That's what it is.  Measure theory allows one to make this the \emph{definition} of the integral.  There is no need for these awkward approximations by rectangles.\footnote{Of course, in the spirit of defining things by the properties they uniquely specify, I don't \emph{quite} state it like this.  Instead I say something like ``The area under the curve is the unique extended real-valued function on nonnegative Borel functions such that\textellipsis''---see \cref{Integral}.}  The counter-argument to this is ``Okay, sure, but that just reduces the complication of the Lebesgue integral to measure theory.''.  To be fair, this is correct---if you take this route, you have to develop measure theory, whereas, with the Riemann integral, you can start talking about partitions, etc., almost immediately.  That being said, I take my time developing measure theory more fully because I don't like approaches which `run straight to the finish line', but in principle, if you really don't like the amount of build-up that measure theory requires, you can probably cut-down the length of my development by two-thirds or so to get to the definition of the integral.  To those who would still object to even a minimal measure theoretic treatment, I would argue the following.  There are two types of students:  those who want to become mathematicians and those who do not.  The former need to learn the Lebesgue integral anyways, and so I don't see much point dicking around with the Riemann integral when they'll have to learn the Lebesgue integral in a graduate analysis course (if not earlier) anyways.  In the latter case, simply stating informally ``It's the area under the curve.'' should suffice---in fact, that's not even really a lie, or even a stretch of the truth---if you set things up right, that's \emph{literally} what it is.

\subsubsection{Tangent spaces in differentiation in \texorpdfstring{$\R ^d$}{Rd}}

I actually found differentiation to be quite a bit easier when studying it on manifolds as opposed to in $\R ^d$.  I think this is because it is easy to mix up objects that should be thought of as \emph{points} and objects that should be thought of as \emph{vectors} in $\R ^d$, something that is just plain impossible to do in a general manifold.  In an attempt to circumvent this potential confusion, I introduce the term \emph{tangent space} as well as notation for it, $\tangent{\R ^d}[x]$.  This is really only for conceptual clarity:  it is not the definition one would use on a manifold.  In fact, it would be essentially impossible to do this because the usual definition of the tangent space on a manifold requires one to already have a theory of differentiation on $\R ^d$.  Instead, I just take $\tangent{\R ^d}[x]$ as a \emph{metric vector space} whereas the `entire space' $\R ^d$ is considered as a \emph{metric space}.\footnote{Note the implicit elementary categorical ideas coming into play here.  Also note the two distinct uses of the term ``metric''.  Who the hell came-up with this terminology anyways?  Mathematicians need to get better at naming things\textellipsis Surely you can find a more descriptive adjective than ``normal'' or ``regular'' for your fancy new idea, yes?}

I also made a point to introduce index notation and tensor fields.  The motivation for this one really comes from my physics background:  index notation and tensor calculus are mandatory background if one wants to do theoretical physics.  But this is not the only reason for introducing it of course---I personally find index notation to sometimes be very useful, even in pure Riemannian geometry.  In particular, I find that index notation can make some equations much more transparent.  For example, compare\footnote{At least for a torsion-free covariant derivative.}
\begin{equation*}
R\indices{_{ab}^c_d}\coloneqq \nabla _a\nabla _b-\nabla _b\nabla _a
\end{equation*}
versus
\begin{equation*}
R(u,v)w\coloneqq \nabla _u\nabla _vw-\nabla _v\nabla _uw-\nabla _{[u,v]}w.
\end{equation*}
Morally, the curvature tensor is supposed to measure the lack of commutativity of the covariant derivative, and the former makes that perfectly clear---it is literally the commutator $[\nabla _a,\nabla _b]$---whereas the latter has this awkward extra term $\nabla _{[u,v]}w$ that feels like it shouldn't be there.  Mathematicians then also tend to make this (what I find awkward) distinction between\footnote{Using the notation of \cite{Lee}.} the curvature endomorphism $R$, the curvature tensor $Rm$, and the curvature form $\mathfrak{R}$, but in my mind, these are all just different versions of $R\indices{_{ab}^c_d}$:  $R\indices{_{ab}^c_d}$, $R_{abcd}$, and $[R_{ab}]\indices{^c_d}$ respectively.  To each his own, but, in this case, I personally find physicist's notation much clearer, even in the realm of pure mathematics.  Note that there is no discussion of riemannian geometry here---this is just a motivating example to justify the teaching of index notation.

\subsubsection{Differentiability}

Finally, I also use what seems to be a nonstandard definition of differentiable.  This was almost by accident.  Without consulting a reference, I just wrote down what to me seemed to be the most obvious thing:  a real-valued function on $\R ^d$ is differentiable at a point $x$ iff (i)~all the directional derivatives at $x$ exist and (ii)~the map that sends a tangent vector to its directional derivative is linear and continuous.  That is, I put the minimal conditions in order to guarantee that the differential was a one-form.  It wasn't until I started trying to prove things that I realized this was nonstandard.  As it turns out, with this definition, there are \emph{infinitely-differentiable functions which are not continuous}.\footnote{See \cref{exm6.2.15}.}  Oopsies.  Despite this pathology, I just decided to roll with it, partially due to time constraints.  To give you some idea, the first draft when I had finished it was 344 pages, which had been written in just a little more than 8 weeks.  I had completely underestimated how much work this was going to be\footnote{By the end, I pretty much did nothing besides teach and write these notes.} and at that point in the course, were I try to go back and change the definition of differentiable, I would have been at serious risk of just simply not being prepared for future classes.  In the end, things worked out just fine though---I found that a lot of the time I didn't need differentiable functions to be continuous, and when I did, a simple distinction between smooth and infinitely-differentiable did the trick.\footnote{\emph{Smooth} wound-up meaning that (i)~the function was infinitely differentiable and (ii)~all the derivatives where continuous.}  In fact, as I find this definition simpler than the one usually given (Fr\'{e}chet differentiable), I may just stick with it the next time I teach.

\section*{A note to the reader}

The mathematics in these notes is developed ``from the ground up''.  In particular, in principle, there are no prerequisites.  That said, there is a modest amount of basic material that I cover sufficiently fast that it would be very helpful if you had at least passing familiarity with.  Essentially all of this `prerequisite' material is given in the appendices.\footnote{Though there is also quite a bit of material there that I would not expect you to know.}  I recommend you read these notes linearly, and refer to the appendices as needed when you come across concepts you are not familiar with.  The notes are written in such a way that I would expect a student with no background to refer to the appendices \emph{very often} in the beginning, but very little by the end.

Of all the statements which are true in these notes, they are roughly divided into two broad categories:  the statements which are true by definition and the statements which are true because we can prove them.  For the former, we have \emph{definitions}; for the latter, we have \emph{theorems}, \emph{propositions}, \emph{corollaries}, \emph{lemmas}, and \emph{claims}.  We also have ``meta'' versions of (some of) these.

A definition is exactly what you think it is.  A ``meta-definition'' is actually a whole collection of definitions---whenever you plug something in for XYZ, you get an actual definition.  This explanation is probably not very lucid now, but I imagine it should be pretty obvious what I mean by this when you actually come to them---see, for example, \cref{EventuallyXYZ}.  For the record, ``meta-definition'' is not a standard term (and I don't really think there is a standard term for this).\footnote{The closest I can think of is \emph{axiom schema}, but ``definition schema'' sounds quite awkward to my ear.}  Similarly for ``'meta-propositions'', etc..

There is no hard and fast distinction between what I called theorems, propositions, corollaries, and lemmas.  I tried to roughly adhere to the following conventions.  If a result is used only in a proof of a single result and nowhere else, it is a \emph{lemma}.  If a result follows immediately or almost immediately from another result, it is a \emph{corollary}.  Results of particular significance are \emph{theorems}.  Everything else is a \emph{proposition}.  Claims, on the other hand, are distinct in that, not only only are they used in the proof of a single result like lemmas, but furthermore they wouldn't even make sense as stand-alone results (for example, if they use notation specific to the proof).

In particular, note that the distinction between theorems and propositions has to do with the relative \emph{significance}\footnote{Obviously this is completely subjective and I would not expect any mathematician to pick out the exact same results which deserve the title of ``theorem''.} of the \emph{statement} of the result, and has nothing to to with the \emph{difficulty} of the \emph{proof}.  Indeed, there are quite a few rather trivial results labeled as theorems simply because they are important.

I should mention that every now and then I give nonstandard names to results which would otherwise not have names.  Part of the motivation for this is that I personally find this makes it easier to remember which result is which.  For example, would you rather I refer to ``\cref{KelleysConvergenceAxioms}'' or the ``\nameref{KelleysConvergenceAxioms}''?  Just be warned that you shouldn't go up to other mathematicians, use these names, and expect them to know what you're talking about.  (I will point it out when a name is nonstandard.)

As an unimportant comment, I mention in case you're curious that I used double quotes when I am quoting something, usually a term or phrase either I or people in general use, and single quotes to indicate that the thing is quotes is not literally that thing.  For example, mathematicians prove theorems and physicists prove `theorems'.

Finally, I want to make a comment on the difficulty of these notes.  This course is supposed to be hard.  Very hard.  But not impossible.  I do expect you to seriously bust your ass to learn the material.  But it's also reasonable enough that, if you do put in the work, you will be able to do decently in the course.

\blankline
\horizontalrule
\blankline

\hfill Jonathan Gleason \\
\hfill Ph.~D. Student, Department of Mathematics, University of California, Berkeley \\
\hfill First draft (of preface):  16 September 2015 \\
\hfill 16 September 2015

\addtocontents{toc}{\protect\setcounter{tocdepth}{3}}